CONTENT

Section 15 294. Application Failure
Section 15 296. Practice Test - Application Failure
Section 15 297. Control Plane Failure
Section 15 299. Practice Test - Control Plane Failure
Section 15 300. Worker Node Failure
Section 15 302. Practice Test - Worker Node Failure
Section 15 303. Network Troubleshooting
Section 15 304. Practice Test - Network Troubleshooting



===================================
Section 15 294. Application Failure
===================================

Example application troubleshooting
-----------------------------------
2 tier application with web and database server.

It is a good practice to draw a chart or map of how the applications are configured before start the troubleshooting.

--------------------------------------------------------
DB Pod / Db Service --> Web Pod / Web Service --> Client 
--------------------------------------------------------

Depending of how much we know about the failure, we can start from the beginning or the end of the chart. We have to check every object and link of this map in order to find the root cause of the issue.


In this case we are going from the client side first.

1. Testing the accessability of the web server
	terminal --> curl http://web-service-ip:node-port

	# result: curl: (7) Failed to connect to web-server-ip port node-port: Connection timed out

2. Check the web service for configured endpoint
	terminal --> kubectl describe service web-service

# result:
Name:				web-service
Namespace: 			default
Labels: 			<none>
Annotations: 			<none>
Selector: 			name=webapp-mysql	# this is the configured selector
Type: 				NodePort
IP: 				10.96.0.156
Port: 				<unset> 8080/TCP
TargetPort: 			8080/TCP
NodePort: 			<unset> 31672/TCP
Endpoints: 			10.32.0.6:8080		# in this example we have configured endpoint
Session Affinity: 		None 
External Traffic Policy: 	Cluster 
Events: 			<none>


If 'Endpoints' are not listed, we need to check service to pod discovery.

Check for matching selectors. in the service description and the pod definition file.


web-pod-definition.yanl
------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: webapp-mysql
  labels:
    app: example-app
    name: webapp-mysql				# this is the label of the pod that should match the service selector
spec:
  containers:
  - name: webapp-mysql
    image: simple-webapp-mysql
    ports:
    - containerPort: 8080
------------------------------------------


3. Check the status of the pod and the count of restarts can give us idea if the pod is running or getting restarted.
	terminal --> kubectl get pods

	# result
	NAME		READY		STATUS		RESTARTS	AGE
	Web		1/1		Running		5		50m


4. Check the events accured in the pod
	terminal --> kubectl describe pod web

# result:

Events:
Type Reason Age From Message
---- ------ ---- ---- -------
Normal Scheduled 52m default-scheduler Successfully assigned webapp-mysql to worker-1 Normal Pulling 52m kubelet, worker-1 pulling image "simple-webapp-mysql"
Normal Pulled 52m kubelet, worker-1 Successfully pulled image "simple-webapp-mysql" Normal Created 52m kubelet, worker-1 Created container
Normal Started 52m kubelet, worker-1 Started container


We can look at the logs of the pod. To see the reason for the restart we have to use '-f' option or '--previous' to look at the logs from the last running pod

5. Check the logs of the current or previous pod 	# look at the current pod's logs and wait for the failure
	terminal --> kubectl logs web -f		# terminal --> kubectl logs web -f --previous

# result:
10.32.0.1 - - [01/Apr/2019 12:51:55] "GET / HTTP/1.1" 200 -
10.32.0.1 - - [01/Apr/2019 12:51:55] "GET /static/img/success.jpg HTTP/1.1" 200 - 
10.32.0.1 - - [01/Apr/2019 12:51:55] "GET /favicon.ico HTTP/1.1" 404 -
10.32.0.1 - - [01/Apr/2019 12:51:57] "GET / HTTP/1.1" 200 -
10.32.0.1 - - [01/Apr/2019 12:51:57] "GET / HTTP/1.1" 200 -
10.32.0.1 - - [01/Apr/2019 12:51:58] "GET / HTTP/1.1" 200 -
10.32.0.1 - - [01/Apr/2019 12:51:58] "GET / HTTP/1.1" 200 –
10.32.0.1 - - [01/Apr/2019 12:51:58] "GET / HTTP/1.1" 400 – Some Database Error application exiting!


6. Check the status of the DB service
	terminal --> kubectl describe service db-service

7. Check the logs of the DB pod			# look at the current pod's logs and wait for the failure
	terminal --> kubectl logs db -f		# terminal --> kubectl logs db -f --previous - for previous running pod

Troubleshooting references in Kubernetes documentation
	- https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/





===================================================
Section 15 296. Practice Test - Application Failure
===================================================

1. Troubleshooting Test 1: A simple 2 tier application is deployed in the alpha namespace. It must display a green web page on success. Click on the App tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
------------------------------------------------------------------------------------------
Stick to the given architecture. Use the same names and port numbers as given in the below architecture diagram. Feel free to edit, delete or recreate objects as necessary.

When we open the app we recieve FAILURE with message:
Environment Variables: DB_Host=mysql-service; DB_Database=Not Set; DB_User=root; DB_Password=paswrd; 2003: Can't connect to MySQL server on 'mysql-service:3306' (-2 Name does not resolve)
From webapp-mysql-78fd9544f6-wr5b7!

--------------------------------------------------------
mysql Pod / mysql-service --> webapp pod / webapp service --> Client 
--------------------------------------------------------
			   ^
			   |

From the error we can localize that the problem is between mysql-service and webapp pod, because the app is loading but cannot connect to the DB.

We want to set current context to the specific namespace to avoid typing the namespace param in every command

Show context help commands
	terminal --> kubectl config --help			
# set-context       Set a context entry in kubeconfig
# Usage:
  kubectl config set-context [NAME | --current] [--cluster=cluster_nickname] [--user=user_nickname]
[--namespace=namespace] [options]

Show context config help commands
	terminal --> kubectl config set-context --help		# --namespace='':

Set the specific context (default namespace)
	terminal --> kubectl config set-context --current --namespace=alpha
	
	# kubectl				# common kubernetes command
	# config set-context --current		# modify current default context
	# --namespace=alpha			# set the namespace

	# Context "default" modified.


List rthe pods in alpha namespace
	terminal --> kubectl get pods 

	# result:
	NAME                            READY   STATUS    RESTARTS   AGE
	mysql                           1/1     Running   0          7m29s
	webapp-mysql-78fd9544f6-wr5b7   1/1     Running   0          7m29s		# this is deployment pod


List deployments
	terminal --> k get deploy

	# result:
	NAME           READY   UP-TO-DATE   AVAILABLE   AGE
	webapp-mysql   1/1     1            1           18m


List services
	terminal --> k get svc

	# result:
	NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
	mysql         ClusterIP   10.43.131.32   <none>        3306/TCP         19m	# service of the mysql pod
	web-service   NodePort    10.43.91.137   <none>        8080:30081/TCP   19m	# service of the webapp-mysql pod


We can test accessability with curl command
	terminal --> curl http://localhost:30081

	# the result is html with the error we see when we open the app


Print the deployment
	terminal --> k describe deployment webapp-mysql

# result:
------------------------------------------------------------------------------
Name:                   webapp-mysql
Namespace:              alpha
CreationTimestamp:      Mon, 17 Feb 2025 13:04:13 +0000
Labels:                 name=webapp-mysql
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               name=webapp-mysql
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=webapp-mysql
  Containers:
   webapp-mysql:
    Image:      mmumshad/simple-webapp-mysql
    Port:       8080/TCP			
    Host Port:  0/TCP
    Environment:
      DB_Host:      mysql-service			# used service
      DB_User:      root
      DB_Password:  paswrd
    Mounts:         <none>
  Volumes:          <none>
  Node-Selectors:   <none>
  Tolerations:      <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   webapp-mysql-78fd9544f6 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  22m   deployment-controller  Scaled up replica set webapp-mysql-78fd9544f6 to 1
------------------------------------------------------------------------------


List services
	terminal --> k get svc

	# result:
	NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
	mysql         ClusterIP   10.43.131.32   <none>        3306/TCP         19m	# service name do not match the deployment
	web-service   NodePort    10.43.91.137   <none>        8080:30081/TCP   19m	


Fix the service name to match the deployment configuration
	terminal --> k edit service mysql


-------------------------------------------
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2025-02-17T13:04:13Z"
  name: mysql-service				# fixed from mysql to mysql-service
  namespace: alpha
  resourceVersion: "724"
  uid: 690a281d-9107-4cfc-9673-298cfa13b19f
spec:
  clusterIP: 10.43.131.32
  clusterIPs:
  - 10.43.131.32
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - port: 3306
    protocol: TCP
    targetPort: 3306
  selector:
    name: mysql
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
-------------------------------------------
save changes - escape, :wq!, enter

# result:
A copy of your changes has been stored to "/tmp/kubectl-edit-3817483867.yaml"
error: At least one of apiVersion, kind and name was changed

Delete the current service
	terminal --> k delete svc mysql
	
	# result: service "mysql" deleted


Create the service with the new configuration
	terminal --> k create -f /tmp/kubectl-edit-3817483867.yaml

	# result: service/mysql-service created


List services to check the creation
	terminal --> k get svc

	# result:
	NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
	mysql-service   ClusterIP   10.43.131.32   <none>        3306/TCP         37s	# created successfully
	web-service     NodePort    10.43.91.137   <none>        8080:30081/TCP   30m


Check if the app is running correctly now.

- click 'Check' button




2. Troubleshooting Test 2: The same 2 tier application is deployed in the beta namespace. It must display a green web page on success. Click on the App tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
---------------------------------------------------------------------------------------------------------------------
Stick to the given architecture. Use the same names and port numbers as given in the below architecture diagram. Feel free to edit, delete or recreate objects as necessary.


When we open the app we recieve FAILURE with message:
Environment Variables: DB_Host=mysql-service; DB_Database=Not Set; DB_User=root; DB_Password=paswrd; 2003: Can't connect to MySQL server on 'mysql-service:3306' (111 Connection refused)
From webapp-mysql-78fd9544f6-q64lr!


--------------------------------------------------------
mysql Pod / mysql-service --> webapp pod / webapp service --> Client 
--------------------------------------------------------
			   ^
			   |

From the error we can localize that the problem is between mysql-service and webapp pod, because the app is loading but cannot connect to the DB.

We want to set current context to the specific namespace to avoid typing the namespace param in every command

Show context help commands
	terminal --> kubectl config --help			
# set-context       Set a context entry in kubeconfig
# Usage:
  kubectl config set-context [NAME | --current] [--cluster=cluster_nickname] [--user=user_nickname]
[--namespace=namespace] [options]

Show context config help commands
	terminal --> kubectl config set-context --help		# --namespace='':

Set the specific context (default namespace)
	terminal --> kubectl config set-context --current --namespace=beta
	
	# kubectl				# common kubernetes command
	# config set-context --current		# modify current default context
	# --namespace=beta			# set the namespace

	# Context "default" modified.



List pods
	terminal --> k get pods

	# result:
	NAME                            READY   STATUS    RESTARTS   AGE
	mysql                           1/1     Running   0          3m21s	# db pod
	webapp-mysql-78fd9544f6-q64lr   1/1     Running   0          3m21s	# web app pod


List services
	terminal --> k get svc

	# result:
	NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
	mysql-service   ClusterIP   10.43.127.37   <none>        3306/TCP         4m9s	# db service
	web-service     NodePort    10.43.14.9     <none>        8080:30081/TCP   4m9s	# web service


List deployments
	terminal --> k get deploy

	# result:
	NAME           READY   UP-TO-DATE   AVAILABLE   AGE
	webapp-mysql   1/1     1            1           6m32s


Print the deployment
	terminal --> k describe deployment webapp-mysql

# result:
------------------------------------------------------------------------------
Name:                   webapp-mysql
Namespace:              beta
CreationTimestamp:      Mon, 17 Feb 2025 13:36:32 +0000
Labels:                 name=webapp-mysql
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               name=webapp-mysql
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=webapp-mysql
  Containers:
   webapp-mysql:
    Image:      mmumshad/simple-webapp-mysql
    Port:       8080/TCP
    Host Port:  0/TCP
    Environment:
      DB_Host:      mysql-service			# the service name matches, so its OK
      DB_User:      root
      DB_Password:  paswrd
    Mounts:         <none>
  Volumes:          <none>
  Node-Selectors:   <none>
  Tolerations:      <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   webapp-mysql-78fd9544f6 (1/1 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  6m48s  deployment-controller  Scaled up replica set webapp-mysql-78fd9544f6 to 1
------------------------------------------------------------------------------


List services
	terminal --> k get svc

	# result:
	NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
	mysql-service   ClusterIP   10.43.127.37   <none>        3306/TCP         8m17s
	web-service     NodePort    10.43.14.9     <none>        8080:30081/TCP   8m17s


Print the db service
	terminal --> k describe svc mysql-service

# result:
-----------------------------------------
Name:                     mysql-service
Namespace:                beta
Labels:                   <none>
Annotations:              <none>
Selector:                 name=mysql			# we have correct selector
Type:                     ClusterIP
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.43.127.37
IPs:                      10.43.127.37
Port:                     <unset>  3306/TCP
TargetPort:               8080/TCP
Endpoints:                10.42.0.11:8080	# endpoint match the db pod ip configuration, but the port is not correct
Session Affinity:         None
Internal Traffic Policy:  Cluster
Events:                   <none>
-----------------------------------------
# The port must be 3306

Check the pod connection info
	terminal --> k get pods -o wide

# result:
NAME                            READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
mysql                           1/1     Running   0          10m   10.42.0.11   controlplane   <none>           <none>
webapp-mysql-78fd9544f6-q64lr   1/1     Running   0          10m   10.42.0.12   controlplane   <none>           <none>
								


Edit the DB service and change the used port
	terminal --> k edit svc mysql-service

-------------------------------------------------
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2025-02-17T13:36:32Z"
  name: mysql-service
  namespace: beta
  resourceVersion: "1401"
  uid: 1c504ed1-a8d2-47e3-b209-d3b3ab91c2ce
spec:
  clusterIP: 10.43.127.37
  clusterIPs:
  - 10.43.127.37
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - port: 3306
    protocol: TCP
    targetPort: 3306				# changed from 8080 to 3306
  selector:
    name: mysql
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
-------------------------------------------------
save changes - escape, :wq!, enter

# result: service/mysql-service edited


Verify the service correction
	terminal --> k describe svc 

-------------------------------------------------
Name:                     mysql-service
Namespace:                beta
Labels:                   <none>
Annotations:              <none>
Selector:                 name=mysql
Type:                     ClusterIP
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.43.127.37
IPs:                      10.43.127.37
Port:                     <unset>  3306/TCP
TargetPort:               3306/TCP
Endpoints:                10.42.0.11:3306		# corrected
Session Affinity:         None
Internal Traffic Policy:  Cluster
Events:                   <none>
-------------------------------------------------

Check if the app is running correctly now.

- click 'Check' button




3. Troubleshooting Test 3: The same 2 tier application is deployed in the gamma namespace. It must display a green web page on success. Click on the App tab at the top of your terminal to view your application. It is currently failed or unresponsive. Troubleshoot and fix the issue.
---------------------------------------------------------------------------------------------------------------------
Stick to the given architecture. Use the same names and port numbers as given in the below architecture diagram. Feel free to edit, delete or recreate objects as necessary.


When we open the app we recieve FAILURE with message:
Environment Variables: DB_Host=mysql-service; DB_Database=Not Set; DB_User=root; DB_Password=paswrd; 2003: Can't connect to MySQL server on 'mysql-service:3306' (111 Connection refused)
From webapp-mysql-78fd9544f6-pxbzp!


--------------------------------------------------------
mysql Pod / mysql-service --> webapp pod / webapp service --> Client 
--------------------------------------------------------
			   ^
			   |

From the error we can localize that the problem is between mysql-service and webapp pod, because the app is loading but cannot connect to the DB.

We want to set current context to the specific namespace to avoid typing the namespace param in every command

Show context help commands
	terminal --> kubectl config --help			
# set-context       Set a context entry in kubeconfig
# Usage:
  kubectl config set-context [NAME | --current] [--cluster=cluster_nickname] [--user=user_nickname]
[--namespace=namespace] [options]

Show context config help commands
	terminal --> kubectl config set-context --help		# --namespace='':

Set the specific context (default namespace)
	terminal --> kubectl config set-context --current --namespace=gamma
	
	# kubectl				# common kubernetes command
	# config set-context --current		# modify current default context
	# --namespace=gamma			# set the namespace

	# result: Context "default" modified.



List pods
	terminal --> k get pods

	# result:
	NAME                            READY   STATUS    RESTARTS   AGE
	mysql                           1/1     Running   0          2m5s	# check this pod details
	webapp-mysql-78fd9544f6-pxbzp   1/1     Running   0          2m5s	

List services
	terminal --> k get svc

	# result:
	NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
	mysql-service   ClusterIP   10.43.10.107   <none>        3306/TCP         3m19s
	web-service     NodePort    10.43.185.86   <none>        8080:30081/TCP   3m19s


Print the db service
	terminal --> k describe svc mysql-service

# result:
-------------------------------------------
Name:                     mysql-service
Namespace:                gamma
Labels:                   <none>
Annotations:              <none>
Selector:                 name=sql00001			# the selector is wrong
Type:                     ClusterIP
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.43.197.189
IPs:                      10.43.197.189
Port:                     <unset>  3306/TCP
TargetPort:               3306/TCP
Endpoints:                				# the endpoint is missing
Session Affinity:         None
Internal Traffic Policy:  Cluster
Events:                   <none>
-------------------------------------------

Show deployment details
	terminal --> k describe pod mysql

-------------------------------------------------------------
Name:             mysql
Namespace:        gamma
Priority:         0
Service Account:  default
Node:             controlplane/192.168.243.178
Start Time:       Mon, 17 Feb 2025 14:05:36 +0000
Labels:           name=mysql				# this is the label we need to set in the service
Annotations:      <none>
Status:           Running
IP:               10.42.0.14
IPs:
  IP:  10.42.0.14
Containers:
  mysql:
    Container ID:   containerd://b26d3461a2794ed2db48fc68a952eda6aa96943dfa2cb2f1191c6129cabf3cae
    Image:          mysql:5.6
    Image ID:       docker.io/library/mysql@sha256:20575ecebe6216036d25dab5903808211f1e9ba63dc7825ac20cb975e34cfcae
    Port:           3306/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Mon, 17 Feb 2025 14:05:38 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      MYSQL_ROOT_PASSWORD:  paswrd
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xsbhn (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-api-access-xsbhn:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  15m   default-scheduler  Successfully assigned gamma/mysql to controlplane
  Normal  Pulled     15m   kubelet            Container image "mysql:5.6" already present on machine
  Normal  Created    15m   kubelet            Created container mysql
  Normal  Started    15m   kubelet            Started container mysql
-------------------------------------------------------------


Edit the db service
	terminal --> k edit svc mysql-service

# result:
-------------------------------------------------
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2025-02-17T14:05:36Z"
  name: mysql-service
  namespace: gamma
  resourceVersion: "1274"
  uid: c6024b02-1790-4904-b900-0148549d91b0
spec:
  clusterIP: 10.43.197.189
  clusterIPs:
  - 10.43.197.189
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - port: 3306
    protocol: TCP
    targetPort: 3306
  selector:
    name: mysql				# changed from 'sql00001' to 'mysql'
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
-------------------------------------------------
save changes - escape, :wq!, enter

# result: service/mysql-service edited


Verify service edition
	terminal --> k describe svc mysql-service

-------------------------------------------------
Name:                     mysql-service
Namespace:                gamma
Labels:                   <none>
Annotations:              <none>
Selector:                 name=mysql			# selector is changed
Type:                     ClusterIP
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.43.197.189
IPs:                      10.43.197.189
Port:                     <unset>  3306/TCP
TargetPort:               3306/TCP
Endpoints:                10.42.0.14:3306		# endpoints is linked
Session Affinity:         None
Internal Traffic Policy:  Cluster
Events:                   <none>
-------------------------------------------------

The app should work now.

- click 'Check' button




4. Troubleshooting Test 4: The same 2 tier application is deployed in the delta namespace. It must display a green web page on success. Click on the App tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
---------------------------------------------------------------------------------------------------------------------
Stick to the given architecture. Use the same names and port numbers as given in the below architecture diagram. Feel free to edit, delete or recreate objects as necessary.

DB_Host:mysql-service
DB_User: root
DB_Password: paswd


When we open the app we recieve FAILURE with message:
Environment Variables: DB_Host=mysql-service; DB_Database=Not Set; DB_User=sql-user; DB_Password=paswrd; 1045 (28000): Access denied for user 'sql-user'@'10.42.0.16' (using password: YES)
From webapp-mysql-b9c9f7fbd-rmw6l!


--------------------------------------------------------
mysql Pod / mysql-service --> webapp pod / webapp service --> Client 
--------------------------------------------------------
			  				   ^
			   				   |

From the error we can localize that the problem is between mysql-service and webapp pod, because the app is loading but cannot connect to the DB with the specific  user.


We want to set current context to the specific namespace to avoid typing the namespace param in every command

Show context help commands
	terminal --> kubectl config --help			
# set-context       Set a context entry in kubeconfig
# Usage:
  kubectl config set-context [NAME | --current] [--cluster=cluster_nickname] [--user=user_nickname]
[--namespace=namespace] [options]

Show context config help commands
	terminal --> kubectl config set-context --help		# --namespace='':

Set the specific context (default namespace)
	terminal --> kubectl config set-context --current --namespace=delta
	
	# kubectl				# common kubernetes command
	# config set-context --current		# modify current default context
	# --namespace=delta			# set the namespace

	# result: Context "default" modified.


List pods
	terminal --> k get pods

	# result:
	NAME                           READY   STATUS    RESTARTS   AGE
	mysql                          1/1     Running   0          2m
	webapp-mysql-b9c9f7fbd-rmw6l   1/1     Running   0          2m


List services
	terminal --> k get svc

	# result:
	NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
	mysql-service   ClusterIP   10.43.140.73   <none>        3306/TCP         4m24s
	web-service     NodePort    10.43.43.156   <none>        8080:30081/TCP   4m24s


List deployments
	terminal --> k get deploy

	# result:
	NAME           READY   UP-TO-DATE   AVAILABLE   AGE
	webapp-mysql   1/1     1            1           14m

Show webapp-mysql deployment details
	terminal --> k describe deploy webapp-mysql


# result:
-------------------------------------------------
Name:                   webapp-mysql
Namespace:              delta
CreationTimestamp:      Mon, 17 Feb 2025 14:27:12 +0000
Labels:                 name=webapp-mysql
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               name=webapp-mysql
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=webapp-mysql
  Containers:
   webapp-mysql:
    Image:      mmumshad/simple-webapp-mysql
    Port:       8080/TCP
    Host Port:  0/TCP
    Environment:
      DB_Host:      mysql-service
      DB_User:      sql-user				# wrong user, we need to set it as 'root'
      DB_Password:  paswrd
    Mounts:         <none>
  Volumes:          <none>
  Node-Selectors:   <none>
  Tolerations:      <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   webapp-mysql-b9c9f7fbd (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  14m   deployment-controller  Scaled up replica set webapp-mysql-b9c9f7fbd to 1
-------------------------------------------------



Edit the deployment and fix the user
	terminal --> k edit deploy webapp-mysql

-------------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2025-02-17T14:27:12Z"
  generation: 1
  labels:
    name: webapp-mysql
  name: webapp-mysql
  namespace: delta
  resourceVersion: "1484"
  uid: e2bb6da9-239a-41f3-a47a-94d237b69784
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: webapp-mysql
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: webapp-mysql
      name: webapp-mysql
    spec:
      containers:
      - env:
        - name: DB_Host
          value: mysql-service
        - name: DB_User
          value: root				# changed to root
        - name: DB_Password
          value: paswrd
        image: mmumshad/simple-webapp-mysql
        imagePullPolicy: Always
        name: webapp-mysql
        ports:
        - containerPort: 8080
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2025-02-17T14:27:14Z"
    lastUpdateTime: "2025-02-17T14:27:14Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2025-02-17T14:27:12Z"
    lastUpdateTime: "2025-02-17T14:27:14Z"
    message: ReplicaSet "webapp-mysql-b9c9f7fbd" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1
-------------------------------------------------
save changes - escape, :wq!, enter

# result: deployment.apps/webapp-mysql edited


The app should work now.

- click 'Check' button




5. Troubleshooting Test 5: The same 2 tier application is deployed in the epsilon namespace. It must display a green web page on success. Click on the App tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
------------------------------------------------------------------------------------------------------------------
Stick to the given architecture. Use the same names and port numbers as given in the below architecture diagram. Feel free to edit, delete or recreate objects as necessary.


DB_Host:mysql-service
DB_User: root
DB_Password: paswd


When we open the app we recieve FAILURE with message:
Environment Variables: DB_Host=mysql-service; DB_Database=Not Set; DB_User=sql-user; DB_Password=paswrd; 1045 (28000): Access denied for user 'sql-user'@'10.42.0.19' (using password: YES)
From webapp-mysql-b9c9f7fbd-bsnfb!


--------------------------------------------------------
mysql Pod / mysql-service --> webapp pod / webapp service --> Client 
--------------------------------------------------------
			  				   ^
			   				   |

From the error we can localize that the problem is between mysql-service and webapp pod, because the app is loading but cannot connect to the DB with the specific  user.


We want to set current context to the specific namespace to avoid typing the namespace param in every command

Show context help commands
	terminal --> kubectl config --help			
# set-context       Set a context entry in kubeconfig
# Usage:
  kubectl config set-context [NAME | --current] [--cluster=cluster_nickname] [--user=user_nickname]
[--namespace=namespace] [options]

Show context config help commands
	terminal --> kubectl config set-context --help		# --namespace='':

Set the specific context (default namespace)
	terminal --> kubectl config set-context --current --namespace=epsilon
	
	# kubectl				# common kubernetes command
	# config set-context --current		# modify current default context
	# --namespace=epsilon			# set the namespace

	# result: Context "default" modified.


List pods
	terminal --> k get pods

	# result:
	NAME                           READY   STATUS    RESTARTS   AGE
	mysql                          1/1     Running   0          2m
	webapp-mysql-b9c9f7fbd-rmw6l   1/1     Running   0          2m


List services
	terminal --> k get svc

	# result:
	NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
	mysql-service   ClusterIP   10.43.140.73   <none>        3306/TCP         4m24s
	web-service     NodePort    10.43.43.156   <none>        8080:30081/TCP   4m24s


List deployments
	terminal --> k get deploy

	# result:
	NAME           READY   UP-TO-DATE   AVAILABLE   AGE
	webapp-mysql   1/1     1            1           107s

Show webapp-mysql deployment details
	terminal --> k describe deploy webapp-mysql


# result:
-------------------------------------------------
Name:                   webapp-mysql
Namespace:              epsilon
CreationTimestamp:      Mon, 17 Feb 2025 14:47:32 +0000
Labels:                 name=webapp-mysql
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               name=webapp-mysql
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=webapp-mysql
  Containers:
   webapp-mysql:
    Image:      mmumshad/simple-webapp-mysql
    Port:       8080/TCP
    Host Port:  0/TCP
    Environment:
      DB_Host:      mysql-service
      DB_User:      sql-user				# the user is wrong, we need to set it to root
      DB_Password:  paswrd
    Mounts:         <none>
  Volumes:          <none>
  Node-Selectors:   <none>
  Tolerations:      <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   webapp-mysql-b9c9f7fbd (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  118s  deployment-controller  Scaled up replica set webapp-mysql-b9c9f7fbd to 1
-------------------------------------------------


Edit the deployment and fix the user
	terminal --> k edit deploy webapp-mysql

-------------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2025-02-17T14:47:32Z"
  generation: 1
  labels:
    name: webapp-mysql
  name: webapp-mysql
  namespace: epsilon
  resourceVersion: "1983"
  uid: d5af743a-c04d-4849-a9d1-fca57371cfc5
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: webapp-mysql
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: webapp-mysql
      name: webapp-mysql
    spec:
      containers:
      - env:
        - name: DB_Host
          value: mysql-service
        - name: DB_User	
          value: root					# changed to root
        - name: DB_Password
          value: paswrd
        image: mmumshad/simple-webapp-mysql
        imagePullPolicy: Always
        name: webapp-mysql
        ports:
        - containerPort: 8080
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2025-02-17T14:47:33Z"
    lastUpdateTime: "2025-02-17T14:47:33Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2025-02-17T14:47:32Z"
    lastUpdateTime: "2025-02-17T14:47:33Z"
    message: ReplicaSet "webapp-mysql-b9c9f7fbd" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1
-------------------------------------------------
save changes - escape, :wq!, enter

# result: deployment.apps/webapp-mysql edited


Check the app and see the error message:
Environment Variables: DB_Host=mysql-service; DB_Database=Not Set; DB_User=root; DB_Password=paswrd; 1045 (28000): Access denied for user 'root'@'10.42.0.20' (using password: YES)
From webapp-mysql-78fd9544f6-cmdbf!


So now we fixed the issue from the webapp side and we need to check the issue from the DB side


Show DB pod details
	terminal --> k describe pod mysql

# result:
-------------------------------------------------
Name:             mysql
Namespace:        epsilon
Priority:         0
Service Account:  default
Node:             controlplane/192.168.243.178
Start Time:       Mon, 17 Feb 2025 14:47:32 +0000
Labels:           name=mysql
Annotations:      <none>
Status:           Running
IP:               10.42.0.18
IPs:
  IP:  10.42.0.18
Containers:
  mysql:
    Container ID:   containerd://e31e16d274315aa07e5455e60fc26bd17ed2427015ca448c5d009a57b01fbf81
    Image:          mysql:5.6
    Image ID:       docker.io/library/mysql@sha256:20575ecebe6216036d25dab5903808211f1e9ba63dc7825ac20cb975e34cfcae
    Port:           3306/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Mon, 17 Feb 2025 14:47:33 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      MYSQL_ROOT_PASSWORD:  passwooooorrddd						# wrong password
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gc699 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-api-access-gc699:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  8m39s  default-scheduler  Successfully assigned epsilon/mysql to controlplane
  Normal  Pulled     8m39s  kubelet            Container image "mysql:5.6" already present on machine
  Normal  Created    8m39s  kubelet            Created container mysql
  Normal  Started    8m38s  kubelet            Started container mysql
-------------------------------------------------


Edit the pod configuration and change the wrong password
	terminal --> k edit pod mysql

-------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2025-02-17T14:47:32Z"
  labels:
    name: mysql
  name: mysql
  namespace: epsilon
  resourceVersion: "1982"
  uid: dc730c39-743c-431b-90f0-2118f372066a
spec:
  containers:
  - env:
    - name: MYSQL_ROOT_PASSWORD
      value: paswrd				# changed from 'passwooooorrddd' to 'paswrd'
    image: mysql:5.6
    imagePullPolicy: IfNotPresent
    name: mysql
    ports:
    - containerPort: 3306
      protocol: TCP
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-gc699
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: controlplane
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-gc699
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2025-02-17T14:47:33Z"
    status: "True"
    type: PodReadyToStartContainers
  - lastProbeTime: null
    lastTransitionTime: "2025-02-17T14:47:32Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2025-02-17T14:47:33Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2025-02-17T14:47:33Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2025-02-17T14:47:32Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: containerd://e31e16d274315aa07e5455e60fc26bd17ed2427015ca448c5d009a57b01fbf81
    image: docker.io/library/mysql:5.6
    imageID: docker.io/library/mysql@sha256:20575ecebe6216036d25dab5903808211f1e9ba63dc7825ac20cb975e34cfcae
    lastState: {}
    name: mysql
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2025-02-17T14:47:33Z"
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-gc699
      readOnly: true
      recursiveReadOnly: Disabled
  hostIP: 192.168.243.178
  hostIPs:
  - ip: 192.168.243.178
  phase: Running
  podIP: 10.42.0.18
  podIPs:
  - ip: 10.42.0.18
  qosClass: BestEffort
  startTime: "2025-02-17T14:47:32Z"
-------------------------------------------------
save changes - escape, :wq!, enter
escape error screen - :q!, enter

received message:
error: pods "mysql" is invalid
A copy of your changes has been stored to "/tmp/kubectl-edit-1474650192.yaml"
error: Edit cancelled, no valid changes were saved.

Replace the pod with changes
	terminal --> k replace --force -f /tmp/kubectl-edit-1474650192.yaml

	# result:
	pod "mysql" deleted
	pod/mysql replaced

Wait few minutes for to pod to be recreated

if there are configmaps nvolverd (as usual) we need to go over the configmaps in the troubleshooting process.

The app should work now.

- click 'Check' button





6. Troubleshooting Test 6: The same 2 tier application is deployed in the zeta namespace. It must display a green web page on success. Click on the App tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
--------------------------------------------------------------------------------------------------------------
Stick to the given architecture. Use the same names and port numbers as given in the below architecture diagram. Feel free to edit, delete or recreate objects as necessary.


DB_Host:mysql-service
DB_User: root
DB_Password: paswd


When we open the app we recieve FAILURE with message:
502 Bad Gateway


--------------------------------------------------------
mysql Pod / mysql-service --> webapp pod / webapp service --> Client 
--------------------------------------------------------
			  				   ^
			   				   |

We want to set current context to the specific namespace to avoid typing the namespace param in every command

Show context help commands
	terminal --> kubectl config --help			
# set-context       Set a context entry in kubeconfig
# Usage:
  kubectl config set-context [NAME | --current] [--cluster=cluster_nickname] [--user=user_nickname]
[--namespace=namespace] [options]

Show context config help commands
	terminal --> kubectl config set-context --help		# --namespace='':

Set the specific context (default namespace)
	terminal --> kubectl config set-context --current --namespace=zeta
	
	# kubectl				# common kubernetes command
	# config set-context --current		# modify current default context
	# --namespace=zeta			# set the namespace

	# result: Context "default" modified.


List pods
	terminal --> k get pods

	# result:
	NAME                           READY   STATUS    RESTARTS   AGE
	mysql                          1/1     Running   0          3m17s
	webapp-mysql-b9c9f7fbd-gcd6m   1/1     Running   0          3m17s


List services
	terminal --> k get svc

	# result:
	NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
	mysql-service   ClusterIP   10.43.204.218   <none>        3306/TCP         4m11s
	web-service     NodePort    10.43.128.56    <none>        8080:30088/TCP   4m11s	# wrong port 30088 must be 30081


Edit the service
	terminal --> k edit svc web-service

-------------------------------------------------
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2025-02-17T15:15:58Z"
  name: web-service
  namespace: zeta
  resourceVersion: "1534"
  uid: 43ea6b37-62a9-4d64-a436-a6069c1aa345
spec:
  clusterIP: 10.43.128.56
  clusterIPs:
  - 10.43.128.56
  externalTrafficPolicy: Cluster
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - nodePort: 30081				# changed from 30088 to 30081
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    name: webapp-mysql
  sessionAffinity: None
  type: NodePort
status:
  loadBalancer: {}
-------------------------------------------------
save changes - escape, :wq!, enter

@ result: service/web-service edited


Now when we open tha pp we receive error message:
Environment Variables: DB_Host=mysql-service; DB_Database=Not Set; DB_User=sql-user; DB_Password=paswrd; 1045 (28000): Access denied for user 'sql-user'@'10.42.0.23' (using password: YES)
From webapp-mysql-b9c9f7fbd-gcd6m!


List deployments
	terminal --> k get deploy

	# result:
	NAME           READY   UP-TO-DATE   AVAILABLE   AGE
	webapp-mysql   1/1     1            1           8m9s

Show details for the deployment
	terminal --> k describe deploy webapp-mysql

-------------------------------------------------
Name:                   webapp-mysql
Namespace:              zeta
CreationTimestamp:      Mon, 17 Feb 2025 15:15:58 +0000
Labels:                 name=webapp-mysql
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               name=webapp-mysql
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=webapp-mysql
  Containers:
   webapp-mysql:
    Image:      mmumshad/simple-webapp-mysql
    Port:       8080/TCP
    Host Port:  0/TCP
    Environment:
      DB_Host:      mysql-service
      DB_User:      sql-user				# wrong user, we have to fix it with 'root'
      DB_Password:  paswrd
    Mounts:         <none>
  Volumes:          <none>
  Node-Selectors:   <none>
  Tolerations:      <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   webapp-mysql-b9c9f7fbd (1/1 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  8m32s  deployment-controller  Scaled up replica set webapp-mysql-b9c9f7fbd to 1
-------------------------------------------------


Edit the deployment
	terminal --> k edit deploy webapp-mysql

-------------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "2"
  creationTimestamp: "2025-02-17T15:15:58Z"
  generation: 2
  labels:
    name: webapp-mysql
  name: webapp-mysql
  namespace: zeta
  resourceVersion: "1778"
  uid: a17f1fe2-ef96-4a2f-be1b-971eef7fc327
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: webapp-mysql
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: webapp-mysql
      name: webapp-mysql
    spec:
      containers:
      - env:
        - name: DB_Host
          value: mysql-service
        - name: DB_User
          value: root					# changed to 'root'
        - name: DB_Password
          value: paswrd
        image: mmumshad/simple-webapp-mysql
        imagePullPolicy: Always
        name: webapp-mysql
        ports:
        - containerPort: 8080
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2025-02-17T15:15:59Z"
    lastUpdateTime: "2025-02-17T15:15:59Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2025-02-17T15:15:58Z"
    lastUpdateTime: "2025-02-17T15:26:27Z"
    message: ReplicaSet "webapp-mysql-78fd9544f6" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 2
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1
-------------------------------------------------
save changes - escape, :wq!, enter

# result: deployment.apps/webapp-mysql edited


Try to open the app.
We receive error message:
Environment Variables: DB_Host=mysql-service; DB_Database=Not Set; DB_User=root; DB_Password=paswrd; 1045 (28000): Access denied for user 'root'@'10.42.0.25' (using password: YES)
From webapp-mysql-78fd9544f6-ngwjm!


Look over the DB pod for wrong params

Show DB pod details
	terminal --> k describe pod mysql

# result:
-------------------------------------------------
Name:             mysql
Namespace:        zeta
Priority:         0
Service Account:  default
Node:             controlplane/192.168.20.169
Start Time:       Mon, 17 Feb 2025 15:15:58 +0000
Labels:           name=mysql
Annotations:      <none>
Status:           Running
IP:               10.42.0.24
IPs:
  IP:  10.42.0.24
Containers:
  mysql:
    Container ID:   containerd://91e5147281ce76cb9d11660fa0a78f1b09f5a190c9926b8bf83e05abc258b706
    Image:          mysql:5.6
    Image ID:       docker.io/library/mysql@sha256:20575ecebe6216036d25dab5903808211f1e9ba63dc7825ac20cb975e34cfcae
    Port:           3306/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Mon, 17 Feb 2025 15:15:59 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      MYSQL_ROOT_PASSWORD:  passwooooorrddd						# wrong password
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-97t2c (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-api-access-97t2c:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  14m   default-scheduler  Successfully assigned zeta/mysql to controlplane
  Normal  Pulled     14m   kubelet            Container image "mysql:5.6" already present on machine
  Normal  Created    14m   kubelet            Created container mysql
  Normal  Started    14m   kubelet            Started container mysql
-------------------------------------------------



Edit the pod configuration and change the wrong password
	terminal --> k edit pod mysql

-------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2025-02-17T14:47:32Z"
  labels:
    name: mysql
  name: mysql
  namespace: epsilon
  resourceVersion: "1982"
  uid: dc730c39-743c-431b-90f0-2118f372066a
spec:
  containers:
  - env:
    - name: MYSQL_ROOT_PASSWORD
      value: paswrd				# changed from 'passwooooorrddd' to 'paswrd'
    image: mysql:5.6
    imagePullPolicy: IfNotPresent
    name: mysql
    ports:
    - containerPort: 3306
      protocol: TCP
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-gc699
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: controlplane
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-gc699
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2025-02-17T14:47:33Z"
    status: "True"
    type: PodReadyToStartContainers
  - lastProbeTime: null
    lastTransitionTime: "2025-02-17T14:47:32Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2025-02-17T14:47:33Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2025-02-17T14:47:33Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2025-02-17T14:47:32Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: containerd://e31e16d274315aa07e5455e60fc26bd17ed2427015ca448c5d009a57b01fbf81
    image: docker.io/library/mysql:5.6
    imageID: docker.io/library/mysql@sha256:20575ecebe6216036d25dab5903808211f1e9ba63dc7825ac20cb975e34cfcae
    lastState: {}
    name: mysql
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2025-02-17T14:47:33Z"
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-gc699
      readOnly: true
      recursiveReadOnly: Disabled
  hostIP: 192.168.243.178
  hostIPs:
  - ip: 192.168.243.178
  phase: Running
  podIP: 10.42.0.18
  podIPs:
  - ip: 10.42.0.18
  qosClass: BestEffort
  startTime: "2025-02-17T14:47:32Z"
-------------------------------------------------
save changes - escape, :wq!, enter
escape error screen - :q!, enter

received message:
error: pods "mysql" is invalid
A copy of your changes has been stored to "/tmp/kubectl-edit-3384663823.yaml"
error: Edit cancelled, no valid changes were saved.

Replace the pod with changes
	terminal --> k replace --force -f /tmp/kubectl-edit-3384663823.yaml

	# result:
	pod "mysql" deleted
	pod/mysql replaced

Wait few minutes for to pod to be recreated

The app should work now.

- click 'Check' button







=====================================
Section 15 297. Control Plane Failure
=====================================

Few steps to locate the failure of the controlplane

1. Check the status of the nodes in the cluster and check if they are all healthy
	terminal --> kubectl get nodes

	# result:
	NAME		STATUS 		ROLES 		AGE 		VERSION
	worker-1 	Ready 		<none> 		8d 		v1.13.0 
	worker-2 	Ready 		<none> 		8d 		v1.13.0


2. Check Pods and theyr status
	terminal --> kubectl get pods

	# result:
	NAME 		READY 		STATUS 		RESTARTS 	AGE
	mysql 		1/1 		Running 	0 		113m
	webapp-mysql 	1/1 		Running 	0 		113m



In case the cluster is deployed with kubeadm tool and the controlplane is a pod, we need to find its status

3. Check controlplane Pods
	terminal --> kubectl get pods -n kube-system

# result:
NAME 				READY 		STATUS 			RESTARTS 	AGE 
coredns-78fcdf6894-5dntv 	1/1 		Running 		0 		1h
coredns-78fcdf6894-knpzl 	1/1 		Running 		0 		1h
etcd-master 			1/1 		Running 		0 		1h
kube-apiserver-master 		1/1 		Running 		0 		1h
kube-controller-manager-master 	1/1 		Running 		0 		1h
kube-proxy-fvbpj 		1/1 		Running 		0 		1h
kube-proxy-v5r2t 		1/1 		Running 		0 		1h
kube-scheduler-master 		1/1 		Running 		0 		1h
weave-net-7kd52 		2/2 		Running 		1 		1h
weave-net-jtl5m 		2/2 		Running 		1 		1h



If controlplane components are deployed as services we need to check them

4.1 Show service for api server
	terminal --> service kube-apiserver status

# result:
● kube-apiserver.service - Kubernetes API Server
Loaded: loaded (/etc/systemd/system/kube-apiserver.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2019-03-20 07:57:25 UTC; 1 weeks 1 days ago
Docs: https://github.com/kubernetes/kubernetes Main PID: 15767 (kube-apiserver)
Tasks: 13 (limit: 2362)



4.2 Show service for controller manager
	terminal --> service kube-controller-manager status

# result:
● kube-controller-manager.service - Kubernetes Controller Manager
Loaded: loaded (/etc/systemd/system/kube-controller-manager.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2019-03-20 07:57:25 UTC; 1 weeks 1 days ago
Docs: https://github.com/kubernetes/kubernetes
Main PID: 15771 (kube-controller) Tasks: 10 (limit: 2362)


4.3 Show service for scheduler
	terminal --> service kube-scheduler status

# result:
● kube-scheduler.service - Kubernetes Scheduler
Loaded: loaded (/etc/systemd/system/kube-scheduler.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2019-03-29 01:45:32 UTC; 11min ago
Docs: https://github.com/kubernetes/kubernetes Main PID: 28390 (kube-scheduler)
Tasks: 10 (limit: 2362)



4.4 Show service for kubelet
	terminal --> service kubelet status

# result:
● kubelet.service - Kubernetes Kubelet
Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2019-03-20 14:22:06 UTC; 1 weeks 1 days ago
Docs: https://github.com/kubernetes/kubernetes Main PID: 1281 (kubelet)
Tasks: 24 (limit: 1152)



4.5 Show service for proxy 
	terminal --> service kube-proxy status

# result:
● kube-proxy.service - Kubernetes Kube Proxy
Loaded: loaded (/etc/systemd/system/kube-proxy.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2019-03-20 14:21:54 UTC; 1 weeks 1 days ago
Docs: https://github.com/kubernetes/kubernetes
Main PID: 794 (kube-proxy)



Next we can check the logs of the controlplane components.

In case the cluster is deployed with kubeadm

5.1 Check logs master api server 
	terminal --> kubectl logs kube-apiserver-master -n kube-system

# result:
I0401 13:45:38.190735 1 server.go:703] external host was not specified, using 172.17.0.117 I0401 13:45:38.194290 1 server.go:145] Version: v1.11.3
I0401 13:45:38.819705 1 plugins.go:158] Loaded 8 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,Priority,DefaultTolerationSeconds,DefaultStorageClass,MutatingAdmissionWebhook. I0401 13:45:38.819741 1 plugins.go:161] Loaded 6 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,Priority,PersistentVolumeClaimResize,ValidatingAdmissionWebhook,ResourceQuota.
I0401 13:45:38.821372 1 plugins.go:158] Loaded 8 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,Priority,DefaultTolerationSeconds,DefaultStorageClass,MutatingAdmissionWebhook. I0401 13:45:38.821410 1 plugins.go:161] Loaded 6 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,Priority,PersistentVolumeClaimResize,ValidatingAdmissionWebhook,ResourceQuota.
I0401 13:45:38.985453 1 master.go:234] Using reconciler: lease
W0401 13:45:40.900380 1 genericapiserver.go:319] Skipping API batch/v2alpha1 because it has no resources.
W0401 13:45:41.370677 1 genericapiserver.go:319] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources. W0401 13:45:41.381736 1 genericapiserver.go:319] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.



In case of cintroplane components are deployed nativelly as services, we can use journalctl utility

5.2 Show logs for master api server
	terminal --> sudo journalctl -u kube-apiserver

# result:
Mar 20 07:57:25 master-1 systemd[1]: Started Kubernetes API Server.
Mar 20 07:57:25 master-1 kube-apiserver[15767]: I0320 07:57:25.553377 15767 flags.go:33] FLAG: --address="127.0.0.1"
Mar 20 07:57:25 master-1 kube-apiserver[15767]: I0320 07:57:25.558273 15767 flags.go:33] FLAG: --admission-control="[]"
Mar 20 07:57:25 master-1 kube-apiserver[15767]: I0320 07:57:25.558325 15767 flags.go:33] FLAG: --admission-control-config-file=""
Mar 20 07:57:25 master-1 kube-apiserver[15767]: I0320 07:57:25.558339 15767 flags.go:33] FLAG: --advertise-address="192.168.5.11"
Mar 20 07:57:25 master-1 kube-apiserver[15767]: I0320 07:57:25.558353 15767 flags.go:33] FLAG: --allow-privileged="true"
Mar 20 07:57:25 master-1 kube-apiserver[15767]: I0320 07:57:25.558365 15767 flags.go:33] FLAG: --alsologtostderr="false"
Mar 20 07:57:25 master-1 kube-apiserver[15767]: I0320 07:57:25.558413 15767 flags.go:33] FLAG: --anonymous-auth="true"
Mar 20 07:57:25 master-1 kube-apiserver[15767]: I0320 07:57:25.558425 15767 flags.go:33] FLAG: --api-audiences="[]"
Mar 20 07:57:25 master-1 kube-apiserver[15767]: I0320 07:57:25.558442 15767 flags.go:33] FLAG: --apiserver-count="3"
Mar 20 07:57:25 master-1 kube-apiserver[15767]: I0320 07:57:25.558454 15767 flags.go:33] FLAG: --audit-dynamic-configuration="false"
Mar 20 07:57:25 master-1 kube-apiserver[15767]: I0320 07:57:25.558464 15767 flags.go:33] FLAG: --audit-log-batch-buffer-size="10000"
Mar 20 07:57:25 master-1 kube-apiserver[15767]: I0320 07:57:25.558474 15767 flags.go:33] FLAG: --audit-log-batch-max-size="1"
Mar 20 07:57:25 master-1 kube-apiserver[15767]: I0320 07:57:25.558484 15767 flags.go:33] FLAG: --audit-log-batch-max-wait="0s"
Mar 20 07:57:25 master-1 kube-apiserver[15767]: I0320 07:57:25.558495 15767 flags.go:33] FLAG: --audit-log-batch-throttle-burst="0"
Mar 20 07:57:25 master-1 kube-apiserver[15767]: I0320 07:57:25.558504 15767 flags.go:33] FLAG: --audit-log-batch-throttle-enable="false"
Mar 20 07:57:25 master-1 kube-apiserver[15767]: I0320 07:57:25.558514 15767 flags.go:33] FLAG: --audit-log-batch-throttle-qps="0"
Mar 20 07:57:25 master-1 kube-apiserver[15767]: I0320 07:57:25.558528 15767 flags.go:33] FLAG: --audit-log-format="json"


More info on troubleshooting cluster
	- https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/




=====================================================
Section 15 299. Practice Test - Control Plane Failure
=====================================================

Set kubectl auto completion feature
	terminal --> source <(kubectl completion bash)

Set alias for short syntax of kubectl
	terminal --> alias k=kubectl
	terminal --> complete -F __start_kubectl k


1. The cluster is broken. We tried deploying an application but it's not working. Troubleshoot and fix the issue.
-----------------------------------------------------------------------------------------------------------------
Start looking at the deployments.


List nodes
	terminal --> k get nodes

	# result:
	NAME           STATUS   ROLES           AGE     VERSION
	controlplane   Ready    control-plane   4m50s   v1.31.0


List deployments
	terminal --> k get deploy

	# result:
	NAME   READY   UP-TO-DATE   AVAILABLE   AGE
	app    0/1     1            0           4m37s		# the deployment is in not ready state


Show details for deployment
	terminal --> k describe deploy app

------------------------------------------
Name:                   app
Namespace:              default
CreationTimestamp:      Mon, 17 Feb 2025 16:01:35 +0000
Labels:                 app=app
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=app
Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=app
  Containers:
   nginx:
    Image:         nginx:alpine
    Port:          <none>
    Host Port:     <none>
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      False   MinimumReplicasUnavailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  <none>
NewReplicaSet:   app-776bb5c68f (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  6m9s  deployment-controller  Scaled up replica set app-776bb5c68f to 1
------------------------------------------


List replicasets
	terminal --> k get rs

	# result:
	NAME             DESIRED   CURRENT   READY   AGE
	app-776bb5c68f   1         1         0       15m


Show replicaset details
	terminal --> k describe rs app-776bb5c68f

------------------------------------------
Name:           app-776bb5c68f
Namespace:      default
Selector:       app=app,pod-template-hash=776bb5c68f
Labels:         app=app
                pod-template-hash=776bb5c68f
Annotations:    deployment.kubernetes.io/desired-replicas: 1
                deployment.kubernetes.io/max-replicas: 2
                deployment.kubernetes.io/revision: 1
Controlled By:  Deployment/app
Replicas:       1 current / 1 desired
Pods Status:    0 Running / 1 Waiting / 0 Succeeded / 0 Failed			# we have status 'Waiting'
Pod Template:
  Labels:  app=app
           pod-template-hash=776bb5c68f
  Containers:
   nginx:
    Image:         nginx:alpine
    Port:          <none>
    Host Port:     <none>
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  15m   replicaset-controller  Created pod: app-776bb5c68f-b8ksd
------------------------------------------


List pods
	terminal --> k get pods

	# result:
	NAME                   READY   STATUS    RESTARTS   AGE
	app-776bb5c68f-b8ksd   0/1     Pending   0          16m		# the status is 'Pending'


Show details for the pod
	terminal --> k describe pod app-776bb5c68f-b8ksd

------------------------------------------
Name:             app-776bb5c68f-b8ksd
Namespace:        default
Priority:         0
Service Account:  default
Node:             <none>				# Node is not assigned
Labels:           app=app
                  pod-template-hash=776bb5c68f
Annotations:      <none>
Status:           Pending
IP:               
IPs:              <none>
Controlled By:    ReplicaSet/app-776bb5c68f
Containers:
  nginx:
    Image:        nginx:alpine
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h2wmd (ro)
Volumes:
  kube-api-access-h2wmd:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>
------------------------------------------
# no events, just pending state
# the pod is not assigned to any Node
# assigning pods to Nodes it responsibility of the scheduler


List all pods in the default namespace (all controlplane components)
	terminal --> k get pods -n kube-system

# result:
NAME                                   READY   STATUS             RESTARTS        AGE
coredns-77d6fd4654-64nh2               1/1     Running            0               20m
coredns-77d6fd4654-cp7t2               1/1     Running            0               20m
etcd-controlplane                      1/1     Running            0               21m
kube-apiserver-controlplane            1/1     Running            0               21m
kube-controller-manager-controlplane   1/1     Running            0               21m
kube-proxy-ldgk2                       1/1     Running            0               20m
kube-scheduler-controlplane            0/1     CrashLoopBackOff   8 (4m33s ago)   20m		# scheduler is not running


Show details for the scheduler
	terminal --> k describe pod kube-scheduler-controlplane -n kube-system

------------------------------------------
Name:                 kube-scheduler-controlplane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 controlplane/192.168.75.133
Start Time:           Mon, 17 Feb 2025 16:01:01 +0000
Labels:               component=kube-scheduler
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 3aa60fbba62f9faa79076d6a3f6cb9ba
                      kubernetes.io/config.mirror: 3aa60fbba62f9faa79076d6a3f6cb9ba
                      kubernetes.io/config.seen: 2025-02-17T16:01:23.187554327Z
                      kubernetes.io/config.source: file
Status:               Running
SeccompProfile:       RuntimeDefault
IP:                   192.168.75.133
IPs:
  IP:           192.168.75.133
Controlled By:  Node/controlplane
Containers:
  kube-scheduler:
    Container ID:  containerd://c4fbc7161be86e96a71218752bea8c0a78281a124c17002e9bc392ada4d35f3d
    Image:         registry.k8s.io/kube-scheduler:v1.31.0
    Image ID:      registry.k8s.io/kube-scheduler@sha256:96ddae9c9b2e79342e0551e2d2ec422c0c02629a74d928924aaa069706619808
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-schedulerrrr
      --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       StartError
      Message:      failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: "kube-schedulerrrr": executable file not found in $PATH: unknown
      Exit Code:    128
      Started:      Thu, 01 Jan 1970 00:00:00 +0000
      Finished:     Mon, 17 Feb 2025 16:22:32 +0000
    Ready:          False
    Restart Count:  9
    Requests:
      cpu:        100m
    Liveness:     http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10259/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason   Age                  From     Message
  ----     ------   ----                 ----     -------
  Warning  Failed   21m (x4 over 21m)    kubelet  Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: "kube-schedulerrrr": executable file not found in $PATH: unknown
  Normal   Pulled   20m (x5 over 21m)    kubelet  Container image "registry.k8s.io/kube-scheduler:v1.31.0" already present on machine
  Normal   Created  20m (x5 over 21m)    kubelet  Created container kube-scheduler
  Warning  BackOff  89s (x102 over 21m)  kubelet  Back-off restarting failed container kube-scheduler in pod kube-scheduler-controlplane_kube-system(3aa60fbba62f9faa79076d6a3f6cb9ba)
------------------------------------------

We can see in the error that the command is with wrong name - "kube-schedulerrrr"

After we know that the scheduler pod is a static pod, we know that the definition file is in /etc/kubernetes/manifests/

Print scheduler definition file
	terminal --> cat /etc/kubernetes/manifests/kube-scheduler.yaml

Edit the kube-scheduler definition file and fix the command
	terminal --> vi /etc/kubernetes/manifests/kube-scheduler.yaml

-------------------------------------------
spec:
  containers:
  - command:
    - kube-scheduler							# fixed from "kube-schedulerrrr"
    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
-------------------------------------------
save changes - escape, :wq!, enter

Wait few minutes for changes to take place.


List pods in the default namespace
	terminal --> k get pods -n kube-system

# result:
NAME                                   READY   STATUS    RESTARTS   AGE
coredns-77d6fd4654-64nh2               1/1     Running   0          30m
coredns-77d6fd4654-cp7t2               1/1     Running   0          30m
etcd-controlplane                      1/1     Running   0          30m
kube-apiserver-controlplane            1/1     Running   0          30m
kube-controller-manager-controlplane   1/1     Running   0          30m
kube-proxy-ldgk2                       1/1     Running   0          30m
kube-scheduler-controlplane            1/1     Running   0          117s


- click 'Check' button





2. Scale the deployment app to 2 pods.
--------------------------------------

List deployment
	terminal --> kubectl get deploy

	# result:
	NAME   READY   UP-TO-DATE   AVAILABLE   AGE
	app    1/1     1            1           31m


Scale the app to 2 pods
	terminal --> kubectl scale deploy app --replicas=2

	# result: deployment.apps/app scaled


- click 'Check' button






3. Even though the deployment was scaled to 2, the number of PODs does not seem to increase. Investigate and fix the issue.
---------------------------------------------------------------------------------------------------------------------------
Inspect the component responsible for managing deployments and replicasets.


List deployments
	terminal --> k get deploy

	# result:
	NAME   READY   UP-TO-DATE   AVAILABLE   AGE
	app    1/2     1            1           40m		# we have 1/2 so one of the pods is not running

Show deplyment details
	terminal --> k describe deploy app

-----------------------------------------------------
Name:                   app
Namespace:              default
CreationTimestamp:      Mon, 17 Feb 2025 16:01:35 +0000
Labels:                 app=app
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=app
Replicas:               2 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=app
  Containers:
   nginx:
    Image:         nginx:alpine
    Port:          <none>
    Host Port:     <none>
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   app-776bb5c68f (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  41m   deployment-controller  Scaled up replica set app-776bb5c68f to 1
-----------------------------------------------------
We can see that the replicaset is scaled to 1
Scaling a replicaset is responsibility of controller manager



List pods in the default namespace (all controlplane components)
	terminal --> k get pods -n kube-system

# result:
NAME                                   READY   STATUS             RESTARTS        AGE
coredns-77d6fd4654-64nh2               1/1     Running            0               45m
coredns-77d6fd4654-cp7t2               1/1     Running            0               45m
etcd-controlplane                      1/1     Running            0               45m
kube-apiserver-controlplane            1/1     Running            0               45m
kube-controller-manager-controlplane   0/1     CrashLoopBackOff   7 (2m57s ago)   13m	# controller manager is failure
kube-proxy-ldgk2                       1/1     Running            0               45m
kube-scheduler-controlplane            1/1     Running            0               16m



Show details for the controller manager pod
	terminal --> k describe pod kube-controller-manager-controlplane -n kube-system

----------------------------------------------------------
Name:                 kube-controller-manager-controlplane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 controlplane/192.168.75.133
Start Time:           Mon, 17 Feb 2025 16:01:01 +0000
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: e0bd52188a77797b8345eee3521749a1
                      kubernetes.io/config.mirror: e0bd52188a77797b8345eee3521749a1
                      kubernetes.io/config.seen: 2025-02-17T16:32:30.748987017Z
                      kubernetes.io/config.source: file
Status:               Running
SeccompProfile:       RuntimeDefault
IP:                   192.168.75.133
IPs:
  IP:           192.168.75.133
Controlled By:  Node/controlplane
Containers:
  kube-controller-manager:
    Container ID:  containerd://56a63417bdddcbc6659a12b54939b22a8cbc771d0a33b61eb1e7427e5a5b844c
    Image:         registry.k8s.io/kube-controller-manager:v1.31.0
    Image ID:      registry.k8s.io/kube-controller-manager@sha256:f6f3c33dda209e8434b83dacf5244c03b59b0018d93325ff21296a142b68497d
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=172.17.0.0/16
      --cluster-name=kubernetes
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --kubeconfig=/etc/kubernetes/controller-manager-XXXX.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=172.20.0.0/16
      --use-service-account-credentials=true
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Mon, 17 Feb 2025 16:43:34 +0000
      Finished:     Mon, 17 Feb 2025 16:43:35 +0000
    Ready:          False
    Restart Count:  7
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason   Age                   From     Message
  ----     ------   ----                  ----     -------
  Normal   Started  14m (x4 over 14m)     kubelet  Started container kube-controller-manager
  Normal   Pulled   13m (x5 over 14m)     kubelet  Container image "registry.k8s.io/kube-controller-manager:v1.31.0" already present on machine
  Normal   Created  13m (x5 over 14m)     kubelet  Created container kube-controller-manager
  Warning  BackOff  4m41s (x55 over 14m)  kubelet  Back-off restarting failed container kube-controller-manager in pod kube-controller-manager-controlplane_kube-system(e0bd52188a77797b8345eee3521749a1)
----------------------------------------------------------
No useful information in the description of the pod



Show the logs of the pod
	terminal --> k logs kube-controller-manager-controlplane -n kube-system

# result:
I0217 16:48:50.229072       1 serving.go:386] Generated self-signed cert in-memory
E0217 16:48:50.230013       1 run.go:72] "command failed" err="stat /etc/kubernetes/controller-manager-XXXX.conf: no such file or directory"

Last log give us information that there is a wrong file name

List options in the controller manager
	terminal --> cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep XXX

	# result:     - --kubeconfig=/etc/kubernetes/controller-manager-XXXX.conf


This is wrong configured configuration path.

Edit the controller manager manifests file and fix the path
	terminal --> vi /etc/kubernetes/manifests/kube-controller-manager.yaml

-----------------------------------------------
    - --controllers=*,bootstrapsigner,tokencleaner
    - --kubeconfig=/etc/kubernetes/controller-manager.conf		# fixed name
    - --leader-elect=true
-----------------------------------------------
save changes - escape, :wq!, enter


wqit few minutes for changes to take effect

List pods in the default namespace (all controlplane components)
	terminal --> k get pods -n kube-system

# result:
NAME                                   READY   STATUS    RESTARTS   AGE
coredns-77d6fd4654-64nh2               1/1     Running   0          56m
coredns-77d6fd4654-cp7t2               1/1     Running   0          56m
etcd-controlplane                      1/1     Running   0          56m
kube-apiserver-controlplane            1/1     Running   0          56m
kube-controller-manager-controlplane   1/1     Running   0          44s
kube-proxy-ldgk2                       1/1     Running   0          56m
kube-scheduler-controlplane            1/1     Running   0          27m



List pods
	terminal --> k get pods

	# result:
	NAME                   READY   STATUS    RESTARTS   AGE
	app-776bb5c68f-qcbcn   1/1     Running   0          2m31s
	app-776bb5c68f-vj7k2   1/1     Running   0          18s


List deployments
	terminal --> k get deploy

	# result:
	NAME   READY   UP-TO-DATE   AVAILABLE   AGE
	app    2/2     2            2           3m3s		# running 2/2


Looks like everithing is running now.

- click 'Check' button





4. Something is wrong with scaling again. We just tried scaling the deployment to 3 replicas. But it's not happening.
---------------------------------------------------------------------------------------------------------------------
Investigate and fix the issue.


List pods
	terminal --> k get pods

	# result:
	NAME                   READY   STATUS    RESTARTS   AGE
	app-776bb5c68f-qcbcn   1/1     Running   0          5m57s
	app-776bb5c68f-vj7k2   1/1     Running   0          3m44s



List deployments
	terminal --> k get deploy

	# result:
	NAME   READY   UP-TO-DATE   AVAILABLE   AGE
	app    2/3     2            2           6m32s		# 2/3 something is wrong


Show deployment details
	terminal --> k describe deploy app

--------------------------------------------------
Name:                   app
Namespace:              default
CreationTimestamp:      Mon, 17 Feb 2025 16:59:36 +0000
Labels:                 app=app
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=app
Replicas:               3 desired | 2 updated | 2 total | 2 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=app
  Containers:
   nginx:
    Image:         nginx:alpine
    Port:          <none>
    Host Port:     <none>
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   app-776bb5c68f (2/2 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  7m39s  deployment-controller  Scaled up replica set app-776bb5c68f to 1
  Normal  ScalingReplicaSet  5m26s  deployment-controller  Scaled up replica set app-776bb5c68f to 2 from 1
--------------------------------------------------
The deployment is not scaled to 3, but only to 2

The scaling is responsibility of controller manager


Show controlplane pods (all controlplane components)
	terminal --> k get pods -n kube-system

# result
NAME                                   READY   STATUS             RESTARTS        AGE
coredns-77d6fd4654-9cvhb               1/1     Running            0               11m
coredns-77d6fd4654-nmmf7               1/1     Running            0               11m
etcd-controlplane                      1/1     Running            0               11m
kube-apiserver-controlplane            1/1     Running            0               11m
kube-controller-manager-controlplane   0/1     CrashLoopBackOff   5 (117s ago)    5m19s		# not running
kube-proxy-2js2b                       1/1     Running            0               11m
kube-scheduler-controlplane            1/1     Running            1 (9m18s ago)   8m34s



Show details for controller manager pod
	terminal --> k describe pod kube-controller-manager-controlplane -n kube-system

--------------------------------------------------------------------
Name:                 kube-controller-manager-controlplane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 controlplane/192.168.163.40
Start Time:           Mon, 17 Feb 2025 16:57:07 +0000
Labels:               component=kube-controller-manager
                      tier=control-plane
Annotations:          kubernetes.io/config.hash: 10276f48da31401b6698c88d4267d3a4
                      kubernetes.io/config.mirror: 10276f48da31401b6698c88d4267d3a4
                      kubernetes.io/config.seen: 2025-02-17T17:03:11.506134877Z
                      kubernetes.io/config.source: file
Status:               Running
SeccompProfile:       RuntimeDefault
IP:                   192.168.163.40
IPs:
  IP:           192.168.163.40
Controlled By:  Node/controlplane
Containers:
  kube-controller-manager:
    Container ID:  containerd://8ff4cedf15896c26e95de33b00ca4395f8bfdcf68240fdfd188214fe2e226ddd
    Image:         registry.k8s.io/kube-controller-manager:v1.31.0
    Image ID:      registry.k8s.io/kube-controller-manager@sha256:f6f3c33dda209e8434b83dacf5244c03b59b0018d93325ff21296a142b68497d
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=172.17.0.0/16
      --cluster-name=kubernetes
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --service-cluster-ip-range=172.20.0.0/16
      --use-service-account-credentials=true
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Mon, 17 Feb 2025 17:06:44 +0000
      Finished:     Mon, 17 Feb 2025 17:06:44 +0000
    Ready:          False
    Restart Count:  5
    Requests:
      cpu:        200m
    Liveness:     http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=8
    Startup:      http-get https://127.0.0.1:10257/healthz delay=10s timeout=15s period=10s #success=1 #failure=24
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/WRONG-PKI-DIRECTORY
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute op=Exists
Events:
  Type     Reason   Age                   From     Message
  ----     ------   ----                  ----     -------
  Normal   Started  5m5s (x4 over 6m4s)   kubelet  Started container kube-controller-manager
  Normal   Pulled   4m18s (x5 over 6m4s)  kubelet  Container image "registry.k8s.io/kube-controller-manager:v1.31.0" already present on machine
  Normal   Created  4m18s (x5 over 6m4s)  kubelet  Created container kube-controller-manager
  Warning  BackOff  54s (x29 over 5m58s)  kubelet  Back-off restarting failed container kube-controller-manager in pod kube-controller-manager-controlplane_kube-system(10276f48da31401b6698c88d4267d3a4)
--------------------------------------------------------------------
No useful information about the issue


Show logs for the controller manager pod
	terminal --> k logs kube-controller-manager-controlplane -n kube-system

# result:
I0217 17:09:39.667623       1 serving.go:386] Generated self-signed cert in-memory
E0217 17:09:39.982709       1 run.go:72] "command failed" err="unable to load client CA provider: open /etc/kubernetes/pki/ca.crt: no such file or directory"

We can see that file for certificate is not found. 

Check if the file exists locally
	terminal --> ls /etc/kubernetes/pki/ca.crt

	# result: /etc/kubernetes/pki/ca.crt			# it exists

Print the file
	terminal --> cat /etc/kubernetes/pki/ca.crt
--------------------------------------------------------
-----BEGIN CERTIFICATE-----
MIIDBTCCAe2gAwIBAgIIJbM1lwEL+8MwDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE
AxMKa3ViZXJuZXRlczAeFw0yNTAyMTcxNjUxNDFaFw0zNTAyMTUxNjU2NDFaMBUx
EzARBgNVBAMTCmt1YmVybmV0ZXMwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEK
AoIBAQCzrj2K5flGjZmkD42Ch8rPLMkMvNVqqKDOzzTeRTk8ey9x6RnIjI0TG/fW
P56xCchc7lld+dJQPIuYnHM0KVENbnHmMAolO9xNZDYMO/yQGlFs7X03M44JYELM
62lxWn7RLDKk3YU3iJ8cPanCdGaBYwAtDX8DPxh8GJ8WV8pa47VSRxCmnqgRn+Mo
+saFFaMADN24+7VPMPN5JlLIcvBBzR56pPsMV2wugDm/pzL1sDj/5qtzmaPjRxiv
SRHScRUpVc2lbLsBa2v5qDo+3isdZCZm/EnYUO6pq8m6i0sG3eYdASuI2SWzMc0L
PO3b8sPszP47yS5BcHv9Xk+039r5AgMBAAGjWTBXMA4GA1UdDwEB/wQEAwICpDAP
BgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBR/X7kzwhNTJSGYVN6Lt69M88SrqzAV
BgNVHREEDjAMggprdWJlcm5ldGVzMA0GCSqGSIb3DQEBCwUAA4IBAQA6L+mXEe7P
m6WPYNDG6TZTCb7p3z2i7BFvHUrY/KAw3jZaSYE+6aaeT7HkWZMuxLHcGnvbvVSH
oJiHczB4xnQ/aOpY5uTtObrhi3O05vycOuz3w05+ipOc9pfWf0Ht32AyHMm4pdpL
pOgv4RpI+/VwsfvoSzuUxQTbGA71mqjBVGs4m/cIPw7r25DkdDl+VD3r1ZD/spjy
0WHUv0mjKMwg3tZfri1vseCFiydd/LlxOoEREA6LGtXhEfEr+VtrPqrYWvwVv1QY
Dur43P/DJImZshYbLEEUJRsGGuXIgNvxXIjd7zpNkxEBcZwmM1zSi9l4+2K7BVCL
lOnKa/aMs4MD
-----END CERTIFICATE-----
--------------------------------------------------------
The certificate look correct format


The certificate files are on the host controlplane. Then we use volumes to mount this directories in the configuration file in the controller manager.

Edit the controller manages manifests file and fix the path
	terminal --> vi /etc/kubernetes/manifests/kube-controller-manager.yaml

---------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-controller-manager
    tier: control-plane
  name: kube-controller-manager
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-controller-manager
    - --allocate-node-cidrs=true
    - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --bind-address=127.0.0.1
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --cluster-cidr=172.17.0.0/16
    - --cluster-name=kubernetes
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
    - --controllers=*,bootstrapsigner,tokencleaner
    - --kubeconfig=/etc/kubernetes/controller-manager.conf
    - --leader-elect=true
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --root-ca-file=/etc/kubernetes/pki/ca.crt
    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=172.20.0.0/16
    - --use-service-account-credentials=true
    image: registry.k8s.io/kube-controller-manager:v1.31.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-controller-manager
    resources:
      requests:
        cpu: 200m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/ca-certificates
      name: etc-ca-certificates
      readOnly: true
    - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      name: flexvolume-dir
    - mountPath: /etc/kubernetes/pki					# this is the mounted directory
      name: k8s-certs							# this is the name of the used volume
      readOnly: true
    - mountPath: /etc/kubernetes/controller-manager.conf
      name: kubeconfig
      readOnly: true
    - mountPath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readOnly: true
    - mountPath: /usr/share/ca-certificates
      name: usr-share-ca-certificates
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  - hostPath:
      path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      type: DirectoryOrCreate
    name: flexvolume-dir
  - hostPath:
      path: /etc/kubernetes/pki				# changed from "WRONG-PKI-DIRECTORY" to "pki"
      type: DirectoryOrCreate
    name: k8s-certs					# this is the volume name used in the volumeMount section
  - hostPath:
      path: /etc/kubernetes/controller-manager.conf
      type: FileOrCreate
    name: kubeconfig
  - hostPath:
      path: /usr/local/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-local-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-share-ca-certificates
status: {}
---------------------------------------------
save changes - escape, :wq!, enter




Show controlplane pods (all controlplane components)
	terminal --> k get pods -n kube-system

# result
NAME                                   READY   STATUS    RESTARTS      AGE
coredns-77d6fd4654-9cvhb               1/1     Running   0             29m
coredns-77d6fd4654-nmmf7               1/1     Running   0             29m
etcd-controlplane                      1/1     Running   0             29m
kube-apiserver-controlplane            1/1     Running   0             29m
kube-controller-manager-controlplane   1/1     Running   0             28s
kube-proxy-2js2b                       1/1     Running   0             29m
kube-scheduler-controlplane            1/1     Running   1 (26m ago)   26m


List pods
	terminal --> k get pods

# result:
NAME                   READY   STATUS    RESTARTS   AGE
app-776bb5c68f-l4wb9   1/1     Running   0          53s
app-776bb5c68f-qcbcn   1/1     Running   0          27m
app-776bb5c68f-vj7k2   1/1     Running   0          25m		# 3 pods as planned


List deployments
	terminal --> k get deploy

	# result:
	NAME   READY   UP-TO-DATE   AVAILABLE   AGE
	app    3/3     3            3           27m		# all pods are running


- click 'Check' button





===================================
Section 15 300. Worker Node Failure
===================================

Check nodes status
	terminal --> kubectl get nodes

	# result:
	NAME 		STATUS 		ROLES 	AGE 	VERSION
	worker-1 	Ready 		<none> 	8d 	v1.13.0 
	worker-2 	NotReady 	<none> 	8d 	v1.13.0		# if NotReady we have to chack details



Show details for worker-1 node (Running node state)
---------------------------------------------------
	terminal --> kubectl describe node worker-1

# result:
Conditions:
Type 		Status 	LastHeartbeatTime 		Reason 			 Message 
---- 		------ 	----------------- 		------ 			 ------- 
OutOfDisk 	False 	Mon, 01 Apr 2019 14:30:33 +0000 KubeletHasSufficientDisk kubelet has sufficient disk space available 
MemoryPressure 	False 	Mon, 01 Apr 2019 14:30:33 +0000 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure 	False 	Mon, 01 Apr 2019 14:30:33 +0000 KubeletHasNoDiskPressure kubelet has no disk pressure 
PIDPressure 	False 	Mon, 01 Apr 2019 14:30:33 +0000 KubeletHasSufficientPID  kubelet has sufficient PID available 	
Ready 		True 	Mon, 01 Apr 2019 14:30:33 +0000 KubeletReady kubelet is posting ready status. AppArmor enabled

If there is a issue Ready flag isset to 'false' and conditions OutOfDisk, MemoryPressure, DiskPressure and/or PIDPressure are set to true depending on the issue the pod has failed.



Show details for worker-2 node (Failure node state)
---------------------------------------------------
	terminal --> kubectl describe node worker-2

Conditions: 
Type 		Status 		LastHeartbeatTime 		Reason 			Message 
---- 		------ 		----------------- 		------ 			------- 
OutOfDisk 	Unknown 	Mon, 01 Apr 2019 14:20:20 +0000 NodeStatusUnknown 	Kubelet stopped posting node status. MemoryPressure 	Unknown 	Mon, 01 Apr 2019 14:20:20 +0000 NodeStatusUnknown 	Kubelet stopped posting node status. DiskPressure 	Unknown 	Mon, 01 Apr 2019 14:20:20 +0000 NodeStatusUnknown 	Kubelet stopped posting node status. PIDPressure 	False 		Mon, 01 Apr 2019 14:20:20 +0000 KubeletHasSufficientPID kubelet has sufficient PID available 
Ready 		Unknown 	Mon, 01 Apr 2019 14:20:20 +0000 NodeStatusUnknown Kubelet stopped posting node status.



If the node is crashed, try to connect to it and check issues as
- cpu, memory or disk space issues
	terminal --> ssh worker-2
	
Check info
	terminal --> top

# result:
top - 14:43:56 up 3 days, 19:02, 1 user, load average: 0.35, 0.29, 0.21
Tasks: 112 total, 1 running, 72 sleeping, 0 stopped, 0 zombie
%Cpu(s): 3.9 us, 1.7 sy, 0.1 ni, 94.3 id, 0.0 wa, 0.0 hi, 0.1 si, 0.0 st
KiB Mem : 1009112 total, 74144 free, 736608 used, 198360 buff/cache
KiB Swap: 0 total, 0 free, 0 used. 129244 avail Mem
PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 34 root 20 0 0 0 0 S 5.9 0.0 0:13.14 kswapd0 28826 999 20 0 1361320 383208 3596 S 5.9 38.0 0:46.95 mysqld 1 root 20 0 78260 5924 3192 S 0.0 0.6 0:21.88 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:00.02 kthreadd 4 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 kworker/0:0H


Check memory
	terminal --> df -h


Check the status of the kubelet
	terminal --> service kubelet status

# result:
● kubelet.service - Kubernetes Kubelet
Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2019-03-20 14:22:06 UTC; 1 weeks 1 days ago
Docs: https://github.com/kubernetes/kubernetes Main PID: 1281 (kubelet)
Tasks: 24 (limit: 1152)


Show logs 
	terminal --> sudo journalctl –u kubelet

# result:
-- Logs begin at Wed 2019-03-20 05:30:37 UTC, end at Mon 2019-04-01 14:42:42 UTC. --
Mar 20 08:12:59 worker-1 systemd[1]: Started Kubernetes Kubelet.
Mar 20 08:12:59 worker-1 kubelet[18962]: Flag --tls-cert-file has been deprecated, T the Kubele
Mar 20 08:12:59 worker-1 kubelet[18962]: Flag --tls-private-key-file has been deprec
specified by the
his parameter should be set via the config file specified by ated, This parameter should be set via the config file Mar 20 08:12:59 worker-1 kubelet[18962]: I0320 08:12:59.915179 18962 flags.go:33] FLAG: --address="0.0.0.0" Mar 20 08:12:59 worker-1 kubelet[18962]: I0320 08:12:59.918149 18962 flags.go:33] FLAG: --allow-privileged="true" Mar 20 08:12:59 worker-1 kubelet[18962]: I0320 08:12:59.918339 18962 flags.go:33] FLAG: --allowed-unsafe-sysctls="[]" Mar 20 08:12:59 worker-1 kubelet[18962]: I0320 08:12:59.918502 18962 flags.go:33] FLAG: --alsologtostderr="false" Mar 20 08:12:59 worker-1 kubelet[18962]: I0320 08:12:59.918648 18962 flags.go:33] FLAG: --anonymous-auth="true" Mar 20 08:12:59 worker-1 kubelet[18962]: I0320 08:12:59.918841 18962 flags.go:33] FLAG: --application-metrics-count-limit="100" Mar 20 08:12:59 worker-1 kubelet[18962]: I0320 08:12:59.918974 18962 flags.go:33] FLAG: --authentication-token-webhook="false" Mar 20 08:12:59 worker-1 kubelet[18962]: I0320 08:12:59.919096 18962 flags.go:33] FLAG: --authentication-token-webhook-cache-ttl="2m0s" Mar 20 08:12:59 worker-1 kubelet[18962]: I0320 08:12:59.919299 18962 flags.go:33] FLAG: --authorization-mode="AlwaysAllow" Mar 20 08:12:59 worker-1 kubelet[18962]: I0320 08:12:59.919466 18962 flags.go:33] FLAG: --authorization-webhook-cache-authorized-ttl="5m0s" Mar 20 08:12:59 worker-1 kubelet[18962]: I0320 08:12:59.919598 18962 flags.go:33] FLAG: --authorization-webhook-cache-unauthorized-ttl="30s" Mar 20 08:12:59 worker-1 kubelet[18962]: I0320 08:12:59.919791 18962 flags.go:33] FLAG: --azure-container-registry-config="" Mar 20 08:12:59 worker-1 kubelet[18962]: I0320 08:12:59.919971 18962 flags.go:33] FLAG: --boot-id-file="/proc/sys/kernel/random/boot_id" Mar 20 08:12:59 worker-1 kubelet[18962]: I0320 08:12:59.920102 18962 flags.go:33] FLAG: --bootstrap-checkpoint-path=""


Check if kubelet certificates are expired, they are added to the right group and are signed by the correct CA
	terminal --> openssl x509 -in /var/lib/kubelet/worker-1.crt -text

# result:
Certificate:
  Data:
      Version: 3 (0x2) 
      Serial Number:
          ff:e0:23:9d:fc:78:03:35
  Signature Algorithm: sha256WithRSAEncryption 
      Issuer: CN = KUBERNETES-CA				# Check CA
      Validity
	  Not Before: Mar 20 08:09:29 2019 GMT 		
	  Not After : Apr 19 08:09:29 2019 GMT 			# Check if expired
      Subject: CN = system:node:worker-1, O = system:nodes 	# Check if in the right group
      Subject Public Key Info:
	  Public Key Algorithm: rsaEncryption 
	  Public-Key: (2048 bit) 
	  Modulus:
		00:b4:28:0c:60:71:41:06:14:46:d9:97:58:2d:fe:
		a9:c7:6d:51:cd:1c:98:b9:5e:e6:e4:02:d3:e3:71:
		58:a1:60:fe:cb:e7:9b:4b:86:04:67:b5:4f:da:d6:
		6c:08:3f:57:e9:70:59:57:48:6a:ce:e5:d4:f3:6e:
		b2:fa:8a:18:7e:21:60:35:8f:44:f7:a9:39:57:16:
		4f:4e:1e:b1:a3:77:32:c2:ef:d1:38:b4:82:20:8f:
		11:0e:79:c4:d1:9b:f6:82:c4:08:84:84:68:d5:c3:
		e2:15:a0:ce:23:3c:8d:9c:b8:dd:fc:3a:cd:42:ae:
		5e:1b:80:2d:1b:e5:5d:1b:c1:fb:be:a3:9e:82:ff:
		a1:27:c8:b6:0f:3c:cb:11:f9:1a:9b:d2:39:92:0e:
		47:45:b8:8f:98:13:c6:4d:6a:18:75:a4:01:6f:73:
		f6:f8:7f:eb:5d:59:94:46:d8:da:37:75:cf:27:0b:
		39:7f:48:20:c5:fd:c7:a7:ce:22:9a:33:4a:30:1d:
		95:ef:00:bd:fe:47:22:42:44:99:77:5a:c4:97:bb:
		37:93:7c:33:64:f4:b8:3a:53:8c:f4:10:db:7f:5f:
		2b:89:18:d6:0e:68:51:34:29:b1:f1:61:6b:4b:c6






===================================================
Section 15 302. Practice Test - Worker Node Failure
===================================================


1. Fix the broken cluster
-------------------------

List nodes
	terminal --> k get nodes

	# result:
	NAME           STATUS     ROLES           AGE     VERSION
	controlplane   Ready      control-plane   6m4s    v1.31.0
	node01         NotReady   <none>          5m38s   v1.31.0		# not working node


Show information about the not working node
	terminal --> k describe node node01

# result:
-------------------------------------------------------------------------
Name:               node01
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=node01
                    kubernetes.io/os=linux
Annotations:        flannel.alpha.coreos.com/backend-data: {"VNI":1,"VtepMAC":"2e:cd:0a:a4:d1:fb"}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 192.168.212.189
                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 18 Feb 2025 09:29:29 +0000
Taints:             node.kubernetes.io/unreachable:NoExecute
                    node.kubernetes.io/unreachable:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  node01
  AcquireTime:     <unset>
  RenewTime:       Tue, 18 Feb 2025 09:33:44 +0000
Conditions:
  Type                 Status    LastHeartbeatTime                 LastTransitionTime                Reason              Message
  ----                 ------    -----------------                 ------------------                ------              -------
  NetworkUnavailable   False     Tue, 18 Feb 2025 09:29:35 +0000   Tue, 18 Feb 2025 09:29:35 +0000   FlannelIsUp         Flannel is running on this node
  MemoryPressure       Unknown   Tue, 18 Feb 2025 09:30:00 +0000   Tue, 18 Feb 2025 09:34:28 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  DiskPressure         Unknown   Tue, 18 Feb 2025 09:30:00 +0000   Tue, 18 Feb 2025 09:34:28 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  PIDPressure          Unknown   Tue, 18 Feb 2025 09:30:00 +0000   Tue, 18 Feb 2025 09:34:28 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  Ready                Unknown   Tue, 18 Feb 2025 09:30:00 +0000   Tue, 18 Feb 2025 09:34:28 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
Addresses:
  InternalIP:  192.168.212.189
  Hostname:    node01
Capacity:
  cpu:                16
  ephemeral-storage:  772706776Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             65838292Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  712126563583
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             65735892Ki
  pods:               110
System Info:
  Machine ID:                 132e3d2451f947fe9214456160254717
  System UUID:                88c76b9b-79b9-2d71-8083-5fa7b96f698b
  Boot ID:                    7b2b806c-5b39-4f54-82c5-128fdb06dee9
  Kernel Version:             5.15.0-1075-gcp
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.26
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      172.17.1.0/24
PodCIDRs:                     172.17.1.0/24
Non-terminated Pods:          (2 in total)
  Namespace                   Name                     CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                     ------------  ----------  ---------------  -------------  ---
  kube-flannel                kube-flannel-ds-9l4pq    100m (0%)     0 (0%)      50Mi (0%)        0 (0%)         6m56s
  kube-system                 kube-proxy-cxhhj         0 (0%)        0 (0%)      0 (0%)           0 (0%)         6m56s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (0%)  0 (0%)
  memory             50Mi (0%)  0 (0%)
  ephemeral-storage  0 (0%)     0 (0%)
  hugepages-1Gi      0 (0%)     0 (0%)
  hugepages-2Mi      0 (0%)     0 (0%)
Events:
  Type     Reason                   Age                    From             Message
  ----     ------                   ----                   ----             -------
  Normal   Starting                 6m52s                  kube-proxy       
  Normal   Starting                 6m56s                  kubelet          Starting kubelet.
  Warning  InvalidDiskCapacity      6m56s                  kubelet          invalid capacity 0 on image filesystem
  Warning  CgroupV1                 6m56s                  kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeHasSufficientMemory  6m56s (x2 over 6m56s)  kubelet          Node node01 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    6m56s (x2 over 6m56s)  kubelet          Node node01 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     6m56s (x2 over 6m56s)  kubelet          Node node01 status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  6m56s                  kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode           6m52s                  node-controller  Node node01 event: Registered Node node01 in Controller
  Normal   NodeReady                6m52s                  kubelet          Node node01 status is now: NodeReady
  Normal   NodeNotReady             117s                   node-controller  Node node01 status is now: NodeNotReady
-------------------------------------------------------------------------

We can see information in the events that occur.


Connect to node01
	terminal --> ssh node01


Check if the kubelet is working on node01
	node01 terminal --> service kubelet status

# result:
-------------------------------------------------------------------------
○ kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: inactive (dead) since Tue 2025-02-18 09:33:46 UTC; 6min ago
       Docs: https://kubernetes.io/docs/
    Process: 2562 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, statu>
   Main PID: 2562 (code=exited, status=0/SUCCESS)

Feb 18 09:29:30 node01 kubelet[2562]: I0218 09:29:30.650769    2562 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started >
Feb 18 09:29:30 node01 kubelet[2562]: I0218 09:29:30.650788    2562 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started >
Feb 18 09:29:30 node01 kubelet[2562]: I0218 09:29:30.650811    2562 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started >
Feb 18 09:29:30 node01 kubelet[2562]: I0218 09:29:30.650862    2562 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started >
Feb 18 09:29:30 node01 kubelet[2562]: I0218 09:29:30.650900    2562 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started >
Feb 18 09:29:30 node01 kubelet[2562]: I0218 09:29:30.650924    2562 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started >
Feb 18 09:29:30 node01 kubelet[2562]: I0218 09:29:30.650945    2562 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started >
Feb 18 09:29:30 node01 kubelet[2562]: I0218 09:29:30.757061    2562 swap_util.go:54] "Running under a user namespace - tmpfs noswap is not supported"
Feb 18 09:29:33 node01 kubelet[2562]: I0218 09:29:33.019835    2562 kubelet_node_status.go:488] "Fast updating node status as it just became ready"
Feb 18 09:29:33 node01 kubelet[2562]: I0218 09:29:33.727779    2562 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system>
-------------------------------------------------------------------------

We can see the status is not active - Active: inactive (dead)


Start the kubelet service on node01
	node01 terminal --> service kubelet start


Check if the kubelet started
	node01 terminal --> service kubelet status

-------------------------------------------------------------------------
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: active (running) since Tue 2025-02-18 09:43:48 UTC; 5s ago
       Docs: https://kubernetes.io/docs/
   Main PID: 9875 (kubelet)
      Tasks: 23 (limit: 77143)
     Memory: 22.9M
     CGroup: /system.slice/kubelet.service
             └─9875 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/v>

Feb 18 09:43:49 node01 kubelet[9875]: I0218 09:43:49.336605    9875 kubelet_node_status.go:111] "Node was previously registered" node="node01"
Feb 18 09:43:49 node01 kubelet[9875]: I0218 09:43:49.336688    9875 kubelet_node_status.go:75] "Successfully registered node" node="node01"
Feb 18 09:43:50 node01 kubelet[9875]: I0218 09:43:50.035536    9875 apiserver.go:52] "Watching apiserver"
Feb 18 09:43:50 node01 kubelet[9875]: I0218 09:43:50.046334    9875 desired_state_of_world_populator.go:154] "Finished populating initial desired state >
Feb 18 09:43:50 node01 kubelet[9875]: I0218 09:43:50.052208    9875 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started >
Feb 18 09:43:50 node01 kubelet[9875]: I0218 09:43:50.052252    9875 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started >
Feb 18 09:43:50 node01 kubelet[9875]: I0218 09:43:50.052265    9875 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started >
Feb 18 09:43:50 node01 kubelet[9875]: I0218 09:43:50.052290    9875 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started >
Feb 18 09:43:50 node01 kubelet[9875]: I0218 09:43:50.052302    9875 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started >
Feb 18 09:43:50 node01 kubelet[9875]: I0218 09:43:50.052318    9875 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started
-------------------------------------------------------------------------

We can see now the status is running - Active: active (running)


Go back to the controlplane node
	node01 terminal --> exit


Check the status of the nodes
	terminal --> k get nodes

	# result:
	NAME           STATUS   ROLES           AGE   VERSION
	controlplane   Ready    control-plane   17m   v1.31.0
	node01         Ready    <none>          16m   v1.31.0		# now node01 is in Ready state


- click 'Check' button





2. The cluster is broken again. Investigate and fix the issue.
--------------------------------------------------------------


List nodes
	terminal --> k get nodes

	# result:
	NAME           STATUS     ROLES           AGE   VERSION
	controlplane   Ready      control-plane   19m   v1.31.0
	node01         NotReady   <none>          19m   v1.31.0		# node01 is not working


Show details for the node
	terminal --> k describe node node01

# result
-----------------------------------------------------------------
Name:               node01
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=node01
                    kubernetes.io/os=linux
Annotations:        flannel.alpha.coreos.com/backend-data: {"VNI":1,"VtepMAC":"2e:cd:0a:a4:d1:fb"}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 192.168.212.189
                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 18 Feb 2025 09:29:29 +0000
Taints:             node.kubernetes.io/unreachable:NoExecute
                    node.kubernetes.io/unreachable:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  node01
  AcquireTime:     <unset>
  RenewTime:       Tue, 18 Feb 2025 09:47:32 +0000
Conditions:
  Type                 Status    LastHeartbeatTime                 LastTransitionTime                Reason              Message
  ----                 ------    -----------------                 ------------------                ------              -------
  NetworkUnavailable   False     Tue, 18 Feb 2025 09:29:35 +0000   Tue, 18 Feb 2025 09:29:35 +0000   FlannelIsUp         Flannel is running on this node
  MemoryPressure       Unknown   Tue, 18 Feb 2025 09:43:49 +0000   Tue, 18 Feb 2025 09:48:13 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  DiskPressure         Unknown   Tue, 18 Feb 2025 09:43:49 +0000   Tue, 18 Feb 2025 09:48:13 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  PIDPressure          Unknown   Tue, 18 Feb 2025 09:43:49 +0000   Tue, 18 Feb 2025 09:48:13 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  Ready                Unknown   Tue, 18 Feb 2025 09:43:49 +0000   Tue, 18 Feb 2025 09:48:13 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
Addresses:
  InternalIP:  192.168.212.189
  Hostname:    node01
Capacity:
  cpu:                16
  ephemeral-storage:  772706776Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             65838292Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  712126563583
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             65735892Ki
  pods:               110
System Info:
  Machine ID:                 132e3d2451f947fe9214456160254717
  System UUID:                88c76b9b-79b9-2d71-8083-5fa7b96f698b
  Boot ID:                    7b2b806c-5b39-4f54-82c5-128fdb06dee9
  Kernel Version:             5.15.0-1075-gcp
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.26
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      172.17.1.0/24
PodCIDRs:                     172.17.1.0/24
Non-terminated Pods:          (2 in total)
  Namespace                   Name                     CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                     ------------  ----------  ---------------  -------------  ---
  kube-flannel                kube-flannel-ds-9l4pq    100m (0%)     0 (0%)      50Mi (0%)        0 (0%)         22m
  kube-system                 kube-proxy-cxhhj         0 (0%)        0 (0%)      0 (0%)           0 (0%)         22m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (0%)  0 (0%)
  memory             50Mi (0%)  0 (0%)
  ephemeral-storage  0 (0%)     0 (0%)
  hugepages-1Gi      0 (0%)     0 (0%)
  hugepages-2Mi      0 (0%)     0 (0%)
Events:
  Type     Reason                   Age                    From             Message
  ----     ------                   ----                   ----             -------
  Normal   Starting                 22m                    kube-proxy       
  Normal   NodeAllocatableEnforced  22m                    kubelet          Updated Node Allocatable limit across pods
  Warning  InvalidDiskCapacity      22m                    kubelet          invalid capacity 0 on image filesystem
  Warning  CgroupV1                 22m                    kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   Starting                 22m                    kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  22m (x2 over 22m)      kubelet          Node node01 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    22m (x2 over 22m)      kubelet          Node node01 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     22m (x2 over 22m)      kubelet          Node node01 status is now: NodeHasSufficientPID
  Normal   RegisteredNode           22m                    node-controller  Node node01 event: Registered Node node01 in Controller
  Normal   NodeReady                22m                    kubelet          Node node01 status is now: NodeReady
  Normal   NodeReady                8m17s                  kubelet          Node node01 status is now: NodeReady
  Normal   Starting                 8m17s                  kubelet          Starting kubelet.
  Warning  CgroupV1                 8m17s                  kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Warning  InvalidDiskCapacity      8m17s                  kubelet          invalid capacity 0 on image filesystem
  Normal   NodeAllocatableEnforced  8m17s                  kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  8m17s (x2 over 8m17s)  kubelet          Node node01 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    8m17s (x2 over 8m17s)  kubelet          Node node01 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     8m17s (x2 over 8m17s)  kubelet          Node node01 status is now: NodeHasSufficientPID
  Normal   NodeNotReady             3m53s (x2 over 17m)    node-controller  Node node01 status is now: NodeNotReady
-----------------------------------------------------------------
We cant locate the problem from the events messages.


Connect to node01
	terminal --> ssh node01


Check if the kubelet process is working on node01
	node01 terminal --> service kubelet status

# result:
-----------------------------------------
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: activating (auto-restart) (Result: exit-code) since Tue 2025-02-18 09:49:24 UTC; 3s ago
       Docs: https://kubernetes.io/docs/
    Process: 12584 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, stat>
   Main PID: 12584 (code=exited, status=1/FAILURE)
-----------------------------------------

We can see that the state is 'activating' - Active: activating (auto-restart)

This means that the process is not able to start


Try to start the service
	node01 terminal --> service kubelet start

Check if the kubelet have started
	node01 terminal --> service kubelet status

# result:
-----------------------------------------
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: activating (auto-restart) (Result: exit-code) since Tue 2025-02-18 09:55:12 UTC; 6s ago
       Docs: https://kubernetes.io/docs/
    Process: 15634 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, stat>
   Main PID: 15634 (code=exited, status=1/FAILURE)
-----------------------------------------
Still the same state - Active: activating (auto-restart)



Show logs for the kubelet service
	node01 terminal --> journalctl -u kubelet 


# result:
-----------------------------------------
Feb 18 09:28:34 node01 kubelet[893]: E0218 09:28:34.908298     893 run.go:72] "command failed" err="failed to load kubelet config file, path: /var/lib/k>
Feb 18 09:28:45 node01 kubelet[1434]: E0218 09:28:45.303606    1434 run.go:72] "command failed" err="failed to load kubelet config file, path: /var/lib/>
Feb 18 09:28:55 node01 kubelet[1827]: E0218 09:28:55.498180    1827 run.go:72] "command failed" err="failed to load kubelet config file, path: /var/lib/>
Feb 18 09:29:05 node01 kubelet[1869]: E0218 09:29:05.705558    1869 run.go:72] "command failed" err="failed to load kubelet config file, path: /var/lib/>
Feb 18 09:29:16 node01 kubelet[1910]: E0218 09:29:16.000262    1910 run.go:72] "command failed" err="failed to load kubelet config file, path: /var/lib/>
Feb 18 09:29:26 node01 kubelet[1953]: E0218 09:29:26.197938    1953 run.go:72] "command failed" err="failed to load kubelet config file, path: /var/lib/>
Feb 18 09:29:29 node01 kubelet[2562]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified >
Feb 18 09:29:29 node01 kubelet[2562]: Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector>
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.211716    2562 server.go:206] "--pod-infra-container-image will not be pruned by the image garbage >
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.595435    2562 server.go:486] "Kubelet version" kubeletVersion="v1.31.0"
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.595476    2562 server.go:488] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.595836    2562 server.go:929] "Client rotation is on, will bootstrap in background"
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.600472    2562 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/etc/ku>
Feb 18 09:29:29 node01 kubelet[2562]: E0218 09:29:29.604114    2562 log.go:32] "RuntimeConfig from runtime service failed" err="rpc error: code = Unimpl>
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.604146    2562 server.go:1403] "CRI implementation should be updated to support RuntimeConfig when >
Feb 18 09:29:29 node01 kubelet[2562]: E0218 09:29:29.607908    2562 info.go:104] Failed to get disk map: could not parse device numbers from  for device>
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.623212    2562 server.go:744] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  def>
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.623947    2562 swap_util.go:113] "Swap is on" /proc/swaps contents="Filename                       >
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.632819    2562 container_manager_linux.go:264] "Container manager verified user specified cgroup-ro>
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.632866    2562 container_manager_linux.go:269] "Creating Container Manager object based on Node Con>
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.633033    2562 topology_manager.go:138] "Creating topology manager with none policy"
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.633040    2562 container_manager_linux.go:300] "Creating device plugin manager"
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.633127    2562 state_mem.go:36] "Initialized new in-memory state store"
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.633234    2562 kubelet.go:408] "Attempting to sync node with API server"
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.633244    2562 kubelet.go:303] "Adding static pod path" path="/etc/kubernetes/manifests"
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.633274    2562 kubelet.go:314] "Adding apiserver pod source"
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.633287    2562 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.633932    2562 kuberuntime_manager.go:262] "Container runtime initialized" containerRuntime="contai>
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.634428    2562 kubelet.go:837] "Not starting ClusterTrustBundle informer because we are in static k>
Feb 18 09:29:29 node01 kubelet[2562]: W0218 09:29:29.634596    2562 probe.go:272] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins>
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.641972    2562 server.go:1269] "Started kubelet"
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.642031    2562 server.go:163] "Starting to listen" address="0.0.0.0" port=10250
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.642031    2562 ratelimit.go:55] "Setting rate limiting for endpoint" service="podresources" qps=100>
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.642771    2562 server.go:236] "Starting to serve the podresources API" endpoint="unix:/var/lib/kube>
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.643557    2562 server.go:460] "Adding debug handlers to kubelet server"
Feb 18 09:29:29 node01 kubelet[2562]: E0218 09:29:29.644124    2562 kubelet.go:1478] "Image garbage collection failed once. Stats initialization may not>
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.644427    2562 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.644472    2562 dynamic_serving_content.go:135] "Starting controller" name="kubelet-server-cert-file>
Feb 18 09:29:29 node01 kubelet[2562]: E0218 09:29:29.644488    2562 oomparser.go:149] exiting analyzeLines. OOM events will not be reported.
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.644538    2562 volume_manager.go:289] "Starting Kubelet Volume Manager"
Feb 18 09:29:29 node01 kubelet[2562]: E0218 09:29:29.644569    2562 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \>
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.644623    2562 desired_state_of_world_populator.go:146] "Desired state populator starts to run"
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.644686    2562 reconciler.go:26] "Reconciler: start to sync state"
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.644984    2562 factory.go:221] Registration of the systemd container factory successfully
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.645116    2562 factory.go:219] Registration of the crio container factory failed: Get "http://%2Fva>
Feb 18 09:29:29 node01 kubelet[2562]: I0218 09:29:29.646347    2562 factory.go:221] Registration of the containerd container factory successfully
-----------------------------------------
We can see some error connected to kubelet config file.


Print kubelet config files
	node01 terminal --> cat /etc/kubernetes/kubelet.conf

# result:
-----------------------------------------
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJUi9MelZONG1adEl3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TlRBeU1UZ3dPVEl6TXpaYUZ3MHpOVEF5TVRZd09USTRNelphTUJVeApFekFSQmdOVkJBTVRDbXQxWW1WeWJtVjBaWE13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUURGQyt3S1NReVR4M29EM1BWRkM5Q0V6aXNFcXpDTEIxV2ZMaTVydW5zZDB6MXlmUkFvcUp4Vm1IVVoKakRMWEZEeGdOZzRZVGNWSzlaYVZJYmZPZXJ1VjFraDNoMmRsRmRYeithb1F5R3Y3MWZ4RDcvTXp4RUpuSitpYwpldk50YUdQcWQ4OERxZ3EzYytXUGtTMW4vMkl3MUJ3cVpoczFZTU5SMGY2NmJnWTYxbWM5Y0NIL2NNT0tyaWVBCkp3cUgvR3hkbnpJTUFuUzB5TnhOZGs1dmY2YW8xZ1JDR3hrb285VEpvbUlxUjdaeHZGUU9qdW4vemcrd1NKUDIKWk40Y2VUbmh2SEdjZTUyb1F2OTljU1plWi9OclNTT281WCtYeXE5anZOVnF2THNzcjMwUXhPSFBYSVhMckpBcwp1cyt5QXYrQ3RmeWJ5NHpCSGRhempzV1RNdWUzQWdNQkFBR2pXVEJYTUE0R0ExVWREd0VCL3dRRUF3SUNwREFQCkJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXQkJRWGZLTmJRR1RsbTNRVCtmNlRRaWdUVXYvU1pqQVYKQmdOVkhSRUVEakFNZ2dwcmRXSmxjbTVsZEdWek1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQVUwTHh4bWxWKwprZ3lJMjdlM2h4dGV6alJhMW13WXpWakJzOUVjZ1FuZExXd2tPNzlrK3pVV0I3UmhqMkxZcjBpY1Ywb1dTNm9yCjRzNjl0SExmM0htMmIxTy9VQnlRaU5vRWRtemcxM2h6d0hoUm5wQ0VGZHcwb2ZmNlhFVWo2S2NKQWRtSXhyZ04KRUg3NXdnQVk5VXJGVXJ5UFR4K1VEaVMxb2p3VXIzM2hiK1Z0Zk1XcDYyMFZGYkFUT1JPcFVoK3VSMkJhUE42UAp4UVJzTE5zTGRUZTNLR1hvZW9rZjVwZXhpaTRxaG5EL2c4YlY2SE90bDRSc1d3UTNXY3FvWGJYMU9OaVNwWlkyCk1zNXRGbXIzV0VYeThHQ0lBRjZyQmt6SDk2NTlWa3BPa3NMUVhpcDN1bDhVMU1zUkE1ek5XMlY1ek1lT0IvOTIKMW5yWWVKTm9MQ01nCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://controlplane:6443
  name: default-cluster
contexts:
- context:
    cluster: default-cluster
    namespace: default
    user: default-auth
  name: default-context
current-context: default-context
kind: Config
preferences: {}
users:
- name: default-auth
  user:
    client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem
    client-key: /var/lib/kubelet/pki/kubelet-client-current.pem
-----------------------------------------
We dont see any issues in the config file.



Check files connected to kubelet service
	node01 terminal --> ls /var/lib/kubelet

# result - all kubelet service files
checkpoints  cpu_manager_state  kubeadm-flags.env     pki      plugins_registry  pods
config.yaml  device-plugins     memory_manager_state  plugins  pod-resources





Print the config file
	node01 terminal --> cat /var/lib/kubelet/config.yaml

# result:
---------------------------------------------
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/WRONG-CA-FILE.crt			# we need to fix this path
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
cgroupDriver: cgroupfs
clusterDNS:
- 172.20.0.10
clusterDomain: cluster.local
containerRuntimeEndpoint: ""
cpuManagerReconcilePeriod: 0s
evictionPressureTransitionPeriod: 0s
fileCheckFrequency: 0s
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 0s
imageMaximumGCAge: 0s
imageMinimumGCAge: 0s
kind: KubeletConfiguration
logging:
  flushFrequency: 0
  options:
    json:
      infoBufferSize: "0"
    text:
      infoBufferSize: "0"
  verbosity: 0
memorySwap: {}
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
resolvConf: /run/systemd/resolve/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 0s
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s
---------------------------------------------


Find the correct path to the CA file
	node01 terminal --> ls /etc/kubernetes/pki/

	# result: ca.crt


Edit the /var/lib/kubelet/config.yaml file
	node01 terminal --> vi /var/lib/kubelet/config.yaml

# result:
---------------------------------------------
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt			# changed from "WRONG-CA-FILE.crt" to ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
cgroupDriver: cgroupfs
clusterDNS:
- 172.20.0.10
clusterDomain: cluster.local
containerRuntimeEndpoint: ""
cpuManagerReconcilePeriod: 0s
evictionPressureTransitionPeriod: 0s
fileCheckFrequency: 0s
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 0s
imageMaximumGCAge: 0s
imageMinimumGCAge: 0s
kind: KubeletConfiguration
logging:
  flushFrequency: 0
  options:
    json:
      infoBufferSize: "0"
    text:
      infoBufferSize: "0"
  verbosity: 0
memorySwap: {}
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
resolvConf: /run/systemd/resolve/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 0s
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s
---------------------------------------------
save changes - escape, :wq!, enter


Restart kubelet service
	node01 terminal --> service kubelet restart


Check if the kubelet have started
	node01 terminal --> service kubelet status

# result:
-----------------------------------------
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: active (running) since Tue 2025-02-18 10:18:47 UTC; 11s ago
       Docs: https://kubernetes.io/docs/
   Main PID: 27726 (kubelet)
      Tasks: 22 (limit: 77143)
     Memory: 23.6M
     CGroup: /system.slice/kubelet.service
             └─27726 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/>
-----------------------------------------
We can see now the status - Active: active (running)


Go to the controlplane node
	node01 terminal --> exit


List nodes
	terminal --> k get nodes

	# result:
	NAME           STATUS   ROLES           AGE   VERSION
	controlplane   Ready    control-plane   51m   v1.31.0
	node01         Ready    <none>          50m   v1.31.0

	# all nodes are in Ready state


- click 'Check' button





3. The cluster is broken again. Investigate and fix the issue.
--------------------------------------------------------------

List nodes
	terminal --> k get nodes

	# result:
	NAME           STATUS     ROLES           AGE   VERSION
	controlplane   Ready      control-plane   53m   v1.31.0
	node01         NotReady   <none>          52m   v1.31.0		# node01 is in NotReady state


Connect to node node01
	terminal --> ssh node01


Check status of the kubelet service
	node01 terminal --> service kubelet status

# result
-----------------------------------------------------
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: active (running) since Tue 2025-02-18 10:36:36 UTC; 1min 7s ago
       Docs: https://kubernetes.io/docs/
   Main PID: 6289 (kubelet)
      Tasks: 22 (limit: 77143)
     Memory: 30.6M
     CGroup: /system.slice/kubelet.service
             └─6289 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/v>
-----------------------------------------------------
We can see that the kubelet status is running - Active: active (running)


Show kubelet service logs
	node01 terminal --> journalctl -u kubelet

# result:
-----------------------------------------------------
Feb 18 10:31:24 node01 kubelet[873]: E0218 10:31:24.707361     873 run.go:72] "command failed" err="failed to load kubelet config file, path: /var/lib/k>
Feb 18 10:31:34 node01 kubelet[1327]: E0218 10:31:34.999097    1327 run.go:72] "command failed" err="failed to load kubelet config file, path: /var/lib/>
Feb 18 10:31:45 node01 kubelet[1823]: E0218 10:31:45.201152    1823 run.go:72] "command failed" err="failed to load kubelet config file, path: /var/lib/>
Feb 18 10:31:55 node01 kubelet[1864]: E0218 10:31:55.501188    1864 run.go:72] "command failed" err="failed to load kubelet config file, path: /var/lib/>
Feb 18 10:32:05 node01 kubelet[1911]: E0218 10:32:05.701547    1911 run.go:72] "command failed" err="failed to load kubelet config file, path: /var/lib/>
Feb 18 10:32:16 node01 kubelet[1948]: E0218 10:32:16.006629    1948 run.go:72] "command failed" err="failed to load kubelet config file, path: /var/lib/>
Feb 18 10:32:26 node01 kubelet[1988]: E0218 10:32:26.201443    1988 run.go:72] "command failed" err="failed to load kubelet config file, path: /var/lib/>
Feb 18 10:32:36 node01 kubelet[2027]: E0218 10:32:36.512712    2027 run.go:72] "command failed" err="failed to load kubelet config file, path: /var/lib/>
Feb 18 10:32:46 node01 kubelet[2062]: E0218 10:32:46.699014    2062 run.go:72] "command failed" err="failed to load kubelet config file, path: /var/lib/>
Feb 18 10:32:56 node01 kubelet[2105]: E0218 10:32:56.998852    2105 run.go:72] "command failed" err="failed to load kubelet config file, path: /var/lib/>
Feb 18 10:33:07 node01 kubelet[2150]: E0218 10:33:07.200787    2150 run.go:72] "command failed" err="failed to load kubelet config file, path: /var/lib/>
Feb 18 10:33:14 node01 kubelet[2764]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified >
Feb 18 10:33:14 node01 kubelet[2764]: Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector>
Feb 18 10:33:14 node01 kubelet[2764]: I0218 10:33:14.726545    2764 server.go:206] "--pod-infra-container-image will not be pruned by the image garbage >
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.047551    2764 server.go:486] "Kubelet version" kubeletVersion="v1.31.0"
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.047578    2764 server.go:488] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.048378    2764 server.go:929] "Client rotation is on, will bootstrap in background"
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.051203    2764 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/etc/ku>
Feb 18 10:33:15 node01 kubelet[2764]: E0218 10:33:15.053902    2764 log.go:32] "RuntimeConfig from runtime service failed" err="rpc error: code = Unimpl>
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.053940    2764 server.go:1403] "CRI implementation should be updated to support RuntimeConfig when >
Feb 18 10:33:15 node01 kubelet[2764]: E0218 10:33:15.057008    2764 info.go:104] Failed to get disk map: could not parse device numbers from  for device>
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.103293    2764 server.go:744] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  def>
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.103957    2764 swap_util.go:113] "Swap is on" /proc/swaps contents="Filename                       >
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.111094    2764 container_manager_linux.go:264] "Container manager verified user specified cgroup-ro>
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.111143    2764 container_manager_linux.go:269] "Creating Container Manager object based on Node Con>
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.111316    2764 topology_manager.go:138] "Creating topology manager with none policy"
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.111325    2764 container_manager_linux.go:300] "Creating device plugin manager"
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.111411    2764 state_mem.go:36] "Initialized new in-memory state store"
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.111516    2764 kubelet.go:408] "Attempting to sync node with API server"
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.111529    2764 kubelet.go:303] "Adding static pod path" path="/etc/kubernetes/manifests"
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.111557    2764 kubelet.go:314] "Adding apiserver pod source"
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.111570    2764 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.112194    2764 kuberuntime_manager.go:262] "Container runtime initialized" containerRuntime="contai>
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.112591    2764 kubelet.go:837] "Not starting ClusterTrustBundle informer because we are in static k>
Feb 18 10:33:15 node01 kubelet[2764]: W0218 10:33:15.112656    2764 probe.go:272] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins>
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.120623    2764 server.go:1269] "Started kubelet"
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.120676    2764 server.go:163] "Starting to listen" address="0.0.0.0" port=10250
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.120747    2764 ratelimit.go:55] "Setting rate limiting for endpoint" service="podresources" qps=100>
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.121417    2764 server.go:236] "Starting to serve the podresources API" endpoint="unix:/var/lib/kube>
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.122366    2764 server.go:460] "Adding debug handlers to kubelet server"
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.123210    2764 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
Feb 18 10:33:15 node01 kubelet[2764]: E0218 10:33:15.123261    2764 oomparser.go:149] exiting analyzeLines. OOM events will not be reported.
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.123283    2764 dynamic_serving_content.go:135] "Starting controller" name="kubelet-server-cert-file>
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.123336    2764 volume_manager.go:289] "Starting Kubelet Volume Manager"
Feb 18 10:33:15 node01 kubelet[2764]: E0218 10:33:15.123377    2764 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \>
Feb 18 10:33:15 node01 kubelet[2764]: I0218 10:33:15.123384    2764 desired_state_of_world_populator.go:146] "Desired state populator starts to run"
-----------------------------------------------------
We can see error - err="failed to load kubelet config file, path: /var/lib/k>


Show kubelet config files
	node01 terminal --> ls /etc/kubernetes/

	# result: kubelet.conf  manifests  pki


Print the kubelet.config file
	node01 terminal --> cat /etc/kubernetes/kubelet.conf

# result:
------------------------------------------------------------------
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJRWtSZ1dJenkva0F3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TlRBeU1UZ3hNREkyTkRoYUZ3MHpOVEF5TVRZeE1ETXhORGhhTUJVeApFekFSQmdOVkJBTVRDbXQxWW1WeWJtVjBaWE13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUURDNWNXV05kTlF1T2dvV3NTaisrNXpzdUU1R20rRk9QL01pd3dienNjUGdzbTZzWnpzWkZBRTVTdzAKNnJZdkY4NFdDTHRhaEhIOWdZS01vYkNVUnRUSFRKc1luTEVPVTloME90anBuaTJoektTNlRFOXZKcEFhd25CVgpVcmdxQXQ1cnoreTdHYlp2Slp4T0VLNDNsc1JoV0h0Qlo0WlR4WnF1RVhvcWl1SXduVENpeVhGaFRNV2VwQ2ZMCjNkWEJzeE94OWJzU2hBaGVnN3ZwYjFWWHhLV3ZqRTFTdXZCWFZpcDZyR203ejkvUS9tL29zRXcvMlhidklWS2wKOXhOdEdleVNVM3l1KzJsR0lQb2xUa2dIZFdBZ3BZSEZUVjhmbWhqYWhrZ1h1T1JFQ1ZWZm5rTUd3cFE0NmpZTwoxNXprVnhDeFR3Sm5jd2pVSE12bjY4QldTUzdIQWdNQkFBR2pXVEJYTUE0R0ExVWREd0VCL3dRRUF3SUNwREFQCkJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXQkJUNGxQc2pLVDI2YjNTZW15QVB4bUc0L2VnZ3hEQVYKQmdOVkhSRUVEakFNZ2dwcmRXSmxjbTVsZEdWek1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQWR4SnJRcjhxVwozNjNPeFhOTDQxYTRncFhxZ283Q3NabjV6Y3dvYW1yS0Zmb25uQ0dOWDhnVjVqbkVXSWgxNnR4TzJIaisyQ2ZlClB4MEVBd0JYeDg4OExHVDlwTkYrUVpHK3BVVEUwRXFsd1RJS1lWUGpoMklSUUw1V014TDI1V0NEeUJvQ3pIbWQKSXhITkZtc05YRXFVTU00Y0YyWmhRN2d5OGhra1AzZ2xKTGF2TERXSXRjNG9kNHRvb2FvSGpWWGVLYnM1NE9xbApGdWhRUGdnMkt3SW51RDVTelgrVWpCUkFBa0l0ZEFvby9lRVhzeWFVQ2tiaDBGbXdNNDNHZUFDQ1l4aFdCVlNWCks2cnZ6cDNKWXQ3TWg4dWFhZ1JyTUJGOEV5d3h2ajdGb1FQclhpRXVtaDFMMkVzYzFNYkROcXRCM2Ziam43V0EKRzNxUWxwY0tsUTQ0Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://controlplane:6553				# wrong port
  name: default-cluster
contexts:
- context:
    cluster: default-cluster
    namespace: default
    user: default-auth
  name: default-context
current-context: default-context
kind: Config
preferences: {}
users:
- name: default-auth
  user:
    client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem
    client-key: /var/lib/kubelet/pki/kubelet-client-current.pem
------------------------------------------------------------------



Edit the  /etc/kubernetes/kubelet.conf file
	node01 terminal --> vi /etc/kubernetes/kubelet.conf

# result:
---------------------------------------------
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJRWtSZ1dJenkva0F3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TlRBeU1UZ3hNREkyTkRoYUZ3MHpOVEF5TVRZeE1ETXhORGhhTUJVeApFekFSQmdOVkJBTVRDbXQxWW1WeWJtVjBaWE13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUURDNWNXV05kTlF1T2dvV3NTaisrNXpzdUU1R20rRk9QL01pd3dienNjUGdzbTZzWnpzWkZBRTVTdzAKNnJZdkY4NFdDTHRhaEhIOWdZS01vYkNVUnRUSFRKc1luTEVPVTloME90anBuaTJoektTNlRFOXZKcEFhd25CVgpVcmdxQXQ1cnoreTdHYlp2Slp4T0VLNDNsc1JoV0h0Qlo0WlR4WnF1RVhvcWl1SXduVENpeVhGaFRNV2VwQ2ZMCjNkWEJzeE94OWJzU2hBaGVnN3ZwYjFWWHhLV3ZqRTFTdXZCWFZpcDZyR203ejkvUS9tL29zRXcvMlhidklWS2wKOXhOdEdleVNVM3l1KzJsR0lQb2xUa2dIZFdBZ3BZSEZUVjhmbWhqYWhrZ1h1T1JFQ1ZWZm5rTUd3cFE0NmpZTwoxNXprVnhDeFR3Sm5jd2pVSE12bjY4QldTUzdIQWdNQkFBR2pXVEJYTUE0R0ExVWREd0VCL3dRRUF3SUNwREFQCkJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXQkJUNGxQc2pLVDI2YjNTZW15QVB4bUc0L2VnZ3hEQVYKQmdOVkhSRUVEakFNZ2dwcmRXSmxjbTVsZEdWek1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQWR4SnJRcjhxVwozNjNPeFhOTDQxYTRncFhxZ283Q3NabjV6Y3dvYW1yS0Zmb25uQ0dOWDhnVjVqbkVXSWgxNnR4TzJIaisyQ2ZlClB4MEVBd0JYeDg4OExHVDlwTkYrUVpHK3BVVEUwRXFsd1RJS1lWUGpoMklSUUw1V014TDI1V0NEeUJvQ3pIbWQKSXhITkZtc05YRXFVTU00Y0YyWmhRN2d5OGhra1AzZ2xKTGF2TERXSXRjNG9kNHRvb2FvSGpWWGVLYnM1NE9xbApGdWhRUGdnMkt3SW51RDVTelgrVWpCUkFBa0l0ZEFvby9lRVhzeWFVQ2tiaDBGbXdNNDNHZUFDQ1l4aFdCVlNWCks2cnZ6cDNKWXQ3TWg4dWFhZ1JyTUJGOEV5d3h2ajdGb1FQclhpRXVtaDFMMkVzYzFNYkROcXRCM2Ziam43V0EKRzNxUWxwY0tsUTQ0Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://controlplane:6443				# changed from 6553 to 6443
  name: default-cluster
contexts:
- context:
    cluster: default-cluster
    namespace: default
    user: default-auth
  name: default-context
current-context: default-context
kind: Config
preferences: {}
users:
- name: default-auth
  user:
    client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem
    client-key: /var/lib/kubelet/pki/kubelet-client-current.pem
---------------------------------------------
save changes - escape, :wq!, enter


Restart the kubelet service
	node01 terminal --> service kubelet restart

Check the status of the kubelet service
	node01 terminal --> service kubelet status

# result:
---------------------------------------------
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: active (running) since Tue 2025-02-18 10:48:43 UTC; 30s ago
       Docs: https://kubernetes.io/docs/
   Main PID: 11163 (kubelet)
      Tasks: 23 (limit: 77143)
     Memory: 25.0M
     CGroup: /system.slice/kubelet.service
             └─11163 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/>
---------------------------------------------
The status is running - Active: active (running)


Check the logs for errors
	node01 terminal --> journalctl -u kubelet

	# in the logs there are no error


Exit node01
	node01 terminal --> exit

Check nodes status
	terminal --> k get nodes

	# result:
	NAME           STATUS   ROLES           AGE   VERSION
	controlplane   Ready    control-plane   20m   v1.31.0
	node01         Ready    <none>          19m   v1.31.0

	# all nodes are running

- click 'Check' button


Common Worker node troubleshooting process
	- check node status from the controlplane node
	- describe nodes and check for info about the issue
	- connect to the failed node
		- check kubelet service (status)
		- check the logs of the kubelet service
	- continue with the information gathered






=======================================
Section 15 303. Network Troubleshooting
=======================================

Network Plugin in Kubernetes
============================

There are several plugins available and these are some.

1. Weave Net:

To install,
	terminal --> kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml

You can find details about the network plugins in the following documentation :
	- https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy


2. Flannel :

To install,
	terminal --> kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml

Note: As of now flannel does not support kubernetes network policies.



3. Calico :

To install,
	terminal --> curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml -O

Apply the manifest using the following command.
	terminal --> kubectl apply -f calico.yaml

Calico is said to have most advanced cni network plugin.



In CKA and CKAD exam, you won't be asked to install the CNI plugin. But if asked you will be provided with the exact URL to install it.

Note: If there are multiple CNI configuration files in the directory, the kubelet uses the configuration file that comes first by name in lexicographic order.





DNS in Kubernetes
=================
Kubernetes uses CoreDNS. CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS.

Memory and Pods

In large scale Kubernetes clusters, CoreDNS's memory usage is predominantly affected by the number of Pods and Services in the cluster. Other factors include the size of the filled DNS answer cache, and the rate of queries received (QPS) per CoreDNS instance.


Kubernetes resources for coreDNS are:   

1. a service account named coredns,
2. cluster-roles named coredns and kube-dns
3. clusterrolebindings named coredns and kube-dns, 
4. a deployment named coredns,
5. a configmap named coredns and a
6. service named kube-dns.


While analyzing the coreDNS deployment you can see that the the Corefile plugin consists of important configuration which is defined as a configmap.

Port 53 is used for for DNS resolution.

--------------------------------------------------------
    kubernetes cluster.local in-addr.arpa ip6.arpa {
       pods insecure
       fallthrough in-addr.arpa ip6.arpa
       ttl 30
    }
--------------------------------------------------------

This is the backend to k8s for cluster.local and reverse domains.
	terminal --> proxy . /etc/resolv.conf


Forward out of cluster domains directly to right authoritative DNS server.



Troubleshooting issues related to coreDNS

1. If you find CoreDNS pods in pending state first check network plugin is installed.
2. coredns pods have CrashLoopBackOff or Error state

If you have nodes that are running SELinux with an older version of Docker you might experience a scenario where the coredns pods are not starting. To solve that you can try one of the following options:

a)Upgrade to a newer version of Docker.

b)Disable SELinux.

c)Modify the coredns deployment to set allowPrivilegeEscalation to true:
	terminal --> kubectl -n kube-system get deployment coredns -o yaml | \
sed 's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g' | \
kubectl apply -f -


d)Another cause for CoreDNS to have CrashLoopBackOff is when a CoreDNS Pod deployed in Kubernetes detects a loop.


There are many ways to work around this issue, some are listed here:



- Add the following to your kubelet config yaml: resolvConf: <path-to-your-real-resolv-conf-file> This flag tells kubelet to pass an alternate resolv.conf to Pods. For systems using systemd-resolved, /run/systemd/resolve/resolv.conf is typically the location of the "real" resolv.conf, although this can be different depending on your distribution.

- Disable the local DNS cache on host nodes, and restore /etc/resolv.conf to the original.

- A quick fix is to edit your Corefile, replacing forward . /etc/resolv.conf with the IP address of your upstream DNS, for example forward . 8.8.8.8. But this only fixes the issue for CoreDNS, kubelet will continue to forward the invalid resolv.conf to all default dnsPolicy Pods, leaving them unable to resolve DNS.



3. If CoreDNS pods and the kube-dns service is working fine, check the kube-dns service has valid endpoints.
	terminal --> kubectl -n kube-system get ep kube-dns

If there are no endpoints for the service, inspect the service and make sure it uses the correct selectors and ports.


Kube-Proxy
==========
kube-proxy is a network proxy that runs on each node in the cluster. kube-proxy maintains network rules on nodes. These network rules allow network communication to the Pods from network sessions inside or outside of the cluster.

In a cluster configured with kubeadm, you can find kube-proxy as a daemonset.

kubeproxy is responsible for watching services and endpoint associated with each service. When the client is going to connect to the service using the virtual IP the kubeproxy is responsible for sending traffic to actual pods.



If you run a 
	terminal --> kubectl describe ds kube-proxy -n kube-system 

you can see that the kube-proxy binary runs with following command inside the kube-proxy container.


Command:
	terminal --> /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=$(NODE_NAME)
 

So it fetches the configuration from a configuration file ie, /var/lib/kube-proxy/config.conf and we can override the hostname with the node name of at which the pod is running.

In the config file we define the clusterCIDR, kubeproxy mode, ipvs, iptables, bindaddress, kube-config etc.

 

Troubleshooting issues related to kube-proxy
1. Check kube-proxy pod in the kube-system namespace is running.

2. Check kube-proxy logs.

3. Check configmap is correctly defined and the config file for running kube-proxy binary is correct.

4. kube-config is defined in the config map.

5. check kube-proxy is running inside the container

# netstat -plan | grep kube-proxy
tcp        0      0 0.0.0.0:30081           0.0.0.0:*               LISTEN      1/kube-proxy
tcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      1/kube-proxy
tcp        0      0 172.17.0.12:33706       172.17.0.12:6443        ESTABLISHED 1/kube-proxy
tcp6       0      0 :::10256                :::*                    LISTEN      1/kube-proxy


References:
Debug Service issues:
	- https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/

DNS Troubleshooting:
	- https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/





=======================================================
Section 15 304. Practice Test - Network Troubleshooting
=======================================================


1. **Troubleshooting Test 1:** A simple 2 tier application is deployed in the triton namespace. It must display a green web page on success. Click on the app tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
--------------------------------------------------------------------------------------------------------------------------
Stick to the given architecture. Use the same names and port numbers as given in the below architecture diagram. Feel free to edit, delete or recreate objects as necessary.



We want to set current context to the specific namespace to avoid typing the namespace param in every command

Show context help commands
	terminal --> kubectl config --help			
# set-context       Set a context entry in kubeconfig
# Usage:
  kubectl config set-context [NAME | --current] [--cluster=cluster_nickname] [--user=user_nickname]
[--namespace=namespace] [options]

Show context config help commands
	terminal --> kubectl config set-context --help		# --namespace='':

Set the specific context (default namespace)
	terminal --> kubectl config set-context --current --namespace=triton
	
	# kubectl				# common kubernetes command
	# config set-context --current		# modify current default context
	# --namespace=triton			# set the namespace

	# Context "default" modified.

When we ope nthe app we receive error message: 502 Bad Gateway

Check all controlplane components
Show controlplane pods (all controlplane components)
	terminal --> k get pods -n kube-system

# result:
--------------------------------------------------------
NAME                                   READY   STATUS    RESTARTS   AGE
coredns-6f6b679f8f-dfvm2               1/1     Running   0          17m
coredns-6f6b679f8f-wjmnb               1/1     Running   0          17m
etcd-controlplane                      1/1     Running   0          17m
kube-apiserver-controlplane            1/1     Running   0          17m
kube-controller-manager-controlplane   1/1     Running   0          17m
kube-proxy-n78c5                       1/1     Running   0          17m
kube-scheduler-controlplane            1/1     Running   0          17m
--------------------------------------------------------
All components are working properly

List pods
	terminal --> k get pods
	
	# result:
	NAME                           READY   STATUS              RESTARTS   AGE
	mysql                          0/1     ContainerCreating   0          2m39s	# not working pod
	webapp-mysql-d89894b4b-55ww9   0/1     ContainerCreating   0          2m39s	# not working pod


Show details for mysql pod 
	terminal --> k describe pod mysql

# result:
------------------------------------------------------
Name:             mysql
Namespace:        triton
Priority:         0
Service Account:  default
Node:             controlplane/192.168.121.17
Start Time:       Tue, 18 Feb 2025 11:09:42 +0000
Labels:           name=mysql
Annotations:      <none>
Status:           Pending
IP:               
IPs:              <none>
Containers:
  mysql:
    Container ID:   
    Image:          mysql:5.6
    Image ID:       
    Port:           3306/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Environment:
      MYSQL_ROOT_PASSWORD:  paswrd
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5dlpc (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  kube-api-access-5dlpc:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason                  Age                  From               Message
  ----     ------                  ----                 ----               -------
  Normal   Scheduled               12m                  default-scheduler  Successfully assigned triton/mysql to controlplane
  Warning  FailedMount             12m                  kubelet            MountVolume.SetUp failed for volume "kube-api-access-5dlpc" : failed to sync configmap cache: timed out waiting for the condition
  Warning  FailedCreatePodSandBox  11m                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "342dc7a9bd8d6d7b15c534c661c2247e8869b1908d7d781fa9d37cacc01d1206": plugin type="weave-net" name="weave" failed (add): unable to allocate IP address: Post "http://127.0.0.1:6784/ip/342dc7a9bd8d6d7b15c534c661c2247e8869b1908d7d781fa9d37cacc01d1206": dial tcp 127.0.0.1:6784: connect: connection refused
  Normal   SandboxChanged          116s (x48 over 11m)  kubelet            Pod sandbox changed, it will be killed and re-created.
Error from server (NotFound): pods "mysqlk" not found
Error from server (NotFound): pods "describe" not found
Error from server (NotFound): pods "pod" not found
------------------------------------------------------

We can see that our config is for "weave-net" plugin


Dowload the installation files
	terminal --> wget https://reweave.azurewebsites.net/k8s/v1.29/net.yaml

Find the range we have to configure in WEAVE
--------------------------------------------

List Pods is default namespace to find kube-proxy
	terminal --> k get pods -n kube-system

# reult:
NAME                                   READY   STATUS    RESTARTS   AGE
coredns-6f6b679f8f-dfvm2               1/1     Running   0          34m
coredns-6f6b679f8f-wjmnb               1/1     Running   0          34m
etcd-controlplane                      1/1     Running   0          34m
kube-apiserver-controlplane            1/1     Running   0          34m
kube-controller-manager-controlplane   1/1     Running   0          34m
kube-proxy-n78c5                       1/1     Running   0          34m		# describe this pod
kube-scheduler-controlplane            1/1     Running   0          34m


Describe kube-proxy pod in the default namespace
	terminal --> k describe pod kube-proxy-n78c5 -n kube-system

kube-system
-----------------------------------------------------------------------
Name:                 kube-proxy-n78c5
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Service Account:      kube-proxy
Node:                 controlplane/192.168.121.17
Start Time:           Tue, 18 Feb 2025 10:57:25 +0000
Labels:               controller-revision-hash=5976bc5f75
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   192.168.121.17
IPs:
  IP:           192.168.121.17
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://57f75b4db7c480d488ce0f006f577c252fb0154daa3842505c291049b5bf8d65
    Image:         registry.k8s.io/kube-proxy:v1.31.0
    Image ID:      registry.k8s.io/kube-proxy@sha256:c727efb1c6f15a68060bf7f207f5c7a765355b7e3340c513e582ec819c5cd2fe
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf			# 1. see the loaded configs from dir
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Tue, 18 Feb 2025 10:57:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)						# 2. search for volume kube-proxy
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sb4fd (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy						# describe configmap kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-sb4fd:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  37m   default-scheduler  Successfully assigned kube-system/kube-proxy-n78c5 to controlplane
  Normal  Pulled     37m   kubelet            Container image "registry.k8s.io/kube-proxy:v1.31.0" already present on machine
  Normal  Created    37m   kubelet            Created container kube-proxy
  Normal  Started    37m   kubelet            Started container kube-proxy
-----------------------------------------------------------------------


List configmaps in default namespace
	terminal --> k get cm -n kube-system
	
	# result:
NAME                                                   DATA   AGE
coredns                                                1      39m
extension-apiserver-authentication                     6      40m
kube-apiserver-legacy-service-account-token-tracking   1      40m
kube-proxy                                             2      39m		# this is the target configmap
kube-root-ca.crt                                       1      39m
kubeadm-config                                         1      39m
kubelet-config                                         1      39m
weave-net                                              0      39m


Print configmap kube-proxy
	terminal --> k describe cm kube-proxy -n kube-system

kube-proxy configmap
----------------------------------------------------------------
Name:         kube-proxy
Namespace:    kube-system
Labels:       app=kube-proxy
Annotations:  kubeadm.kubernetes.io/component-config.hash: sha256:906b8697200819e8263843f43965bb3614545800b82206dcee8ef93a08bc4f4b

Data
====
config.conf:
----
apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 0.0.0.0
bindAddressHardFail: false
clientConnection:
  acceptContentTypes: ""
  burst: 0
  contentType: ""
  kubeconfig: /var/lib/kube-proxy/kubeconfig.conf
  qps: 0		
clusterCIDR: 10.244.0.0/16					# this is the range for WEAVE - 10.244.0.0/16
configSyncPeriod: 0s
conntrack:
  maxPerCore: null
  min: null
  tcpBeLiberal: false
  tcpCloseWaitTimeout: null
  tcpEstablishedTimeout: null
  udpStreamTimeout: 0s
  udpTimeout: 0s
detectLocal:
  bridgeInterface: ""
  interfaceNamePrefix: ""
detectLocalMode: ""
enableProfiling: false
healthzBindAddress: ""
hostnameOverride: ""
iptables:
  localhostNodePorts: null
  masqueradeAll: false
  masqueradeBit: null
  minSyncPeriod: 0s
  syncPeriod: 0s
ipvs:
  excludeCIDRs: null
  minSyncPeriod: 0s
  scheduler: ""
  strictARP: false
  syncPeriod: 0s
  tcpFinTimeout: 0s
  tcpTimeout: 0s
  udpTimeout: 0s
kind: KubeProxyConfiguration
logging:
  flushFrequency: 0
  options:
    json:
      infoBufferSize: "0"
    text:
      infoBufferSize: "0"
  verbosity: 0
metricsBindAddress: ""
mode: ""
nftables:
  masqueradeAll: false
  masqueradeBit: null
  minSyncPeriod: 0s
  syncPeriod: 0s
nodePortAddresses: null
oomScoreAdj: null
portRange: ""
showHiddenMetricsForVersion: ""
winkernel:
  enableDSR: false
  forwardHealthCheckVip: false
  networkName: ""
  rootHnsEndpointName: ""
  sourceVip: ""

kubeconfig.conf:
----
apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    server: https://controlplane:6443
  name: default
contexts:
- context:
    cluster: default
    namespace: default
    user: default
  name: default
current-context: default
users:
- name: default
  user:
    tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token


BinaryData
====

Events:  <none>
----------------------------------------------------------------

Edit the dowloaded config file
	terminal --> vi net.yaml

net.yaml
----------------------------------------------------------------
...
          containers:
            - name: weave
              command:
                - /home/weave/launch.sh
              env:
                - name: IPALLOC_RANGE			# added
                  value: 10.244.0.0/16			# added
                - name: INIT_CONTAINER
                  value: 'true'
                - name: HOSTNAME
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: spec.nodeName
                - name: CHECKPOINT_DISABLE
                  value: '1'
              image: rajchaudhuri/weave-kube:2.9.0
...
----------------------------------------------------------------
save changes - escape, :wq!, enter


Verify changes
	terminal --> cat net.yaml

Apply the file net.yaml
	terminal --> kubectl apply -f net.yaml

Wait until pods are created and list them
	terminal --> k get pods -n kube-system

# result:
---------------------------------------------------
NAME                                   READY   STATUS    RESTARTS   AGE
coredns-6f6b679f8f-dfvm2               1/1     Running   0          45m
coredns-6f6b679f8f-wjmnb               1/1     Running   0          45m
etcd-controlplane                      1/1     Running   0          45m
kube-apiserver-controlplane            1/1     Running   0          45m
kube-controller-manager-controlplane   1/1     Running   0          45m
kube-proxy-n78c5                       1/1     Running   0          45m
kube-scheduler-controlplane            1/1     Running   0          45m
weave-net-m8q98                        2/2     Running   0          28s
---------------------------------------------------

Wait a few minutes for changes to take effect

- click 'Çheck' button





2. **Troubleshooting Test 2:** The same 2 tier application is having issues again. It must display a green web page on success. Click on the app tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
----------------------------------------------------------------------------------------------------------------------------------
Stick to the given architecture. Use the same names and port numbers as given in the below architecture diagram. Feel free to edit, delete or recreate objects as necessary.


When we ope nthe app we receive error message: 502 Bad Gateway

Check all controlplane components
Show controlplane pods (all controlplane components)
	terminal --> k get pods -n kube-system

# result:
--------------------------------------------------------
NAME                                   READY   STATUS    RESTARTS      AGE
coredns-6f6b679f8f-dfvm2               1/1     Running   0             47m
coredns-6f6b679f8f-wjmnb               1/1     Running   0             47m
etcd-controlplane                      1/1     Running   0             47m
kube-apiserver-controlplane            1/1     Running   0             47m
kube-controller-manager-controlplane   1/1     Running   0             47m
kube-proxy-gz95x                       0/1     Error     4 (59s ago)   104s		# not working controlplane pod
kube-scheduler-controlplane            1/1     Running   0             47m
weave-net-m8q98                        2/2     Running   0             3m6s
--------------------------------------------------------


Show details for the failed pod
	terminal --> k describe pod kube-proxy-gz95x -n kube-system

# result:
--------------------------------------------------------
Name:                 kube-proxy-gz95x
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Service Account:      kube-proxy
Node:                 controlplane/192.168.121.17
Start Time:           Tue, 18 Feb 2025 11:43:28 +0000
Labels:               controller-revision-hash=578f8cdbd7
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   192.168.121.17
IPs:
  IP:           192.168.121.17
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://0368c901c30e09cd79684b6924aa92490a6ca0fbc1ba7562b6afeac16db031aa
    Image:         registry.k8s.io/kube-proxy:v1.26.0
    Image ID:      registry.k8s.io/kube-proxy@sha256:1e9bbe429e4e2b2ad32681c91deb98a334f1bf4135137df5f84f9d03689060fe
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/configuration.conf
      --hostname-override=$(NODE_NAME)
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Tue, 18 Feb 2025 11:45:08 +0000
      Finished:     Tue, 18 Feb 2025 11:45:08 +0000
    Ready:          False
    Restart Count:  4
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xp4dw (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-xp4dw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                  From               Message
  ----     ------     ----                 ----               -------
  Normal   Scheduled  3m                   default-scheduler  Successfully assigned kube-system/kube-proxy-gz95x to controlplane
  Normal   Pulling    2m59s                kubelet            Pulling image "registry.k8s.io/kube-proxy:v1.26.0"
  Normal   Pulled     2m57s                kubelet            Successfully pulled image "registry.k8s.io/kube-proxy:v1.26.0" in 1.7s (1.7s including waiting). Image size: 21536465 bytes.
  Normal   Created    80s (x5 over 2m57s)  kubelet            Created container kube-proxy
  Normal   Started    80s (x5 over 2m57s)  kubelet            Started container kube-proxy
  Normal   Pulled     80s (x4 over 2m56s)  kubelet            Container image "registry.k8s.io/kube-proxy:v1.26.0" already present on machine
  Warning  BackOff    79s (x9 over 2m55s)  kubelet            Back-off restarting failed container kube-proxy in pod kube-proxy-gz95x_kube-system(9b1ac3a3-320f-4f50-9af9-d4d35b19795e)
--------------------------------------------------------
No useful information can be seen


Show logs of the pod
	terminal --> k logs kube-proxy-gz95x -n kube-system

# result:
E0218 11:46:32.709862       1 run.go:74] "command failed" err="failed complete: open /var/lib/kube-proxy/configuration.conf: no such file or directory"


List directories and files
	terminal --> ls /var/lib/
	
	# no kube-proxy directory is found

Create the directory
	terminal --> sudo mkdir -p /var/lib/kube-proxy

Verify directory creation
	terminal --> ls /var/lib/

Create the configuration file
	terminal --> vi /var/lib/kube-proxy/configuration.conf

configuration.conf
---------------------------------------------
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
bindAddress: 0.0.0.0
clientConnection:
  kubeconfig: /var/lib/kube-proxy/kubeconfig.conf
clusterCIDR: 10.244.0.0/16
mode: "iptables"
---------------------------------------------


List configmaps
	terminal --> kubectl get configmap -n kube-system kube-proxy -o yaml

# result:
---------------------------------------------
apiVersion: v1
data:
  config.conf: |-
    apiVersion: kubeproxy.config.k8s.io/v1alpha1
    bindAddress: 0.0.0.0
    bindAddressHardFail: false
    clientConnection:
      acceptContentTypes: ""
      burst: 0
      contentType: ""
      kubeconfig: /var/lib/kube-proxy/kubeconfig.conf			# fix the used file
      qps: 0
    clusterCIDR: 10.244.0.0/16
    configSyncPeriod: 0s
    conntrack:
      maxPerCore: null
      min: null
      tcpBeLiberal: false
      tcpCloseWaitTimeout: null
      tcpEstablishedTimeout: null
      udpStreamTimeout: 0s
      udpTimeout: 0s
    detectLocal:
      bridgeInterface: ""
      interfaceNamePrefix: ""
    detectLocalMode: ""
    enableProfiling: false
    healthzBindAddress: ""
    hostnameOverride: ""
    iptables:
      localhostNodePorts: null
      masqueradeAll: false
      masqueradeBit: null
      minSyncPeriod: 0s
      syncPeriod: 0s
    ipvs:
      excludeCIDRs: null
      minSyncPeriod: 0s
      scheduler: ""
      strictARP: false
      syncPeriod: 0s
      tcpFinTimeout: 0s
      tcpTimeout: 0s
      udpTimeout: 0s
    kind: KubeProxyConfiguration
    logging:
      flushFrequency: 0
      options:
        json:
          infoBufferSize: "0"
        text:
          infoBufferSize: "0"
      verbosity: 0
    metricsBindAddress: ""
    mode: ""
    nftables:
      masqueradeAll: false
      masqueradeBit: null
      minSyncPeriod: 0s
      syncPeriod: 0s
    nodePortAddresses: null
    oomScoreAdj: null
    portRange: ""
    showHiddenMetricsForVersion: ""
    winkernel:
      enableDSR: false
      forwardHealthCheckVip: false
      networkName: ""
      rootHnsEndpointName: ""
      sourceVip: ""
  kubeconfig.conf: |-
    apiVersion: v1
    kind: Config
    clusters:
    - cluster:
        certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        server: https://controlplane:6443
      name: default
    contexts:
    - context:
        cluster: default
        namespace: default
        user: default
      name: default
    current-context: default
    users:
    - name: default
      user:
        tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
kind: ConfigMap
metadata:
  annotations:
    kubeadm.kubernetes.io/component-config.hash: sha256:906b8697200819e8263843f43965bb3614545800b82206dcee8ef93a08bc4f4b
  creationTimestamp: "2025-02-18T11:57:10Z"
  labels:
    app: kube-proxy
  name: kube-proxy
  namespace: kube-system
  resourceVersion: "233"
  uid: d28fea6f-79c0-413b-a81b-00630b78fc3d
---------------------------------------------



Edit the configmap and fix the used file
	terminal --> k edit cm kube-proxy -n kube-system

---------------------------------------------------------------
apiVersion: v1
data:
  config.conf: |-
    apiVersion: kubeproxy.config.k8s.io/v1alpha1
    bindAddress: 0.0.0.0
    bindAddressHardFail: false
    clientConnection:
      acceptContentTypes: ""
      burst: 0
      contentType: ""
      kubeconfig: /var/lib/kube-proxy/configuration.conf		# changed to configuration.conf
      qps: 0
    clusterCIDR: 10.244.0.0/16
    configSyncPeriod: 0s
    conntrack:
      maxPerCore: null
      min: null
      tcpBeLiberal: false
      tcpCloseWaitTimeout: null
      tcpEstablishedTimeout: null
      udpStreamTimeout: 0s
      udpTimeout: 0s
    detectLocal:
      bridgeInterface: ""
      interfaceNamePrefix: ""
    detectLocalMode: ""
    enableProfiling: false
    healthzBindAddress: ""
    hostnameOverride: ""
    iptables:
      localhostNodePorts: null
      masqueradeAll: false
      masqueradeBit: null
      minSyncPeriod: 0s
      syncPeriod: 0s
    ipvs:
      excludeCIDRs: null
      minSyncPeriod: 0s
      scheduler: ""
      strictARP: false
      syncPeriod: 0s
      tcpFinTimeout: 0s
      tcpTimeout: 0s
      udpTimeout: 0s
    kind: KubeProxyConfiguration
    logging:
      flushFrequency: 0
      options:
        json:
          infoBufferSize: "0"
        text:
          infoBufferSize: "0"
      verbosity: 0
    metricsBindAddress: ""
    mode: ""
    nftables:
      masqueradeAll: false
      masqueradeBit: null
      minSyncPeriod: 0s
      syncPeriod: 0s
    nodePortAddresses: null
    oomScoreAdj: null
    portRange: ""
    showHiddenMetricsForVersion: ""
    winkernel:
      enableDSR: false
      forwardHealthCheckVip: false
      networkName: ""
      rootHnsEndpointName: ""
      sourceVip: ""
  kubeconfig.conf: |-
    apiVersion: v1
    kind: Config
    clusters:
    - cluster:
        certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        server: https://controlplane:6443
      name: default
    contexts:
    - context:
        cluster: default
        namespace: default
        user: default
      name: default
    current-context: default
    users:
    - name: default
      user:
        tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
kind: ConfigMap
metadata:
  annotations:
    kubeadm.kubernetes.io/component-config.hash: sha256:906b8697200819e8263843f43965bb3614545800b82206dcee8ef93a08bc4f4b
  creationTimestamp: "2025-02-18T11:57:10Z"
  labels:
    app: kube-proxy
  name: kube-proxy
  namespace: kube-system
  resourceVersion: "233"
  uid: d28fea6f-79c0-413b-a81b-00630b78fc3d
---------------------------------------------------------------
save changes - escape, :wq!, enter

# result: configmap/kube-proxy edited


Set permissions
	terminal --> sudo chmod 755 /var/lib/kube-proxy
	terminal --> sudo chmod 644 /var/lib/kube-proxy/configuration.conf

Delete the kube-proxy pod
	terminal --> k delete pod -n kube-system kube-proxy-gz95x

Wait few minutes to recreate kube-proxy pod

- click 'Check' button

