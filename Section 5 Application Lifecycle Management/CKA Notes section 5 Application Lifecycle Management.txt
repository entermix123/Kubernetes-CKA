CONTENT

Section 5 98. Rolling Updates and Rollbacks
Section 5 99. Practice Test - Rolling Updates and Rollbacks
Section 5 101. Configuration Application
Section 5 102. Commands
Section 5 103. Commands and Arguments
Section 5 104. Practice Test - Commands and Arguments
Section 5 106. Configure Environment Varibles in Applications
Section 5 107. Configure ConfigMaps in Applications
Section 5 108. Practice Test - Environment Variables
Section 5 110. Configure Secrets in Applications
Section 5 111. A Note about Secrets
Section 5 112. Additional Resource
Section 5 113. Practice Test - Secrets
Section 5 115. Encrypting Secret Data at Rest
Section 5 117. Multi-Container Pods
Section 5 118. Practice Test - Multi-Container Pods
Section 5 119. Multi-Container Pods Design Patterns
Section 5 120. InitContainers
Section 5 122. Practice Test - InitContainers
Section 5 123. Lelf Healing Applications
Section 5 124. Introduction to Autscaling
Section 5 125. Horizontal Pod Autoscaler (HPA)
Section 5 126. Practice Test - Manual Scaling HPA
Section 5 127. Practice Test - HPA
Section 5 128. Inplace Resize of Pods
Section 5 129. Vertical Pod Autoscaling (VPA)
Section 5 130. Practice Test - Install VPA
Section 5 131. Practice Test - Modifing CPU reousrces in VPA


===========================================
Section 5 98. Rolling Updates and Rollbacks
===========================================


Explain Rollout and Versioning
==============================

When we first create a deployment (example: nginx:1.7.0) it creates a Rollout. A new rollout create a new deployment revision - Revision 1.

When the application container is updated (nginx:1.7.1), a new Rollout is triggered and a new deployment revision is created - Revision 2.


Rollout Commands
----------------

Show rollouts status
	terminal --> kubectl rollout status deployment/myapp-deployment

	# kubectl 				- common kubernetes command
	# rollout status			- information about obejct required
	# deployment/myapp-deployment		- taget object


Show revisions and history of rollout
	terminal --> kubectl rollout history deployment/myapp-deployment

	# kubectl 				- common kubernetes command
	# rollout history			- information about obejct required
	# deployment/myapp-deployment		- taget object



Deployment Strategies
=====================

1. Reacreate Strategy - Destroy all application pods at ones and create all pods again with the new version
	- this creates a downtime of the app, because there is period of no working application 
	- not the default deployment strategy
	- not recommended


2. Rolling Update Strategy - take down and update each pod one by one
	- this strategy do not have downtime period
	- default kubernetes strategy
	- recommended


How we update deployment
========================

Make changes to the deployemnt-definition file

deployment-definition.yaml
------------------------------------------------
apiVersion: v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
	app: myapp
	type: front-end
    spec:
      containers:
      - name: nginx-container
	image: nginx:1.7.1		# add version
  replicas: 3
  selector:
    matchLabels:
       type: front-end
------------------------------------------------


Update command we use usually
	temrinal --> kubectl apply -f deployment-definition.yaml

	# new rollout is triggered 
	# new revision of the deployment is created


Image update command
	terminal --> kubectl set image deployemnt/myapp-deployment nginx-container=nginx:1.9.1

	# this command DO NOT update deployemnt-definition.yaml file !!! Not Recommneded !!!



Find what strategy was used in update
-------------------------------------

Show deployemnt details
	terminal --> kubectl describe deployment my-depoyment

Under 'StrategyType' field we can see 'Recreate' or 'RollingUpdate' values

Also messages with information about setting replicas to '0' laso means that 'Recreate' strategy was used



How RollingUpdate is working?
-----------------------------

When new deployment is created with 5 replicas, it first creates a ReplicaSet automatically that creates 5 replicas.

When Upgrade is made a new ReplicaSet is created. The two replicasets are working simultaneously. Old ReplicaSet deletes one pod and the new ReplicaSet create one pod and so on.


How Rollback work?
------------------

Star a rollback
	terminal --> kubectl rollout undo deployment/myapp-deployment

	# the deployment will destroy the pods in the last ReplicaSet and restore the pods in the previous ReplicaSet

We can trace this process by listing ReplicaSets before and after a rollback command
	terminal --> kubectl get replicasets



Commands Summarize
------------------

Craete deployemnt
	terminal --> kubectl create deployment -f deployment-definition.yaml

List deployments
	terminal --> kubectl get deployments

Update deployemnts
	terminal --> kubectl apply -f deployment-definition.yaml			# recommended
	or
	terminal --> kubectl set image deployemnt/myapp-deployment nginx=nginx:1.9.1	# not recommended, DO NOT update def file

Show deployemnt status
	terminal --> kubect rollout status deployemnt/myapp-deployment

Show deployment history
	terminal --> kubectl rollout history deployemnt/myapp-deployment


===========================================================
Section 5 99. Practice Test - Rolling Updates and Rollbacks
===========================================================

We will use 'k' alias for 'kubectl' command. We can see all alias with
	terminal --> alias 

1. We have deployed a simple web application. Inspect the PODs and the Services
-------------------------------------------------------------------------------
Wait for the application to fully deploy and view the application using the link called Webapp Portal above your terminal.

List pods
	terminal --> k get pods

List deployemnts
	terminal --> k get deploy

Click on 'Webapp Portal' above top right corner of the console
	- window with the working app should open

- click 'Ok' button



2. What is the current color of the web application?
----------------------------------------------------
Access the Webapp Portal.

Click on 'Webapp Portal' above top right corner of the console
	- window with the working app should open

- choose 'blue' as answer (or whatever color is the background of the application)



3. Run the script named curl-test.sh to send multiple requests to test the web application. Take a note of the output.
----------------------------------------------------------------------------------------------------------------------
Execute the script at /root/curl-test.sh.


Run command:
	terminal --> ./curl-test.sh

	# result:
	Hello, Application Version: v1 ; Color: blue OK

	Hello, Application Version: v1 ; Color: blue OK

	Hello, Application Version: v1 ; Color: blue OK
	...

- click 'Ok' button




4. Inspect the deployment and identify the number of PODs deployed by it
------------------------------------------------------------------------

List deployments
	terminal --> k get deploy

	# result:
	NAME       READY   UP-TO-DATE   AVAILABLE   AGE
	frontend   4/4     4            4           106s

- choose '4' as answer



5. What container image is used to deploy the applications?
-----------------------------------------------------------

List deployemnts
	terminal --> k get deploy

	# result:
	NAME       READY   UP-TO-DATE   AVAILABLE   AGE
	frontend   4/4     4            4           2m27s

Show used image in the target deployemnt
	terminal --> k describe deploy frontend | grep Image

	# result:     Image:         kodekloud/webapp-color:v1

OR 

List pods
	terminal --> k get pods

Show image used in one of the pods
	terminal --> k describe pod frontend-7b5df69f4-bms77 | grep Image

- choose 'kodekloud/webapp-color:v1' as answer



6. Inspect the deployment and identify the current strategy
-----------------------------------------------------------

List deployemnts
	terminal --> k get deploy

Show used strategy in the target deployemnt
	terminal --> k describe deploy frontend | grep StrategyType

- choose 'RollingUpdate' as answer



7. If you were to upgrade the application now what would happen?
----------------------------------------------------------------

- choose 'PODs are upgraded few at a time' as answer



8. Let us try that. Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v2
---------------------------------------------------------------------------------------------------------------
Do not delete and re-create the deployment. Only set the new image name for the existing deployment.

Task approach
	1. Edit the existing deployment and apply changes

List deployments
	terminal --> k get deploy

	# result:
	NAME       READY   UP-TO-DATE   AVAILABLE   AGE
	frontend   4/4     4            4           5m49s


Option 1:
---------
Update deployemnt configs
	terminal --> k edit deploy frontend

frontedn deployment
------------------------------------------------
...
spec:
  minReadySeconds: 20
  progressDeadlineSeconds: 600
  replicas: 4
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: webapp
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: webapp
    spec:
      containers:
      - image: kodekloud/webapp-color:v2			# changed
        imagePullPolicy: IfNotPresent
...
------------------------------------------------
save changes - escape, :wq!, enter

# result: deployment.apps/frontend edited

Verify the change
	terminal --> k describe deploy frontend

	# We can see changed line 'Image:         kodekloud/webapp-color:v2'


Option 2:
---------
Show details of the deployment and copy the container name
	terminal --> k describe deployment frontend

	# result:
	-----------------
	...
	  Containers:
	   simple-webapp:
	...
	-----------------

	# container name is 'simple-webapp'


Update the deployment without updating the deployment config
	terminal --> k set image deploy frontend simple-webapp=kodekloud/webapp-color:v2

	# k 						- common kubernetes command
	# set image					- update deployment image
	# deploy					- type object
	# frontend					- target object name
	# simple-webapp=kodekloud/webapp-color:v2	- container name and imagename

Verify the change
	terminal --> k describe deploy frontend


- click 'Check' button




9. Run the script curl-test.sh again. Notice the requests now hit both the old and newer versions. However none of them fail.
-----------------------------------------------------------------------------------------------------------------------------
Execute the script at /root/curl-test.sh.

Run tests
	terminal --> ./curl-test.sh

	# result:
	Hello, Application Version: v2 ; Color: green OK

	Hello, Application Version: v2 ; Color: green OK

	Hello, Application Version: v2 ; Color: green OK
	...

	# we can see that there is a new colors in the tests results
	# if we run the tests few times, all colors wull be changed 
	# that is how rollingupdate work

- click 'Ok' button



10 . Up to how many PODs can be down for upgrade at a time
----------------------------------------------------------
Consider the current strategy settings and number of PODs - 4

List pods
	terminal --> k get pods

	# result:
	NAME                        READY   STATUS    RESTARTS   AGE
	frontend-854b57fbbf-ccrbc   1/1     Running   0          6m56s
	frontend-854b57fbbf-fts95   1/1     Running   0          6m34s
	frontend-854b57fbbf-nqzpn   1/1     Running   0          6m56s
	frontend-854b57fbbf-tws7p   1/1     Running   0          6m34s

	# we have 4 pods


Show details about the deployment
	terminal --> k describe deploy frontend

# in the result We have fields:
-----------------------------------
...
	MinReadySeconds:        20
	RollingUpdateStrategy:  25% max unavailable, 25% max surge
...
-----------------------------------

- choose '1' as answer



11. Change the deployment strategy to Recreate
----------------------------------------------
Delete and re-create the deployment if necessary. Only update the strategy type for the existing deployment.


List deployemnts
	terminal --> k get deploy
	
	# result:
	NAME       READY   UP-TO-DATE   AVAILABLE   AGE
	frontend   4/4     4            4           15m


Update deployemnt configs
	terminal --> k edit deploy frontend

frontedn deployment
------------------------------------------------
...
spec:
  minReadySeconds: 20
  progressDeadlineSeconds: 600
  replicas: 4
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: webapp
  strategy:						# removed additional lines
    type: Recreate					# changed
  template:
...
------------------------------------------------
save changes - escape, :wq!, enter

# result: deployment.apps/frontend edited


Verify the change
	terminal --> k describe deploy frontend | grep StrategyType

	# result: StrategyType:       Recreate


- click 'Check' button




12. Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v3
-----------------------------------------------------------------------------------------------
Do not delete and re-create the deployment. Only set the new image name for the existing deployment.

List deployments
	terminal --> k get deploy

	# result:
	NAME       READY   UP-TO-DATE   AVAILABLE   AGE
	frontend   4/4     4            0           20m


Option 1:
---------
Update deployemnt configs
	terminal --> k edit deploy frontend

frontedn deployment
------------------------------------------------
...
spec:
  minReadySeconds: 20
  progressDeadlineSeconds: 600
  replicas: 4
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: webapp
  strategy:
    type: Recreate
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: webapp
    spec:
      containers:
      - image: kodekloud/webapp-color:v3			# changed		
        imagePullPolicy: IfNotPresent
...
------------------------------------------------
save changes - escape, :wq!, enter

# result: deployment.apps/frontend edited


Verify the change
	terminal --> k describe deploy frontend | grep Image

	# result:     Image:         kodekloud/webapp-color:v3


Option 2:
---------
Show details of the deployment and copy the container name
	terminal --> k describe deployment frontend

	# container name is 'simple-webapp'

Update the deployment without updating the deployment config
	terminal --> k set image deploy frontend simple-webapp=kodekloud/webapp-color:v3

	# k 						- common kubernetes command
	# set image					- update deployment image
	# deploy					- type object
	# frontend					- target object name
	# simple-webapp=kodekloud/webapp-color:v3	- container name and imagename

Verify the change
	terminal --> k describe deploy frontend | grep Image

	# result:     Image:         kodekloud/webapp-color:v3


- click 'Check' button



13. Run the script curl-test.sh again. Notice the failures. Wait for the new application to be ready. Notice that the requests now do not hit both the versions
----------------------------------------------------------------------------------------------------------------------------------
Execute the script at /root/curl-test.sh.


Run tests
	terminal --> ./curl-test.sh

	# result:
	Hello, Application Version: v3 ; Color: red OK

	Hello, Application Version: v3 ; Color: red OK

	Hello, Application Version: v3 ; Color: red OK
	...

	# we can see that all test result changed to red at once
	# that is how recrate strategy work

- click 'Ok' button



========================================
Section 5 101. Configuration Application
========================================

Configure Applications
Configuring applications comprises of understanding the following concepts:

	- Configuring Command and Arguments on applications

	- Configuring Environment Variables

	- Configuring Secrets

We will see these next



=======================
Section 5 102. Commands
=======================

Recap Docker
------------

We start a custom ubuntu container - ubuntu-sleeper

Dockerfile
-----------------------------
FROM Ubuntu

ENTRYPOINT ["sleep"]		# This command is executed at the start of the container and argument is not required

CMD ["5"]			# This command is executed at the start of the container and can take argument
-----------------------------

Build docker image
	terminal --> docker build -t ubuntu-sleeper .

	# docker 					- common docker command
	# build						- build image
	# -t ubuntu-sleeper				- name of the image
	# .						- take current folder context (Dockerfile dir)

When we run the container, he is running for 10 seconds and stops. 
	terminal --> docker run ubuntu-sleeper 10

	# the container overwrites 'CMD ["5"]' to 'CMD ["10"]'

We can overwrite ENTRYPOINT with argument
	terminal --> docker run --entrypoint sleep2.0 ubuntu-sleeper 10



=====================================
Section 5 103. Commands and Arguments
=====================================

Dockerfile
-----------------------------
FROM Ubuntu

ENTRYPOINT ["sleep"]		# This command is executed at the start of the container and argument is not required

CMD ["5"]			# This command is executed at the start of the container and can take argument
-----------------------------

Build docker image
	terminal --> docker build -t ubuntu-sleeper .

	# docker 					- common docker command
	# build						- build image
	# -t ubuntu-sleeper				- name of the image
	# .						- take current folder context

Start the custom ubuntu image (ubuntu-sleeper) in a container
	terminal --> docker run --name ubuntu-sleeper ubuntu-sleeper

	# docker 					- common docker command
	# run 						- start a container
	# --name ubuntu-sleeper				- name of the started container
	# ubuntu-sleeper				- used image

We can start a container and pass an argument for 10s (sleep 10)
	terminal --> docker run --name ubuntu-sleeper ubuntu-sleeper 10

	# docker 					- common docker command
	# run 						- start a container
	# --name ubuntu-sleeper				- name of the started container
	# ubuntu-sleeper				- used image
	# 10						- overwrite CMD ["5"] to CMD ["10"]


We can start a pod from this image also.
----------------------------------------

pod-definition.yaml
--------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-pod
spec:
  containers:
  - name: ubuntu-sleeper
    image: ubuntu-sleeper
    command: ["sleep2.0"]	# we can overwrite ENTRYPOINT with 'command' field
    args: ["10"]		# we can overwrite CMD command with 'args' field
--------------------------------------

Create a pod
	terminal --> kubectl create -f pod-definition.yaml



=====================================================
Section 5 104. Practice Test - Commands and Arguments
=====================================================


1. How many PODs exist on the system?
-------------------------------------
In the current(default) namespace

Count pods in default namespace
	terminal --> k get pods --no-headers | wc -l

	# k					- commonkubernetes command
	# get 					- usd action
	# -A					- in all namespaces
	# --no-headers				- remove headers of the result
	# | wc -l				- count result lines

	# reult is 1

Count pods in all namespaces
	terminal --> k get pods -A --no-headers | wc -l

	# k					- commonkubernetes command
	# get 					- usd action
	# -A					- in all namespaces
	# --no-headers				- remove headers of the result
	# | wc -l				- count result lines

- choose '1' as answer



2. What is the command used to run the pod ubuntu-sleeper?
----------------------------------------------------------

Show details about pod ubuntu-sleeper
	terminal --> k describe pod ubuntu-sleeper

	# uder containers / ubuntu / Command we can see 'sleedp 4800'

- choose 'sleedp 4800' as answer



3. Create a pod with the ubuntu image to run a container to sleep for 5000 seconds. Modify the file ubuntu-sleeper-2.yaml.
--------------------------------------------------------------------------------------------------------------------------
Note: Only make the necessary changes. Do not modify the name.

Edit the file ubuntu-sleeper-2.yaml
	terminal --> vi ubuntu-sleeper-2.yaml

ubuntu-sleeper-2.yaml
----------------------------
apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-2 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command: [ "sleep" ]		# added command
    args: [ "5000" ]			# added argument
----------------------------
save changes - escape, :wq!, enter

Verify changes
	terminal --> cat ubuntu-sleeper-2.yaml

Craete the pod
	terminal --> k create -f ubuntu-sleeper-2.yaml

Show details of the pod to verify the command
	terminal --> k describe pod ubuntu-sleeper-2

- click 'Check' button


4. Create a pod using the file named ubuntu-sleeper-3.yaml. There is something wrong with it. Try to fix it!
------------------------------------------------------------------------------------------------------------
Note: Only make the necessary changes. Do not modify the name.

Try to ctreate a pod with provied file
	terminal --> k create -f ubuntu-sleeper-3.yaml

We receive this error:
Error from server (BadRequest): error when creating "ubuntu-sleeper-3.yaml": Pod in version "v1" cannot be handled as a Pod: json: cannot unmarshal number into Go struct field Container.spec.containers.command of type string

Print the file ubuntu-sleeper-3.yaml
	terminal --> cat ubuntu-sleeper-3.yaml

We can see that the command is set with wrong syntax.

Edit the file ubuntu-sleeper-3.yaml
	terminal --> vi ubuntu-sleeper-3.yaml

ubuntu-sleeper-3.yaml
------------------------------------
apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-3 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "1200"			# added dowble quotes
------------------------------------
save changes - escape, :wq!, enter

Create the pod
	terminal --> k create -f ubuntu-sleeper-3.yaml

Verify if the pod is running
	terminal --> k get pods

- click 'Check' button



5. Update pod ubuntu-sleeper-3 to sleep for 2000 seconds.
---------------------------------------------------------
Note: Only make the necessary changes. Do not modify the name of the pod. Delete and recreate the pod if necessary.

Option 1:
=========
Edit the exsisting pod
	terminal --> k edit pod ubuntu-sleeper-3

ubuntu-sleeper-3
--------------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2025-01-09T12:51:53Z"
  name: ubuntu-sleeper-3
  namespace: default
  resourceVersion: "1185"
  uid: 40bd8571-caac-413c-8a0a-c834d70c5416
spec:
  containers:
  - command:
    - sleep
    - "1200"
    image: ubuntu
    imagePullPolicy: Always
    name: ubuntu
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-qrd9v
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: controlplane
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-qrd9v
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2025-01-09T12:51:55Z"
    status: "True"
    type: PodReadyToStartContainers
  - lastProbeTime: null
    lastTransitionTime: "2025-01-09T12:51:53Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2025-01-09T12:51:55Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2025-01-09T12:51:55Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2025-01-09T12:51:53Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: containerd://ec73149f31fe0db6ef04b1235233ad823b014a5823ef2661c1f07f45217a0467
    image: docker.io/library/ubuntu:latest
    imageID: docker.io/library/ubuntu@sha256:80dd3c3b9c6cecb9f1667e9290b3bc61b78c2678c02cbdae5f0fea92cc6734ab
    lastState: {}
    name: ubuntu
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2025-01-09T12:51:54Z"
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-qrd9v
      readOnly: true
      recursiveReadOnly: Disabled
  hostIP: 192.168.211.144
  hostIPs:
  - ip: 192.168.211.144
  phase: Running
  podIP: 10.42.0.11
  podIPs:
  - ip: 10.42.0.11
  qosClass: BestEffort
  startTime: "2025-01-09T12:51:53Z"
--------------------------------------------------------
save changes - escape, :wq!, enter
# Error appear and we cannot save the changes on the existing pod
exit vim - :q!, enter

We receive this message:
error: pods "ubuntu-sleeper-3" is invalid
A copy of your changes has been stored to "/tmp/kubectl-edit-121421931.yaml"
error: Edit cancelled, no valid changes were saved.


We can recreate the pod with temporary created file /tmp/kubectl-edit-121421931.yaml
	terminal --> k replace --force -f /tmp/kubectl-edit-121421931.yaml

	# This will delete the existing pod and recreate it with the file will provided

! THE CHANGES WE MADE ARE NOT SAVED IN THE POD-DEFINITION FILE !


Option 2:
=========
List files
	terminal --> ls

Edit the pod-definition file and apply the changes
	terminal --> vi ubuntu-sleeper-3.yaml

ubuntu-sleeper-3.yaml
---------------------------------
apiVersion: v1
kind: Pod 
metadata:
  name: ubuntu-sleeper-3
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "2000"		# editted to 2000
---------------------------------
save changes - escape, :wq!, enter

Verify the changes
	terminal --> cat ubuntu-sleeper-3.yaml

Apply changes
	terminal --> k apply -f ubuntu-sleeper-3.yaml

- click ;Check' button



6. Inspect the file Dockerfile given at /root/webapp-color directory. What command is run at container startup?
---------------------------------------------------------------------------------------------------------------

List files in the directory /root/webapp-color
	terminal --> ls /root/webapp-color

Print the Dockerfile
	terminal --> cat /root/webapp-color/Dockerfile

- choose "python app.py" as answer



7. Inspect the file Dockerfile2 given at /root/webapp-color directory. What command is run at container startup?
----------------------------------------------------------------------------------------------------------------

Print the Dockerfile2
	terminal --> cat /root/webapp-color/Dockerfile2

- choose 'python app.py --color red' as answer



8. Inspect the two files under directory webapp-color-2. What command is run at container startup?
--------------------------------------------------------------------------------------------------
Assume the image was created from the Dockerfile in this directory.

List files in the dir webapp-color-2
	terminal --> ls webapp-color-2

Print the Dockerfile
	terminal --> cat webapp-color-2/Dockerfile

Dockerfile
------------------------------
FROM python:3.6-alpine

RUN pip install flask

COPY . /opt/

EXPOSE 8080

WORKDIR /opt

ENTRYPOINT ["python", "app.py"]		# start command

CMD ["--color", "red"]			# args
------------------------------

Print the webapp-color-pod.yaml
	terminal --> cat webapp-color-2/webapp-color-pod.yaml

webapp-color-pod.yaml
------------------------------
apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["--color","green"]	# overwrite the command and args
------------------------------

- choose '--color green' as answer


9. Inspect the two files under directory webapp-color-3. What command is run at container startup?
--------------------------------------------------------------------------------------------------
Assume the image was created from the Dockerfile in this directory.


List files in the directory directory webapp-color-3
	terminal --> ls directory webapp-color-3

Print the Dockerfile
	terminal --> cat webapp-color-3/Dockerfile

Dockerfile
------------------------------
FROM python:3.6-alpine

RUN pip install flask

COPY . /opt/

EXPOSE 8080

WORKDIR /opt

ENTRYPOINT ["python", "app.py"]		# command

CMD ["--color", "red"]			# args
------------------------------


Print the webapp-color-pod-2.yaml
	terminal --> cat webapp-color-3/webapp-color-pod-2.yaml

webapp-color-pod.yaml
------------------------------
apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["python", "app.py"]	# overwrites the command
    args: ["--color", "pink"]		# overwrites the arguments
------------------------------

- choose 'python app.py --color pink' as answer


10. Create a pod with the given specifications. By default it displays a blue background. Set the given command line arguments to change it to green.
-----------------------------------------------------------------

Option 1
--------
Create pod
	terminal --> k run webapp-green --image=kodedkloud/webapp-color --dry-run=client -o yaml > green-pod.yaml

Edit the created file
	terminal --> vi green-pod.yaml
	- add 'args: [ "--color" "green" ]'
	- save changes - escape, :wq!, enter

Create the pod 
	terminal --> k create -f green-pod.yaml


Option 2
--------

Show help for run command
	terminal --> kubectl run --help
	
# we have example
	# Start the nginx pod using the default command, but use custom arguments (arg1 ..argN) for thet command
	terminal --> kubectl run nginx --image=nginx -- <arg1> <arg2> .. <argN>

	# kubectl					- common kubernetes command
	# run						- sued action
	# nginx						- name of the pod
	# --image=nginx					- used image
	# -- <arg1> <arg2> .. <argN>			- additional aguments


Start the pod using run command and add arguments for color green
	terminal --> kubectl run webapp-green --image=kodekloud/webapp-color -- --color green

	# kubectl					- common kubernetes command
	# run						- sued action
	# webapp-green					- name of the pod
	# --image=kodekloud/webapp-color		- used image
	# -- --color green				- additional aguments

Verify the pod creation and commands
	terminal --> k describe pod webapp-green

- click 'Ã‡heck' button




=============================================================
Section 5 106. Configure Environment Varibles in Applications
=============================================================

How we set variables in docker
	terminal --> docker -e APP_COLOR=pink simple-webapp-color

How we set variables in pod-definition file:

pod-definition.yaml
-----------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
spec:
  containers:
  - name: simple-web-app-color
    image: simple-web-app-color
    ports:
      - containerPorts: 8080
    env:
      - name: APP_COLOR_1		# set pod environment variable name 1
	value: pink			# set pod environment variable value 1 from plane value

      - name: APP_COLOR_2		# set pod environment variable name 2
	valueFrom: 			
	    configMapKeyRef:		# set pod environment variable value 2 from ConfigMap

      - name: APP_COLOR_3		# set pod environment variable name 3
	valueFrom: 			
	    secretKeyRef: 		# set pod environment variable value 3 from Secrets
-----------------------------------



===================================================
Section 5 107. Configure ConfigMaps in Applications
===================================================


Create configMap:

Option 1: Imperative approach
=============================
We will not create a configMap definition file, but pass all values in the command directly or with config file

Option 1.1:
-----------
Create configMap with one command. Can be complicated if multiple key-value parirs are passed
	terminal --> kubectl create configmap app-config \
			--from-literal=APP_COLOR=blue \
			--from-literal=APP_MOD=prod   \

	# kubectl					- common kubernetes command
	# create					- sued action
	# configmap					- type object
	# app-config					- name of the object
	# --from-literal=APP_COLOR=blue			- pass env variable 1
	# --from-literal=APP_MOD=prod			- pass env variable 2


Option 1.2:
-----------
	terminal --> kubectl create configmap app-config --from-file=app_config.properties

	# kubectl					- common kubernetes command
	# create					- sued action
	# configmap					- type object
	# app-config					- name of the object
	# --from-file=app_config.properties		- used file with key-value data



Option 2: Declarative approach
==============================
We will create configmap definition file and specify it in the pod-definition file

config-map.yaml
-----------------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_COLOR: blue
  APP_MODE: prod
-----------------------------------

Create the configmap object
	terminal --> kubectl create -f config-map.yaml


Wehave to name correctly files of different comnponents, because we will have to use them in other files. For example:

app-config			mysql-config			redis-config
---------------------		---------------------		---------------------
APP_COLOR: blue			port: 3306			port: 6379
APP_MODE=prod			max_allowed_packets: 128M	rdb_compression: yes
---------------------		---------------------		---------------------

View ConfigMaps
	terminal --> kubectl get configmaps

Print configmap
	terminal --> kubectl describe configmaps



ConfigMap in Pods
=================

pod-definition.yaml
-----------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
spec:
  containers:
  - name: simple-web-app-color
    image: simple-web-app-color
    ports:
      - containerPorts: 8080
    envFrom	
    - configMapKeyRef:			# set pod ConfigMap items
	  name: app-config		# name of the configmap
-----------------------------------

Create the pod
	terminal --> kubectl create -f pod-definition.yaml



We can inject single environment variable:
-----------------------------------------------

pod-definition.yaml
-----------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
spec:
  containers:
  - name: simple-web-app-color
    image: simple-web-app-color
    ports:
      - containerPorts: 8080
    env:
      - name: APP_COLOR	
    - configMapKeyRef:			# set pod ConfigMap items
	  name: app-config		# name of the configmap
	  key: APP_COLOR
-----------------------------------


We can inject volumes:
----------------------

pod-definition.yaml
-----------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
spec:
  containers:
  - name: simple-web-app-color
    image: simple-web-app-color
    ports:
      - containerPorts: 8080
    volumes:
    - name: app-config-volume		# name of the volume
    - configMap:			
	name: app-config		# name of the configmap
-----------------------------------




====================================================
Section 5 108. Practice Test - Environment Variables
====================================================


1. How many PODs exist on the system?
-------------------------------------
in the current(default) namespace


List pods in current namspace
	terminal --> k get pods

- choose '1' as answer




2. What is the environment variable name set on the container in the pod?
-------------------------------------------------------------------------


Show details of the pod
	terminal --> k describe pod webapp-color

	# we can see it in section Containers / webapp-color / Environment

- choose 'APP_COLOR' as answer



3. What is the value set on the environment variable APP_COLOR on the container in the pod?
-------------------------------------------------------------------------------------------

Show details of the pod
	terminal --> k describe pod webapp-color

	# we can see it in section Containers / webapp-color / Environment

- choose 'pink' as answer



4. View the web application UI by clicking on the Webapp Color Tab above your terminal.
---------------------------------------------------------------------------------------
This is located on the right side.

Click on 'Webapp Color' button in above the top right corner of the console

We can see that the background color is pink

- cli 'Ok' button


5. Update the environment variable on the POD to display a green background.
----------------------------------------------------------------------------
Note: Delete and recreate the POD. Only make the necessary changes. Do not modify the name of the Pod.


Option 1:
---------
Edit the existing pod
	terminal --> k edit pod webapp-color


----------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2025-01-10T09:30:08Z"
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
  resourceVersion: "744"
  uid: 87101307-c64b-4ae6-925e-6fc49e068189
spec:
  containers:
  - env:
    - name: APP_COLOR
      value: green				# set this field to green
...
----------------------------------------------
save changes - esxape, :wq!, enter
Error will appear that we cannot change existing pod
exit editor - :q!, enter

We receive this message:
error: pods "webapp-color" is invalid
A copy of your changes has been stored to "/tmp/kubectl-edit-2446419648.yaml"
error: Edit cancelled, no valid changes were saved.


Use the craeted temporary file to recreate the pod
	terminal --> k replace --force -f /tmp/kubectl-edit-2446419648.yaml

	# k					- common kubernetes command (kubectl)
	# replace				- used action
	# --force				- delete existing pod and recreate it
	# -f /tmp/kubectl-edit-2446419648.yaml	- use file

Verify pod creation
	terminal --> k get pods

- click 'Check' button




6. View the changes to the web application UI by clicking on the Webapp Color Tab above your terminal.
------------------------------------------------------------------------------------------------------
If you already have it open, simply refresh the browser.

Click on 'Webapp Color' button in above the top right corner of the console

We can see that the background color is green

- cli 'Ok' button



7. How many ConfigMaps exists in the default namespace?
-------------------------------------------------------

List configmaps
	terminal --> k get configmap

	# we can see 2 lines in the result

Or we can receive directly the count
	terminal --> k get configmap --no-headers | wc -l

- choose '2' as answer


8. Identify the database host from the config map db-config.
------------------------------------------------------------

Verify the existance of db-config
	terminal --> k get configmap

Show details for db-config
	terminal --> k describe cm db-config

	# k					- common kubernetes command
	# describe				- used action
	# cm					- config map short syntax
	# db-config				- target configmap


We can see in the result
------------------------
DB_HOST:
----
SQL01.example.com
------------------------

- choose 'SQL01.example.com' as answer



9. Create a new ConfigMap for the webapp-color POD. Use the spec given below.
-----------------------------------------------------------------------------

Show create configmap help command
	terminal --> k create cm --help

We can see the third example
----------------------------
  # Create a new config map named my-config with key1=config1 and key2=config2
  kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2


Create configmap
	terminal --> k create cm webapp-config-map --from-literal=APP_COLOR=darkblue --from-literal=APP_OTHER=disregard

	# k					- common kubernetes command
	# create				- used action
	# cm					- config map short syntax
	# webapp-config-map			- name of the configmap
	# --from-literal=APP_COLOR=darkblue	- set first variable
	# --from-literal=APP_OTHER=disregard	- set second variable

Verify configmap creation
	terminal --> k get cm

- click 'Check' button


10. Update the environment variable on the POD to use only the APP_COLOR key from the newly created ConfigMap.
--------------------------------------------------------------------------------------------------------------
Note: Delete and recreate the POD. Only make the necessary changes. Do not modify the name of the Pod.


Edit the existing pod
	terminal --> k edit pod webapp-color

We cab see the right syntax here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/


----------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2025-01-10T09:40:45Z"
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
  resourceVersion: "952"
  uid: 74625adc-ace2-4c53-b9f8-691ee899472f
spec:
  containers:
  - env:
    - name: APP_COLOR
      valueFrom:
        configMapKeyRef:
          name: webapp-config-map
          key: APP_COLOR
    image: kodekloud/webapp-color
----------------------------------------------
save changes - esxape, :wq!, enter
Error will appear that we cannot change existing pod
exit editor - :q!, enter

We receive this message:
error: pods "webapp-color" is invalid
A copy of your changes has been stored to "/tmp/kubectl-edit-3138255488.yaml"
error: Edit cancelled, no valid changes were saved.


Use the craeted temporary file to recreate the pod
	terminal --> k replace --force -f /tmp/kubectl-edit-3138255488.yaml

	# k					- common kubernetes command (kubectl)
	# replace				- used action
	# --force				- delete existing pod and recreate it
	# -f /tmp/kubectl-edit-3138255488.yaml	- use file

Verify pod creation
	terminal --> k get pods

Verify the pod configmap
	terminal --> k describe pod webapp-color

- click 'Check' button


11. View the changes to the web application UI by clicking on the Webapp Color Tab above your terminal.
-------------------------------------------------------------------------------------------------------
If you already have it open, simply refresh the browser.


Click on 'Webapp Color' button in above the top right corner of the console

We can see that the background color is blue

- cli 'Ok' button





================================================
Section 5 110. Configure Secrets in Applications
================================================

We have simple Web-MySQK Application
------------------------------------------------------------------
import os
from flask import Flask

app = Flask(__name__)

@app.route("/")
def main():

    mysql.connector.connect(host='mysql', database='mysql',
			     user='root', password='paswrd')

    return render_template('hello.html', color=fetchcolor())


if __name__ == "__main__":
    app.run(host="0.0.0.0", port="8080")
------------------------------------------------------------------

We can see that the varibles are visible.

We can create a secret and inject it in to the pod configurations


Option 1: Imperative way
========================
We create a secret without using a secret definition file

Option 1.1: using one command to pass mutiple secrets. It can become complicated if we need to pass high count secrets.
-----------
Create a secret with one command
	terminal --> k crate secret generic app-secret \
			--from-literal=DB_Host=mysql \
			--from-literal=DB_User=root \
			--from-literal=DB_Password=paswrd \

Option1.2: using file. We specify key-value pairs in a file nad then pass it in the craetion command
Create a secret with file
	terminal --> k create secret generic app-secret --from-file=app_secret.rpperties




Option 2: Declarative way
=========================
We create secret definition file and use it in pod configurations

We need to encode the values of the secreats
	terminal --> echo -n 'mysql' | base64
	# result: bXlzcWw=

	terminal --> echo -n 'root' | base64
	# result: cm9vdA==

	terminal --> echo -n 'paswrd' | base64
	# result: cGFzd3Jk

We craete secret definition file

secret-data.yaml
-----------------------------------
apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data:
  DB_Host=bXlzcWw=
  DB_User=cm9vdA==
  DB_Password=cGFzd3Jk
-----------------------------------

Create the secret
-----------------
	terminal --> k create -f secret-data.yaml

List secrets
------------
	terminal --> k get secrets

Show secrets length, but hide the values
----------------------------------------
	terminal --> k describe secrets

Show encoded secrets
--------------------
	terminal --> k get secrets app-secret -o yaml

	# k					- common kubernetes command (kubectl)
	# get					- used action
	# secrets				- object type
	# app-secret				- taget object name
	# -o yaml				- set output in yaml format


Decode encoded values
---------------------
	terminal --> echo -n 'bXlzcWw=' | base64 --decode
	# result: mysql

	terminal --> echo -n 'cm9vdA==' | base64 --decode
	# result: root

	terminal --> echo -n 'cGFzd3Jk' | base64 --decode
	# result: paswrd



Secrets in Pods
===============

secret-data.yaml
-----------------------------------
apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data:
  DB_Host=bXlzcWw=
  DB_User=cm9vdA==
  DB_Password=cGFzd3Jk
-----------------------------------


pod-definition.yaml
-----------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
  labels:
    name: simple-webapp-color
spec:
  containers:
  - name: sipmle-webapp-color
    image: sipmle-webapp-color
    ports:
      - containerPort: 8080
    envFrom:
      - secretRef:
            name: app-secret
-----------------------------------

Create the pod
	terminal --> k create -f pod-definition.yaml


Secrets in Pods
===============

ENV
---------------------------
envFrom:
  - secretRef:
	name: app-config
--------------------------

SINGLE-ENV
--------------------------
env:
  - name: DB_Password
    valueFrom:
      secretKeyRef:
	name: app-secret
	key: DB_Password
--------------------------



Secrets in Pods as Volumes
--------------------------

VOLUME
--------------------------
volumes:
- nameL app-secret-volume
  secret:
    secretName: app-secret
--------------------------

All secrets are created as different files in the volume.

List secrets file in volume
	terminal --> ls /opt/app-secret-volumes
	# the result is 'DB_Host DB_Password DB_User'	- 3 files for every secret

Print secret file in volume
	terminal --> cat /opt/app-secret-volumes/DB_Password
	# result 'paswrd' - plain text


=====================
	HINTS
=====================

Risk: Secrets are not encrypted, but only encoded
	Solution: Do not push secrets objects in SCM along with the code !

Risk: Secrets are not encrypted in ETCD - Enable encryption in Rest !
	Solution: info: https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/

Risk: Anyone able to create pods/deployments in the same namespqce can access the secrets
	Solution: Configure least-privilege access to Secrets - RBAC

Solution: Consider third-party secrets store providers
	- AWS Provider, Azure Provider, GCP Provider, Vault Provider





===================================
Section 5 111. A Note about Secrets
===================================

Remember that secrets encode data in base64 format. Anyone with the base64 encoded secret can easily decode it. As such the secrets can be considered as not very safe.

The concept of safety of the Secrets is a bit confusing in Kubernetes. The kubernetes documentation page (https://kubernetes.io/docs/concepts/configuration/secret/) and a lot of blogs out there refer to secrets as a "safer option" to store sensitive data. They are safer than storing in plain text as they reduce the risk of accidentally exposing passwords and other sensitive data. In my opinion it's not the secret itself that is safe, it is the practices around it. 

Secrets are not encrypted, so it is not safer in that sense. However, some best practices around using secrets make it safer. As in best practices like:

Not checking-in secret object definition files to source code repositories.

Enabling Encryption at Rest for Secrets so they are stored encrypted in ETCD. (https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/)


Also the way kubernetes handles secrets. Such as:

A secret is only sent to a node if a pod on that node requires it.

Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage.

Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.

Read about the protections (https://kubernetes.io/docs/concepts/configuration/secret/#protections) and risks of using secrets here https://kubernetes.io/docs/concepts/configuration/secret/#risks


Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, such as using tools like Helm Secrets, HashiCorp Vault (https://www.vaultproject.io/). I hope to make a lecture on these in the future.


==================================
Section 5 112. Additional Resource
==================================

Dive deep into the world of Kubernetes security with our comprehensive guide to Secret Store CSI Driver.

https://www.youtube.com/watch?v=MTnQW9MxnRI

It is recommended to use AWS, Azure, GCP, Vault or other providers, because we don't have saved secrets on our kubernetes cluster, bu they are pulled in livetime. This means less attack surface and one platform less for audit.


Secrets providers can be isntalled with HELM (https://helm.sh/) on our kubernetes cluster and configuration looking like this

aws-secret-provider-configuration				AWS Secrets Manager
------------------------------------				----------------------------------------
apiVersion: secrets-store.csi.x-k8s.io/v1			DB-CREDS
kind: SecretProviderClass					  DB_PASSWORD: mupass
metadata:							  DB_USERNAME: myuser
  name: db-secrets						----------------------------------------
spec:
  provider: aws
  parameters:
    objects:
	- objectName: "db_creds"
	- objectTypeL "secretmanager"
------------------------------------



How to mount secrets from provider in pod
-----------------------------------------

Create a volume

pod-definition.yaml
------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
  labels:
    name: simple-webapp-color
spec:
  volumes:				# created volume section
    - name: db-creds
      csi:
	driver: secrets-store.csi.k8s.io
	readOnly: true
	volumeAttributes:
	  secretProviderClass: db-secrets	# match existing secrets provider object in the k8s cluster
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    volumeMounts:
      - name: db-creds
        mountPath: /tmp
------------------------------------


DEMOS - https://www.youtube.com/watch?v=MTnQW9MxnRI&ab_channel=KodeKloud
=====
- set secrets in AWS             	- 8:15 from video
- install CSI Driver in Kubernetes 	- 9:33 from video
	- create service account
- create secretProvider.yaml		- 19:00 from video
- create a deployment with secrets	- 20:50 from video
- change db passwords			- 26:00 from video




======================================
Section 5 113. Practice Test - Secrets
======================================


1. How many Secrets exist on the system?
----------------------------------------
In the current(default) namespace.

List secrets
	terminal --> k get secrets

- choose '1' as answer


2. How many secrets are defined in the dashboard-token secret?
--------------------------------------------------------------

Show details about the secrets
	terminal  --> k describe secret dashboard-token

We count all key-value pairs after 'Data'

- choose '3' as answer



3. What is the type of the dashboard-token secret?
--------------------------------------------------

Show details about the secrets
	terminal  --> k describe secret dashboard-token

We can see 'Type:  kubernetes.io/service-account-token'

- choose 'kubernetes.io/service-account-token' as answer



4. Which of the following is not a secret data defined in dashboard-token secret?
---------------------------------------------------------------------------------

Show details about the secrets
	terminal  --> k describe secret dashboard-token

- choose 'type' as answer



5. We are going to deploy an application with the below architecture
--------------------------------------------------------------------
We have already deployed the required pods and services. Check out the pods and services created. Check out the web application using the Webapp MySQL link above your terminal, next to the Quiz Portal Link.

List pods
---------
	terminal --> k get pods

result:

NAME         READY   STATUS    RESTARTS   AGE
mysql        1/1     Running   0          105s
webapp-pod   1/1     Running   0          105s



List services
-------------
	terminal --> k get svc

result:

NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kubernetes       ClusterIP   10.43.0.1       <none>        443/TCP          16m
sql01            ClusterIP   10.43.140.43    <none>        3306/TCP         2m8s
webapp-service   NodePort    10.43.227.191   <none>        8080:30080/TCP   2m9s


Open 'Webapp MySQL' tab above top right corner of the console
	- should show 'FAILED' text in with red background
		- error text shown for failed connection with db



6. The reason the application is failed is because we have not created the secrets yet. Create a new secret named db-secret with the data given below.
-----------------------------------------------------------------------------------
You may follow any one of the methods discussed in lecture to create the secret.

requirements:
-------------
Secret Name: db-secret
Secret 1: DB_Host=sql01
Secret 2: DB_User=root
Secret 3: DB_Password=password123

Show create secret help info
	terminal --> k create secret generic --help

The third example:
  # Create a new secret named my-secret with key1=supersecret and key2=topsecret
  kubectl create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret

Create secret with imperative approach - one line command
	terminal --> k create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123

	# k					- common kubernetes command (kubectl)
	# create				- used action
	# secret				- object type
	# generic				- secret from a local file, directory, or literal value
	# db-secret				- secret name
	# --from-literal=DB_Host=sql01		- set key-value paris for each secret

Verify secret creation
	terminal --> k get secrets

Show details for created secret	
	terminal --> k describe secret db-secret

- click 'Check' button



7. Configure webapp-pod to load environment variables from the newly created secret.
------------------------------------------------------------------------------------
Delete and recreate the pod if required.

List pods
	terminal --> k get pods

Show details of webapp-pod
	terminal --> k describe pod webapp-pod

Edit the pod config
	terminal --> k edit pod webapp-pod

We can see the syntax here - https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/#define-container-environment-variables-using-secret-data


-----------------------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2025-01-10T14:40:56Z"
  labels:
    name: webapp-pod
  name: webapp-pod
  namespace: default
  resourceVersion: "949"
  uid: 5a2ea32c-d465-437b-9a86-df644a2e492a
spec:
  containers:
  - image: kodekloud/simple-webapp-mysql
    imagePullPolicy: Always
    name: webapp
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    envFrom:
    - secretRef:
        name: db-secret
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-s2zlq
...
-----------------------------------------------------------------
save changes - esxape, :wq!, enter
Error will appear that we cannot change existing pod
exit editor - :q!, enter

We receive this message:
error: pods "webapp-color" is invalid
A copy of your changes has been stored to "/tmp/kubectl-edit-1832334219.yaml"
error: Edit cancelled, no valid changes were saved.

Use the craeted temporary file to recreate the pod
	terminal --> k replace --force -f /tmp/kubectl-edit-1832334219.yaml

	# k					- common kubernetes command (kubectl)
	# replace				- used action
	# --force				- delete existing pod and recreate it
	# -f /tmp/kubectl-edit-1832334219.yaml	- use file

Verify pod creation
	terminal --> k get pods

Verify the pod configmap
	terminal --> k describe pod webapp-color

- click Ã‡heck'button


8. View the web application to verify it can successfully connect to the database
---------------------------------------------------------------------------------

Open 'Webapp MySQL' tab above top right corner of the console
	- should show 'SUCCESS' text in with green background

- click 'Ok' button



=============================================
Section 5 115. Encrypting Secret Data at Rest
=============================================

Documentation - https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/

HOW SECRETS WORKS IN KUBERNETES - only encoded
-----------------------------------------------------------
Create a secret
	terminal --> k create secret generic my-secret --from-literal=key1=supersecret

Print secret my-secret as yaml file
	terminal --> k describe secret my-secret

Print my-secret detailed information
	terminal --> k get secret my-secret -o yaml

Decode the secret and see the original
	terminal --> echo "c3VwZXJzZWNyZXQ=" | base64 --decode
	# result: supersecret
-----------------------------------------------------------



We will focus on how secrets are stored in ETCD
===============================================

Check if etcdctl is available, if not, intall it
	terminal --> apt-get install etcd-client

We can print the ETCD files
	terminal --> ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key get /registry/secrets/default/my-secret | hexdump -C

	# In default format the secrets are saved in plain text !


Step 1 - Check if encryption is enabled
---------------------------------------

Option 1: 
List running processes
	terminal --> ps -aux

We have to find out if the encription of secrets is enabled. We find ikube-api server and check if encription is running option
	terminal --> ps aux | grep kube-api | grep encryption-provider
	# if no result is reterned on the console, encryption is disabled

Option 2:
List system definition files
	terminal --> ls /etc/kubernetes/manifests/

Print apiserver definition file
	terminal --> cat /etc/kubernetes/manifests/kube-apiserver.yaml

Check if '--encryption-provider' exists in the list of options. If we cannot find it, then is not enabled.


Step 2 - Create a configuration file and pass it to '--encryption-provider' option in the process
-------------------------------------------------------------------------------------------------

Understanding configuration file
	- in resources we have what we want to be encrypted
	- in providers we have what encryption provider we will use (first one is used by default)

encryption configuration example
----------------------------------------------------------------------------
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
      - secrets
      - configmaps
      - pandas.awesome.bears.example 						# a custom resource API
    providers:
      # This configuration does not provide data confidentiality. The first
      # configured provider is specifying the "identity" mechanism, which
      # stores resources as plain text.
      #
      # The first encryption algorithm is enabled by default !!!
      #
      # - identity: {} 		# enabled by default, plain text, NO encryption, NOT RECOMMENDED
      - aesgcm:							# encryption algorithm 1
          keys:
            - name: key1					# key used by the algorithm
              secret: c2VjcmV0IGlzIHNlY3VyZQ==			# will be used to encrypt with the algorithm
            - name: key2
              secret: dGhpcyBpcyBwYXNzd29yZA==
      - aescbc:							# encryption algorithm 2
          keys:
            - name: key1					# key used by the algorithm
              secret: c2VjcmV0IGlzIHNlY3VyZQ==			# will be used to encrypt with the algorithm
            - name: key2
              secret: dGhpcyBpcyBwYXNzd29yZA==
      - secretbox:							# encryption algorithm 2
          keys:
            - name: key1						# key used by the algorithm
              secret: YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=	# will be used to encrypt with the algorithm
  - resources:
      - events
    providers:
      - identity: {} 		# do not encrypt Events even though *.* is specified below
  - resources:
      - '*.apps' 		# wildcard match requires Kubernetes 1.27 or later
    providers:
      - aescbc:
          keys:
          - name: key2
            secret: c2VjcmV0IGlzIHNlY3VyZSwgb3IgaXMgaXQ/Cg==
  - resources:
      - '*.*' 			# wildcard match requires Kubernetes 1.27 or later
    providers:
      - aescbc:
          keys:
          - name: key3
            secret: c2VjcmV0IGlzIHNlY3VyZSwgSSB0aGluaw==
----------------------------------------------------------------------------

More info about providers - https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#providers


Step 2.1 Create a encryption configuration file - enc.yaml 
----------------------------------------------------------
Generate random encoded password
	terminal --> head -c 32 /dev/random | base64
	# result: RBm3yS8qOCH/GrHAx8y+pnHAkKEhjASAvsX0OlImFyQ=

Create simplified encryption configuration file
	terminal --> vi enc.yaml

enc.yaml
----------------------------------------------------
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: RBm3yS8qOCH/GrHAx8y+pnHAkKEhjASAvsX0OlImFyQ=
      # - identity: {} 							# REMOVE THIS LINE, NO ENCRYPTION
----------------------------------------------------


Step 2.2 - Save the new encryption config file to /etc/kubernetes/enc/enc.yaml on the control-plane node.
---------------------------------------------------------------------------------------------------------

Create the direcotiry
	terminal --> mkdir /etc/kubernetes/enc

Move created 'enc.yaml' file there
	terminal --> mv enc.yaml /etc/kubernetes/enc

Verify file location
	terminal --> ls /etc/kubernetes/enc


Step 2.3 - Edit the manifest for the kube-apiserver static pod: /etc/kubernetes/manifests/kube-apiserver.yaml
-------------------------------------------------------------------------------------------------------------

Edit api server definition file
	terminal --> vi /etc/kubernetes/manifests/kube-apiserver.yaml

kube-apiserver.yaml
---------------------------------------------------------------
#
# This is a fragment of a manifest for a static Pod.
# Check whether this is correct for your cluster and for your API server.
#
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.20.30.40:443
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    ...
    - --encryption-provider-config=/etc/kubernetes/enc/enc.yaml  		# add this line
    volumeMounts:
    ...
    - name: enc                           # add this line
      mountPath: /etc/kubernetes/enc      # add this line
      readOnly: true                      # add this line
    ...
  volumes:
  ...
  - name: enc                             # add this line
    hostPath:                             # add this line
      path: /etc/kubernetes/enc           # add this line
      type: DirectoryOrCreate             # add this line
---------------------------------------------------------------
save changes - escape, :wq!, enter

Wait until kube-apiserver reinitialize. We can see api server status
	terminal --> crictl pods

Verify if the encryption enabled
	terminal --> ps aux | grep kube-api | grep encryption-provider
	# there should be result returned on the console



Step 3 - Test if the encryption is working preperly
---------------------------------------------------

Create another secret object
	terminal --> k create secret generic my-secret-2 --from-literal=key2=topsecret

	# k					- common kubernetes command (kubectl)
	# create				- used action
	# secret				- object type
	# generic				- secret from a local file, directory, or literal value
	# db-secret				- secret name
	# --from-literal=key2=topsecret		- set key-value paris for each secret


List secrets to verify creation of my-secret-2 with enabled encritpion
	terminal --> k get secrets

Print the ETCD file again to check if secrets are encrypted
	terminal --> ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key get /registry/secrets/default/my-secret-2 | hexdump -C

	# the result must be encrypted


The my-secret will not be automatically encrypted. Encryption is working only on newly created objects

To encrypt already existing secrets we have to update all obejcts
	terminal --> kubectl get secrets --allnamespaces -o json | kubectl replace -f -

	# kubectl						- common kubernetes command (kubectl)
	# get							- used action
	# secret						- object type
	# --allnamespaces					- from all namespaces
	# -o json						- in json format
	# | kubectl replace -f -				- save files with same data (refresh to encrypt)

Print first created secret my-secret
	terminal --> ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key get /registry/secrets/default/my-secret | hexdump -C

	# the result must be encrypted also



===================================
Section 5 117. Multi-Container Pods
===================================

 We focus on microservice architecture. For example we want each Web server instance to be monitored by one instance of Logging monitoring instance. They should be in separate contaniers and managed by its kind. That is wy we have multicontainer pods. They share same lifecycle, network, storage and namespace. That mean that they can reffer to each other as 'localhost' and they have access to the same volume storage.


Example for multicontainer pod

pod-definition.yaml
-----------------------------------------------------
apiVErsion: v1
kind: Pod
metadata:
  name: simple-webapp
  labels:
    name: simple-webapp
spec:
  contaniners:
  - name: simple-webapp			# container 1
    image: simple-webapp
    ports:
      - containerPort: 8080
  - name: log-agent			# contanier 2
    image: log-agent
-----------------------------------------------------



===================================================
Section 5 118. Practice Test - Multi-Container Pods
===================================================

1. Identify the number of containers created in the red pod.
------------------------------------------------------------

List pods
	terminal --> k get pods

Show info for 'red' pod
	terminal --> k describe pod red

	# under contaniers we have 3 instances - apple, wine and scarlet

- choose '3' as answer



2. Identify the name of the containers running in the blue pod.
---------------------------------------------------------------

Show info for 'blue' pod
	terminal --> k describe pod blue

	# under contaniers we have 2 instances - teal and navy

- choose 'teal & navy' as answer




3. Create a multi-container pod with 2 containers.
--------------------------------------------------
Use the spec given below:

If the pod goes into the crashloopbackoff then add the command sleep 1000 in the lemon container.

Name: yellow
Container 1 Name: lemon
Container 1 Image: busybox
Container 2 Name: gold
Container 2 Image: redis

Create the pod template configuration file & save it in yellow.yaml
	terminal --> k run yellow --image=busybox --dry-run=client -o yaml > yellow.yaml

	# k 					- common kubernetes command
	# run					- start pod
	# yellow				- name of the pod
	# --image=busybox			- image of the pod
	# --dry-run=client			- do not create the object (test run the pod)
	# -o yaml				- set yaml output format
	# > yellow.yaml				- save output in yallow.yaml file


Edit the file
	terminal --> vi yellow.yaml

yellow.yaml
---------------------------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: yellow
  name: yellow
spec:
  containers:
  - image: busybox
    name: lemon				# change to lemon
    resources: {}
    command: [ "sleep", "1000" ]	# add command sleep 1000
  - name: gold				# add second container with name gold
    image: redis			# set second container image as redis
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
---------------------------------
save changes - escape, :wq!, enter

Verify chnages
	terminal --> cat yellow.yaml

Create the pod 
	terminal --> k create -f yellow.yaml

Verify pod creation
	terminal --> k describe pod yellow

- click Ã‡heck'button



4. We have deployed an application logging stack in the elastic-stack namespace. Inspect it.
--------------------------------------------------------------------------------------------
Before proceeding with the next set of questions, please wait for all the pods in the elastic-stack namespace to be ready. This can take a few minutes.

Show pods in 'elastic-stack' namespace
	terminal --> k get pods -n elastic-stack

	# we have 3 pods - app, elastic-search and kibana

- click 'Ok' button



5. Once the pod is in a ready state, inspect the Kibana UI using the link above your terminal. There shouldn't be any logs for now.
---------------------------------------------------------------------------------------------------------
We will configure a sidecar container for the application to send logs to Elastic Search.

NOTE: It can take a couple of minutes for the Kibana UI to be ready after the Kibana pod is ready.

You can inspect the Kibana logs by running:

kubectl -n elastic-stack logs kibana

Click on 'Kibana' tab above top right corner of the console and see the dashboard

We can inspect 'Kibana' logs
	terminal --> kubectl -n elastic-stack logs kibana

Click on 'Elastic Search' tab above top right corner of the console and see current logs

- click 'Ok' button


6. Inspect the app pod and identify the number of containers in it.
-------------------------------------------------------------------
It is deployed in the elastic-stack namespace.

Show details about 'app' pod in elastic-stack namespace
	terminal --> k describe pod app -n elastic-stack

	# we can see only one container

- choose '1' as answer


7. The application outputs logs to the file /log/app.log. View the logs and try to identify the user having issues with Login.
------------------------------------------------------------------------------------------------------------------------------
Inspect the log file inside the pod.

See the logs entering the pod and print them
	terminal --> k -n elastic-stack exec -it app -- cat /log/app/log

	# k						- common kubernetes command
	# -n elastic-stack				- elastic-stack namespace
	# exec						- execute command
	# -it						- interactive mode (enter the pod)
	# -- cat /log/app/log				- print the file in the pod location

OR 

Show logs outside the pod
	terminal --> k logs app -n elastic-stack

	# we can see 'WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.'

- choose 'USER5' as answer



8. Edit the pod in the elastic-stack namespace to add a sidecar container to send logs to Elastic Search. Mount the log volume to the sidecar container.
----------------------------------------------------------------------------------------------
Only add a new container. Do not modify anything else. Use the spec provided below.

Note: State persistence concepts are discussed in detail later in this course. For now please make use of the below documentation link for updating the concerning pod.

https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/

Name: app
Container Name: sidecar
Container Image: kodekloud/filebeat-configured
Volume Mount: log-volume
Mount Path: /var/log/event-simulator/
Existing Container Name: app
Existing Container Image: kodekloud/event-simulator


Edit the pod
	terminal --> k edit pod app -n elastic-stack


---------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2025-01-11T09:03:03Z"
  labels:
    name: app
  name: app
  namespace: elastic-stack
  resourceVersion: "742"
  uid: 34119bbf-a56f-45cf-b8b7-4fdd59c52195
spec:
  containers:
  - image: kodekloud/filebeat-configured		# added from this line
    name: sidecar
    volumeMounts:
    - mountPath: /var/log/event-simulator/
      name: log-volume					# to this line
  - image: kodekloud/event-simulator
    imagePullPolicy: Always
    name: app
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /log
      name: log-volume
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-xxtrb
      readOnly: true
  dnsPolicy: ClusterFirst
.....
---------------------------------------------
save changes - esxape, :wq!, enter
Error will appear that we cannot change existing pod
exit editor - :q!, enter

We receive this message:
error: pods "app" is invalid
A copy of your changes has been stored to "/tmp/kubectl-edit-3129749520.yaml"
error: Edit cancelled, no valid changes were saved.

Use the craeted temporary file to recreate the pod
	terminal --> k replace --force -f /tmp/kubectl-edit-3129749520.yaml

	# k					- common kubernetes command (kubectl)
	# replace				- used action
	# --force				- delete existing pod and recreate it
	# -f /tmp/kubectl-edit-3129749520.yaml	- use file

Verify pod creation
	terminal --> k get pods -n elastic-stack

Verify the pod changes. Show details:
	terminal --> k describe pod app -n elastic-stack

- click 'Ã‡heck' button


9. Inspect the Kibana UI. You should now see logs appearing in the Discover section.
------------------------------------------------------------------------------------
You might have to wait for a couple of minutes for the logs to populate. You might have to create an index pattern to list the logs. If not sure check this video: https://bit.ly/2EXYdHf

Click on 'Kibana' tab above top right corner of the console and go to 'Discover' section to see the logs

- click 'Ok' button


===================================================
Section 5 119. Multi-Container Pods Design Patterns
===================================================

Multi-container Pods Design Patterns
There are 3 common patterns, when it comes to designing multi-container PODs. The first and what we just saw with the logging service example is known as a side car pattern. The others are the adapter and the ambassador pattern.

But these fall under the CKAD curriculum and are not required for the CKA exam. So we will be discuss these in more detail in the CKAD course.



=============================
Section 5 120. InitContainers
=============================

InitContainers
In a multi-container pod, each container is expected to run a process that stays alive as long as the POD's lifecycle. For example in the multi-container pod that we talked about earlier that has a web application and logging agent, both the containers are expected to stay alive at all times. The process running in the log agent container is expected to stay alive as long as the web application is running. If any of them fails, the POD restarts.

But at times you may want to run a process that runs to completion in a container. For example a process that pulls a code or binary from a repository that will be used by the main web application. That is a task that will be run only  one time when the pod is first created. Or a process that waits  for an external service or database to be up before the actual application starts. That's where initContainers comes in.

An initContainer is configured in a pod like all other containers, except that it is specified inside a initContainers section,  like this:

----------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox
    command: ['sh', '-c', 'git clone <some-repository-that-will-be-used-by-application> ; done;']
----------------------------------------------------


When a POD is first created the initContainer is run, and the process in the initContainer must run to a completion before the real container hosting the application starts. 

You can configure multiple such initContainers as well, like how we did for multi-containers pod. In that case each init container is run one at a time in sequential order.
      !---------------------------------!

If any of the initContainers fail to complete, Kubernetes restarts the Pod repeatedly until the Init Container succeeds.

----------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']
----------------------------------------------------


Read more about initContainers here. And try out the upcoming practice test.
	- https://kubernetes.io/docs/concepts/workloads/pods/init-containers/



=============================================
Section 5 122. Practice Test - InitContainers
=============================================

1. Identify the pod that has an initContainer configured.
---------------------------------------------------------

List pods 
	terminal --> k get pods

	# we can see 'green' pod have 2 containers in it
	# result: 'green   2/2     Running   0          33s'
	# so we expect this container to be with InitContainer in it

Show details of all pods 
	terminal --> k describe pod

	# we can see that actually pod 'blue' have InitContainer in it.

- choose 'blue' as answer


2. What is the image used by the initContainer on the blue pod?
---------------------------------------------------------------

Show details about 'blue' pod
	terminal --> k describe pod blue

	We can see in the section InitContainer / Image: busybox

- choose 'busybox' as answer


3. What is the state of the initContainer on pod blue?
------------------------------------------------------

Show details about 'blue' pod
	terminal --> k describe pod blue

	# we can see in section InitContaimer / State: Terminated

- choose 'Terminated' as answer


4. Why is the initContainer terminated? What is the reason?
-----------------------------------------------------------

Show details about 'blue' pod
	terminal --> k describe pod blue

	# we can see in section InitContaimer / Reason: Completed

- choose 'The process completed successfully' as answer


5. We just created a new app named purple. How many initContainers does it have?
--------------------------------------------------------------------------------

List pods
	terminal --> k get pods

Show details of pod purple
	terminal --> k describe pod purple

	# we can see 2 entities under InitContaiers

- choose '2' as answer



6. What is the state of the POD?
--------------------------------

Show details of pod purple
	terminal --> k describe pod purple

	# we can see Status Pending

- choose 'Pending' as answer


7. How long after the creation of the POD will the application come up and be available to users?
-------------------------------------------------------------------------------------------------

Show details of pod purple
	terminal --> k describe pod purple

	# we can see that the commands of hhe two InitContainers are sleep 600 and sleep 1200, combined sleep 1800
	# 1800 / 60 = 30 minutes

- choose '30 minutes' as answer


8. Update the pod red to use an initContainer that uses the busybox image and sleeps for 20 seconds
---------------------------------------------------------------------------------------------------
Delete and re-create the pod if necessary. But make sure no other configurations change.

Edit the 'red' pod
	terminal --> k edit pod red

----------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2025-01-11T10:03:33Z"
  name: red
  namespace: default
  resourceVersion: "836"
  uid: 0ce3d3f6-eb53-4624-976a-69b3119084da
spec:
  initContainers:				# added from this line
  - image: busybox
    name: busybox
    command: [ "sleep", "20" ]			# to this line
  containers:
  - command:
    - sh
    - -c
    - echo The app is running! && sleep 3600
    image: busybox:1.28
    imagePullPolicy: IfNotPresent
----------------------------------------------------
save changes - esxape, :wq!, enter
Error will appear that we cannot change existing pod
exit editor - :q!, enter

We receive this message:
error: pods "red" is invalid
A copy of your changes has been stored to "/tmp/kubectl-edit-2777576492.yaml"
error: Edit cancelled, no valid changes were saved.

Use the craeted temporary file to recreate the pod
	terminal --> k replace --force -f /tmp/kubectl-edit-2777576492.yaml

	# k					- common kubernetes command (kubectl)
	# replace				- used action
	# --force				- delete existing pod and recreate it
	# -f /tmp/kubectl-edit-2777576492.yaml	- use file

Verify pod creation
	terminal --> k get pods

Verify the pod configuration
	terminal --> k describe pod red

- click 'Check' button



9. A new application orange is deployed. There is something wrong with it. Identify and fix the issue.
------------------------------------------------------------------------------------------------------
Once fixed, wait for the application to run before checking solution.

List pods
	terminal --> k get pods

	# we can see pod 'orange' with status 'Init:CrashLoopBackOff'

Show details of pod 'orange'
	terminal --> k describe pod orange

	# we can see that in section initContainers / init-myservice / Dommand / we have 'sleeeep 2'
	# syntax of the command is wrong

We can show log specifically fro init container
	terminal --> k logs orange -c init-myservice

	# result: sh: sleeeep: not found
	# wrong command syntax

Edit the pod and fix the command
	terminal --> k edit pod orange

----------------------------------------------------
...
  initContainers:
  - command:
    - sh
    - -c
    - sleep 2;		# fix the command 
    image: busybox
...
----------------------------------------------------
save changes - esxape, :wq!, enter
Error will appear that we cannot change existing pod
exit editor - :q!, enter

We receive this message:
error: pods "orange" is invalid
A copy of your changes has been stored to "/tmp/kubectl-edit-116032954.yaml"
error: Edit cancelled, no valid changes were saved.

Use the craeted temporary file to recreate the pod
	terminal --> k replace --force -f /tmp/kubectl-edit-116032954.yaml

	# k					- common kubernetes command (kubectl)
	# replace				- used action
	# --force				- delete existing pod and recreate it
	# -f /tmp/kubectl-edit-116032954.yaml	- use file

Verify pod creation
	terminal --> k get pods

Verify the pod configuration
	terminal --> k describe pod orange

- click 'Check' button




========================================
Section 5 123. Lelf Healing Applications
========================================

Kubernetes supports self-healing applications through ReplicaSets and Replication Controllers. The replication controller helps in ensuring that a POD is re-created automatically when the application within the POD crashes. It helps in ensuring enough replicas of the application are running at all times.

Kubernetes provides additional support to check the health of applications running within PODs and take necessary actions through Liveness and Readiness Probes. However these are not required for the CKA exam and as such they are not covered here. These are topics for the Certified Kubernetes Application Developers (CKAD) exam and are covered in the CKAD course.




=========================================
Section 5 124. Introduction to Autscaling
=========================================

We will look over 
	1. Horizontal Pod Autoscaling (HPA) 	- Adding more instatnces of the application server
	2. Vertical Pod Autoscaling (VPA) 	- Adding more resources (CPU & Memory) to the application server

There is an entire course for autoscaling called Kubernetes Autoscaling. Check it.


In Kubernetes tehre is a 2 types of scaling strategies
------------------------------------------------------
	1. Scaling Cluster Infra 		- Adding more clusters 
	2. Scaling Workloads 			- Adding more PODs (containers) to the cluster


Strategy 1 - Scaling Cluster Infra
	- Horizontal Scaling 			- adding more nodes to the cluster
	- Vertical Scaling			- adding more resources to the nodes


Strategy 2 - Scaling Workloads
	- Horizontal Scaling 			- adding more PODs
	- Vertical Scaling 			- increase the resources used by the PODs



There are 2 ways to scale - Manual and Automated
================================================

MANUAL APPRACH
--------------

Manual Horizontally Scaling Cluster 
	- manually add Nodes and the use 'kubectl join ...' command to add new nodes to the cluster

Manual Vertically Scaling Cluster (not used)
	- not common used apprach, because we have to stop the server, add resources and bring it up again - not preferable
	- Yet we can add new server with more resources and stop the existing one which is horizontal scaling


Manual Horizontally Scaling Workloads
	- using 'kubectl scale ...' command to scale up or down the number of PODs

Manual Vertically Scaling Workloads
	- using 'kubectl edit ...' command to modify the deployment or StatefulSet or ReplicaSet and change the resource limits and requests associated with the PODs.


AUTOMATED APPROACH
------------------

Automated Horizontally Scaling Cluster 
	- Cluster Autoscaler			# This will be discussed

Automated Vertically Scaling Cluster
	- not used

Automated Horizontally Scaling Workloads
	- Horizontal Pod Autoscaler (HPA)	# This will be discussed

Automated Vertically Scaling Workloads
	- Vertical Pod Autoscaler (VPA)		# This will be discussed



==============================================
Section 5 125. Horizontal Pod Autoscaler (HPA)
==============================================

We have deployment described below:

deployment-definition.yaml
--------------------------------
apiVErsion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 1
  selector:
    metchLabels:
      app: my-app
  template:
    metadata:
      labels:
	app: my-app
    sepc:
      containers:
      - name: my-app
	image: nginx
	resources:
	  requests:
	    cpu: "250m"
	  limits:
	    cpu: "500m"
--------------------------------



Manually horizontally scaling workloads
---------------------------------------

If we want to monitor the resource usage of the pods we need Metrics server to be installed
Then we can monitor the pods
	terminal --> kubectl top pod my-app-pod

	# result:
	NAME		CPI(cores)		MEMORY(bytes)
	my-app-pod	450m			350Mi

When we need more loadforce we scale the deployemnt replicas
	terminal --> kubectl scale deployment my-app --replicas=3

Desadvatages of manual approach:
	- We have to monitor manually the pod resource consumption and scale manually. 
	- In case of momentum load spikes we cannot react fast enough.


To solve this we use 


Horizontal Pod Autoscalesr (HPA)
================================

Functionality of the Horizontal Pod Autoscalesr (HPA)
	- Monitors the resource usage (Observes metrics)
	- Increse or decrease number of pods in a Deployment, StatefulSet or ReplicaSet based on CPU, Memory or custom metrics
	- Balances Thresholds 
	- Tracks multiple metrics


Imperative approach to manage Horizontal Pod Autoscaler (HPA)
-------------------------------------------------------------

Create Horizontal Pod Autoscaler (HPA)
	terminal --> kubectl autoscale deployemnt my-app --cpu-percent=50 --min=1 --max=10
			
	# kubectl					- common kubernetes command
	# autoscale					- action
	# deployment					- target object type
	# my-app					- target object name
	# --cpu-percent=50				- target cpu treshold of 50%
	# --min=1 --max=10				- minimum pods / maximum pods limits

Horizontal Pod Autoscaler (HPA) works as process reading the resources of the deployment, pulling data from the Metric Server and scale up or down the number of pods


Show status of the Horizontal Pod Autoscaler (HPA)
	terminal --> kubectl get hpa

	# result:
	NAME		REFERENCE		TARGETS		MINPODS		MAXPODS		REPLICAS	AGE
	my-app		Deployment/my-app	30%/50%		1		10		1		1m


Delete Horizontal Pod Autoscaler (HPA)
	terminal --> kubectl delete hpa my-app


Declarative approach to manage Horizontal Pod Autoscaler (HPA)
--------------------------------------------------------------

Create Horizontal Pod Autoscaler (HPA) definition file

hpa-definition.yaml
-------------------------------
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
spec:					# target object section
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 1			# min pod limit
  maxReplicas: 10			# max pod limit
  metrics:				# metrics section defines the resources we are scaling on
  - type: Resource
    resource:
      name: cpu
      target:
	type: Utilization
	averageUtilization: 50
-------------------------------

Horizontal Pod Autoscaler (HPA) comes built-in with Kubernetes since version 1.23. It relies on Metric Server that is required.


Horizontal Pod Autoscaler (HPA) can refer to internal Metric Server or Custom Metric Adapter (custom server) to pull data for target objects (Deployments, StatefulSet or ReplicaSet)

Horizontal Pod Autoscaler (HPA) can refer to external Metric Adapters (external servers) like 'DATADOG' or 'dynatrace'.
	- details can be checked in the separate course 'Kubernetes Autoscaling'


=================================================
Section 5 126. Practice Test - Manual Scaling HPA
=================================================


1. Info
-------

Click 'Nect' button



2. Create a Deployment
----------------------
Using the /root/deployment.yml manifest file provided , create a Kubernetes deployment for the Flask application.
Discovery

Use kubectl get deployments to observe the deployment status.
Use kubectl get pods to see the running pods.


Print the deployment definition file
	terminal --> cat /root/deployment.yml

deployment.yml
----------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flask-web-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: flask-app
  template:
    metadata:
      labels:
        app: flask-app
    spec:
      containers:
      - name: flask
        image: rakshithraka/flask-web-app
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: flask-web-app-service
spec:
  type: ClusterIP
  selector:
    app: flask-app
  ports:
   - port: 80
     targetPort: 80  
----------------------------


Create the deploymnet
	terminal --> k create -f /root/deployment.yml

	# result:
	deployment.apps/flask-web-app created
	service/flask-web-app-service created

List deployments
	terminal --> k get deploy

	# result:
	NAME            READY   UP-TO-DATE   AVAILABLE   AGE
	flask-web-app   2/2     2            2           55s	

List pods
	terminal --> k get pods

	# result:
	NAME                             READY   STATUS    RESTARTS   AGE
	flask-web-app-584dd886c8-f4hsm   1/1     Running   0          75s
	flask-web-app-584dd886c8-kx8w8   1/1     Running   0          75s


- click 'Check' button



3. What is the primary purpose of the kubectl scale command?
------------------------------------------------------------

- choose 'To adjust the number of replicas in a deployment or replicaset' as answer




4. Can the kubectl scale command be used to scale down a statefulset in Kubernetes?
-----------------------------------------------------------------------------------

- choose 'Yes, it can scale both deployments and statefulsets' as answer




5. Manual Scale
---------------
Manually scale the deployment named flask-web-app to have 3 replicas.

Observation

Observe the changes with kubectl get deployments and kubectl get pods.


To view the application, click on the Ingress button at the top of the terminal, or click on Skooner to access the monitoring tool and view the resources in the Kubernetes cluster.

Token for the Skooner can be found in /root/skooner-sa-token.txt

Show the token
	terminal --> cat /root/skooner-sa-token.txt

	# result:
-----------------------------
eyJhbGciOiJSUzI1NiIsImtpZCI6Ing4TlFGQ3RjOGZrTjh4Z2IzaWVmOVBkeE5NSGRwaVdtRDZQM2g5UHd1UVUifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzQwOTA5NTg3LCJpYXQiOjE3NDA5MDU5ODcsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwianRpIjoiMzI4MGNkNGQtYWI2ZC00ZjczLWI1MDUtOGIxNDg2NDA4NzYzIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJza29vbmVyLXNhIiwidWlkIjoiOTlkNzVkYWMtYTFlOS00ZTc4LThiNGYtYTkxNzQ4YTJkZjA5In19LCJuYmYiOjE3NDA5MDU5ODcsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpza29vbmVyLXNhIn0.gTFHM6S_mw0OS_wTR7oIpUJ5r3kRNDuDXA89UOOYmyt4MSAQ-aIM5au78efhnLcS-yux9DebxKS8KKOQ3PCrmzCfH4jKyBGa8qzONA6DdKl5cEqveHIYejbxh2HE3cxcJdUC3PBPKsbkfqtxRdNpvYjFedTaDjXya1wXqA9IGR-HbWhxSnvCDvAG85nUX3bZuwATSu57D4jdxRvHuCksOU3K98RnB9m-jf6s5iopvnYQ8hg_dw_61vWSLBg0Qkfkpy85xC672qVPRj90VOaF6FTdDb7OqhJJAyn8zw0aUMrdH8m1Hp3Ce99CxkTLlbV6j4Mbr7bSI5CE135GjkM7AA
-----------------------------

Test the app with the token


List deployments
	terminal --> k get deploy

	# result:
	NAME            READY   UP-TO-DATE   AVAILABLE   AGE
	flask-web-app   2/2     2            2           7m23s

List pods
	terminal --> k get pods

	# result:
	NAME                             READY   STATUS    RESTARTS   AGE
	flask-web-app-584dd886c8-f4hsm   1/1     Running   0          7m38s
	flask-web-app-584dd886c8-kx8w8   1/1     Running   0          7m38s

Scale the deployment
	terminal --> kubectl scale deployment flask-web-app --replicas=3

	# result: deployment.apps/flask-web-app scaled


List deployments
	terminal --> k get deploy

	# result:
	NAME            READY   UP-TO-DATE   AVAILABLE   AGE
	flask-web-app   3/3     3            3           8m8s		# scaled to 3 pods

List pods
	terminal --> k get pods

	# result:
	NAME                             READY   STATUS    RESTARTS   AGE
	flask-web-app-584dd886c8-f4hsm   1/1     Running   0          8m25s
	flask-web-app-584dd886c8-kx8w8   1/1     Running   0          8m25s
	flask-web-app-584dd886c8-twhl2   1/1     Running   0          33s

- click 'Check' button



6. If you scale a deployment using kubectl scale to a higher number of replicas, but the cluster has insufficient resources to accommodate all new replicas, what will happen?
------------------------------------------------------------------------------------------------------------------------------

- choose 'Some replicas will be created up to the limit of available resources and the deployment will remain in a pending state for the reminding replicas' as answer



==================================
Section 5 127. Practice Test - HPA
==================================

1. INFO
-------
Objectives

HPA with Imperative commands
Requirements for HPA to work
What happens when resources.limit is not mentioned

- click 'Next' button



2. Create a Deployment
----------------------
Using the /root/deployment.yml manifest file provided , create a Kubernetes deployment for the nginx application.

Click on Skooner to access the monitoring tool and view the resources in the Kubernetes cluster.

Token for the Skooner can be found in /root/skooner-sa-token.txt

Print the token
	terminal --> cat /root/skooner-sa-token.txt

# result:
-------------------------
eyJhbGciOiJSUzI1NiIsImtpZCI6ImtQZ3J3RVhlbkxpd0U0ZThCZDM1Q2hOQlJUUlFKdFdjVjJnNVdmbjVzdEEifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzQwOTEwMzk3LCJpYXQiOjE3NDA5MDY3OTcsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwianRpIjoiZmU3ZjlhNTktZjgxZS00Y2ZlLWJkMmUtMjJlNGFiY2MyMjljIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJza29vbmVyLXNhIiwidWlkIjoiMTA3OGFiMTQtNmZjMi00NWYxLWE3YWMtNmQxODY2YTNmYzU3In19LCJuYmYiOjE3NDA5MDY3OTcsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpza29vbmVyLXNhIn0.3Uf7WOUdqegrHFJ0TTWCda-UItwYISbksz2c1O70nrfmz-a-Xd8OmUERxS0f93iCLMjrSqW99AxzBL54Tg4EJRPFTe2A0tJ-3Cmkd3_7cnWo0wKKCGBbMmdz8wagl87BVdBzj9XXEgvRr7t1tTrjAewQXL0dz9KXzxZdC333Fsz23JQW8WmRp6zwl3WI-9hMotJIZW7gMnDj5ksU_IflSpMfAcXMoRMTnhcBfj6SSWk6bvLJrX9prvK2LnLWU9plWsM_eOB-HXM44Iumm4KnKW7ZdETW8tGsb23ADIH6EweoCaIiTMUWXyFuZCcOPJGQWzSZcpaLnat8auJPIVnUUg
-------------------------

Test the app with the token if you want


Print the deplyment definition file
	terminal --> cat /root/deployment.yml

deployment.yml
-------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 7
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
-------------------------------


Create the deployment
	terminal --> k create -f /root/deployment.yml

	# result: deployment.apps/nginx-deployment created

Verify deployment creation
	terminal --> k get deploy

	# result:
	NAME               READY   UP-TO-DATE   AVAILABLE   AGE
	nginx-deployment   7/7     7            7           20s

- click 'Check' button



3. We have a manifest file to create autoscaling for the Nginx deployment located at /root/autoscale.yml. Review the manifest file and identify the current replicas and desired replicas?
----------------------------------------------------------------------------------------------------------------------------------

A. Current replicas= 7
    Desired replicas= 3

B. Current replicas= 3
    Desired replicas= 7

C. Current replicas= 7
    Desired replicas= 1

D. Current replicas= 0
    Desired replicas= 0


Print the autoscale definition file
	terminal --> cat /root/autoscale.yml

# result:
---------------------------------------
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: null
  name: nginx-deployment
spec:
  maxReplicas: 3
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  targetCPUUtilizationPercentage: 80
status:
  currentReplicas: 0				# this is the andwer section
  desiredReplicas: 0
---------------------------------------

- choose 'D' as answer




4. Create an autoscaler for the nginx-deployment with a maximum of 3 replicas and a CPU utilization target of 80%.
------------------------------------------------------------------------------------------------------------------

Task approach
	option 1 - imperative approach
		- create the autoscaler with no definition file
			terminal --> kubectl autoscale deployment nginx-deployment --max=3 --cpu-percent=80

	option 2 - declarative command
		- edit the autoscaler definition file and set the requred parameters

Option 1
--------

Create the autoscaler with one line command
	terminal -->  kubectl autoscale deployment nginx-deployment --max=3 --cpu-percent=80


Option 2
--------

Edit the autoscaler definition file and set the required aparams
	terminal --> vi /root/autoscale.yml

autoscale.yml
---------------------------------
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: null
  name: nginx-deployment
spec:
  maxReplicas: 3
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  targetCPUUtilizationPercentage: 80
status:
  currentReplicas: 0
  desiredReplicas: 3				# changed from '0' to '3'
---------------------------------
save changes - escape, :wq!, enter

Create the autoscaler
	terminal --> k apply -f /root/autoscale.yml

	# result: horizontalpodautoscaler.autoscaling/nginx-deployment created

Verify the autoscaler creation
	terminal --> k get hpa

	# result:
	NAME               REFERENCE                     TARGETS              MINPODS   MAXPODS   REPLICAS   AGE
	nginx-deployment   Deployment/nginx-deployment   cpu: <unknown>/80%   1         3         7          30s


- click 'Check' button



5. What is the primary purpose of the Horizontal Pod Autoscaler (HPA) in Kubernetes?
------------------------------------------------------------------------------------

- choose 'To automate the scaling of the pods based on observed CPU utilization or other select metrics' as answer




6. What component in a Kubernetes cluster is responsible for providing metrics to the HPA?
------------------------------------------------------------------------------------------

- choose 'metrics-server' as answer




7. What is the current replica count of nginx-deployment after deploying the autoscaler?
----------------------------------------------------------------------------------------

List deployments
	terminal --> k get deploy

	# result:
	NAME               READY   UP-TO-DATE   AVAILABLE   AGE
	nginx-deployment   3/3     3            3           11m

- choose '3' as naswer



8. What is the status of HPA target?
------------------------------------

List HPAs
	terminal --> k get hpa

	# result:
	NAME               REFERENCE                     TARGETS              MINPODS   MAXPODS   REPLICAS   AGE
	nginx-deployment   Deployment/nginx-deployment   cpu: <unknown>/80%   1         3         3          4m35s


- choose '<unknown>/80%' as answer





9. The HPA status shows /80 for the CPU target. what could be a possible reason?
--------------------------------------------------------------------------------

Print details for hpa nginx-deployment
	terminal --> kubectl describe hpa nginx-deployment


# result:
------------------------------------------
Name:                                                  nginx-deployment
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Sun, 02 Mar 2025 09:24:16 +0000
Reference:                                             Deployment/nginx-deployment
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  <unknown> / 80%
Min replicas:                                          1
Max replicas:                                          3
Deployment pods:                                       3 current / 3 desired
Conditions:
  Type           Status  Reason                   Message
  ----           ------  ------                   -------
  AbleToScale    True    SucceededGetScale        the HPA controller was able to get the target's current scale
  ScalingActive  False   FailedGetResourceMetric  the HPA was unable to compute the replica count: failed to get cpu utilization: missing request for cpu in container nginx of Pod nginx-deployment-647677fc66-phc49
Events:
  Type     Reason                        Age                    From                       Message
  ----     ------                        ----                   ----                       -------
  Normal   SuccessfulRescale             6m10s                  horizontal-pod-autoscaler  New size: 3; reason: Current number of replicas above Spec.MaxReplicas
  Warning  FailedGetResourceMetric       4m25s (x3 over 5m25s)  horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu in container nginx of Pod nginx-deployment-647677fc66-6wbqm
  Warning  FailedComputeMetricsReplicas  4m25s (x3 over 5m25s)  horizontal-pod-autoscaler  invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container nginx of Pod nginx-deployment-647677fc66-6wbqm
  Warning  FailedGetResourceMetric       3m55s (x2 over 5m55s)  horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu in container nginx of Pod nginx-deployment-647677fc66-wnvpl
  Warning  FailedComputeMetricsReplicas  3m55s (x2 over 5m55s)  horizontal-pod-autoscaler  invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container nginx of Pod nginx-deployment-647677fc66-wnvpl
  Warning  FailedComputeMetricsReplicas  3m10s (x7 over 5m40s)  horizontal-pod-autoscaler  invalid metrics (1 invalid out of 1), first error is: failed to get cpu resource metric value: failed to get cpu utilization: missing request for cpu in container nginx of Pod nginx-deployment-647677fc66-phc49
  Warning  FailedGetResourceMetric       55s (x16 over 5m40s)   horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu in container nginx of Pod nginx-deployment-647677fc66-phc49
------------------------------------------
In the 'Events' section we can see message:  Warning  FailedGetResourceMetric       4m25s (x3 over 5m25s)  horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu in container nginx of Pod nginx-deployment-647677fc66-6wbqm


- choose 'The deployment does not have any resource fields defined' as answer




10. Since the HPA was failing due to the resource field missing in the nginx-deployment, the resource field has been updated in /root/deployment.yml. Update the nginx-deployment using this manifest. Watch the changes made to the nginx-deployment by the HPA after upgrading by using the kubectl get hpa --watch command.
---------------------------------------------------------------------------------------------------------------------------------

Print the updated hps definition file
	terminal --> cat /root/deployment.yml

deployment.yml
-----------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 7
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
        resources:
         requests:
           cpu: 100m
         limits:
           cpu: 200m
-----------------------------------------


Apply the changes in the hpa definition file
	terminal --> kubectl apply -f /root/deployment.yml

# result:
-------------------------
Warning: resource deployments/nginx-deployment is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
deployment.apps/nginx-deployment configured
-------------------------

Watch the changes
	terminal --> kubectl get hpa --watch


- click 'Check' button



11. What does the event ScalingReplicaSet in the nginx-deployment HPA indicate?
-------------------------------------------------------------------------------

Show the details for this parameter
	terminal --> kubectl events hpa nginx-deployment | grep -i "ScalingReplicaSet"

# result:
-------------------------------------
23m                    Normal    ScalingReplicaSet              Deployment/nginx-deployment                Scaled up replica set nginx-deployment-647677fc66 from 0 to 7
16m                    Normal    ScalingReplicaSet              Deployment/nginx-deployment                Scaled down replica set nginx-deployment-647677fc66 from 7 to 3
97s                    Normal    ScalingReplicaSet              Deployment/nginx-deployment                Scaled up replica set nginx-deployment-647677fc66 from 3 to 7
96s                    Normal    ScalingReplicaSet              Deployment/nginx-deployment                Scaled up replica set nginx-deployment-7998fdcbb8 from 2 to 3
96s                    Normal    ScalingReplicaSet              Deployment/nginx-deployment                Scaled down replica set nginx-deployment-647677fc66 from 7 to 6
96s                    Normal    ScalingReplicaSet              Deployment/nginx-deployment                Scaled up replica set nginx-deployment-7998fdcbb8 from 0 to 2
93s                    Normal    ScalingReplicaSet              Deployment/nginx-deployment                Scaled up replica set nginx-deployment-7998fdcbb8 from 4 to 6
93s                    Normal    ScalingReplicaSet              Deployment/nginx-deployment                Scaled down replica set nginx-deployment-647677fc66 from 5 to 3
93s                    Normal    ScalingReplicaSet              Deployment/nginx-deployment                Scaled up replica set nginx-deployment-7998fdcbb8 from 3 to 4
93s                    Normal    ScalingReplicaSet              Deployment/nginx-deployment                Scaled down replica set nginx-deployment-647677fc66 from 6 to 5
91s                    Normal    ScalingReplicaSet              Deployment/nginx-deployment                Scaled down replica set nginx-deployment-647677fc66 from 3 to 2
73s (x5 over 91s)      Normal    ScalingReplicaSet              Deployment/nginx-deployment                (combined from similar events): Scaled down replica set nginx-deployment-7998fdcbb8 from 3 to 1
-------------------------------------


- choose 'The HPA is increasing the number of pods' as answer




12. What is the cause of the FailedGetResourceMetric event in the nginx-deployment HPA?
---------------------------------------------------------------------------------------

Show details
	terminal --> kubectl events hpa nginx-deployment | grep -i "FailedGetResourceMetric"

# result:
---------------------------------------
17m (x3 over 18m)       Warning   FailedGetResourceMetric        HorizontalPodAutoscaler/nginx-deployment   failed to get cpu utilization: missing request for cpu in container nginx of Pod nginx-deployment-647677fc66-6wbqm
17m (x2 over 19m)       Warning   FailedGetResourceMetric        HorizontalPodAutoscaler/nginx-deployment   failed to get cpu utilization: missing request for cpu in container nginx of Pod nginx-deployment-647677fc66-wnvpl
9m16s (x30 over 19m)    Warning   FailedGetResourceMetric        HorizontalPodAutoscaler/nginx-deployment   failed to get cpu utilization: missing request for cpu in container nginx of Pod nginx-deployment-647677fc66-phc49
---------------------------------------

- choose 'CPU or memory requests are missing for a container' as answer





=====================================
Section 5 128. Inplace Resize of Pods
=====================================
Kubernetes in version 1.32

We have example Deployment:

nginx-yaml
-------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app
	image: nginx
	resources:
	  requests:
	    cpu: "250m"
	    memory: "256Mi"
	  limits:
	    cpu: "500m"
	    memory: "512Mi"
-------------------------------


The default behavior when we change container resources in the deployment is that the deplyment deletes the pod and create new one with the modified resources. This is not desirable behavior for changes.

There is a solution called Inplace Resize of Pods. The feature prevent the pod kill but change the resource usage in the pod wile pod is available. The option is NOT enabled by default.

Enable Inplace Resize of Pods vertical scaling feature
	terminal --> FEATURE_GATES=InPlacePodVerticalScaling=true

This feature support section of resize policies in the container spec in the deployment definition file

nginx-yaml
-------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app
	image: nginx
	resizePolicy:				# this is the supported section
	  - resourceNmae: cpu
	    restartPolicy: NotRequired		# change of the cpu will not restart the pod
	  - resourceNmae: memory
	    restartPolicy: RetartContainer	# change of the memory will restart the pod
	resources:
	  requests:
	    cpu: "250m"
	    memory: "256Mi"
	  limits:
	    cpu: "500m"
	    memory: "512Mi"
-------------------------------


Limitations of Inplace Resize of Pods
	- works only for CPU and memory resources
	- Pod QoS class cannot be change - Quality of Service (QoS)
	- Init containers and Ephemeral Containers cannot be resized
	- A container's memory limit may not be reduced below its usage. If request puts a container in this state, the resize status will remain in InProgress until the desired memory limits becomes feasible
	- Windows pods cannot be resized




=============================================
Section 5 129. Vertical Pod Autoscaling (VPA)
=============================================

We have example Deployment:

nginx-yaml
-------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app
	image: nginx
	resources:
	  requests:
	    cpu: "250m"
	  limits:
	    cpu: "500m"
-------------------------------




Manually vertically scaling workload
------------------------------------
Metric server must be installed on the cluster.

As system administrator we are looking at the cluster

Monitor pods
	terminal --> kubectl top pod my-app-pod

	# result:
	NAME			CPU(cores)		MEMORY(bytes)
	my-app-pod		450m			350Mi


To scale the pod verticallt we edit the deployment and change the resources used
	terminal --> kubectl edit pod my-app-pod

nginx-yaml
-------------------------------
...
    spec:
      containers:
      - name: my-app
	image: nginx
	resources:
	  requests:
	    cpu: "1"			# changed
	  limits:
	    cpu: "500m"
-------------------------------

After changes are saved, the deployment will kill the pod and then create new pod with the changed parameters.



Vertical Pod Autoscaler (VPA)
=============================

How Vertical Pod Autoscaler (VPA) works
	- Observe metrics (pull data from metrics server)
	- Adjust pod resources
	- Balances thresholds

Vertical Pod Autoscaler (VPA) do not comes built-in with Kubernetes! We have to deploy it!

Deploy the Vertical Pod Autoscaler (VPA) with definition file available in the repo
	terminal --> k apply -f https://github.com/kubernetes/autoscaler/releases/latest/download/vertical-pod-autoscaler.yaml

List pods for Vertical Pod Autoscaler (VPA)
	terminal --> k get pods -n kube-system | grep vpa

	# result:
	vpa-admission-controller-xxxx	Running
	vpa-recommender-xxxx		Running
	vpa-updater-xxxx		Running


VPA Recommender - responsible for continously monitoring the resource usage from the Kubernetes Metrics API and collect historical and live usage data for pods and then provides recommendations on optimal cpu and memory values. VPA Recommender dows NOT modify the pod directly - only suggest changes.

VPA Updater - compares the recommnedation from the VPA Recommender with the running pods metrics. If pods cross the tresholds - need update, terminates (kills) the pods.

VPA Admission Controller - after the VPA Updater kills pod, the Deployment will recreate the pod and VPA Admission Controller will intervein to set the cpu and memory to the new pod with values from the VPA Recommender.


Vertical Pod Autoscaler (VPA) is not a built-in component and there is not inperative way (one line command) to create it.


Declarative way to create Vertical Pod Autoscaler (VPA)
-------------------------------------------------------

Create Vertical Pod Autoscaler (VPA) definition file

my-app-vpa.yaml
--------------------------------
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: my-app-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  updatePolicy:
    updateMode: :Auto"			# we have 4 modes
  resourcePolicy:
    containerPolicies:
    - containerNmae: "my-app"
      minAllowed:
	cpu: "250m"
      maxAllowed:
	cpu: "2"
      controllerResources: ["cpu"]
--------------------------------


Modes
- Off 		- only recommends. Does not change anything.
- Initial	- Only changes on Pod creation. Not later.
- Recreate	- Evicts (terminate) pods if usege goes beyond range
- Auto		- Updates existing pods to recommended numbers. For now this behaves similar to Recreate. But when support for 			"In-plcae Update of Pod Resources" is available that mode will be preferred.


Show details for vpa	
	terminal --> kubectl describe vpa my-app-vpa
	
	# result:
	Recommendations:
		Target:
		  Cpu: 	1.5


Key Differences
===============

Feature			VPA (Vertical Scaling)					HPA (Horizontal Scaling)
----------------------------------------------------------------------------------------------------------------------
Scaling Method		Increase CPU & memory of exiting Pods			Add/removes Pods on load

Pod Behavior		Restart Pods to apply new resource values		Keeps existing Pods running

Handling Traffic 	No, because scaling requires a Pod erstart		Yes, instantly adds more Pods
Spikes			(Not preferred)						(Preferred)

Optimize Costs?		Prevents over-provisioning of CPU/memory		Avoids unnecessary idle Pods

Best For		Stateful workload, CPU/memory-heavy apps		Web apps, microservices, 
			(DBs, ML workloads)					stateless services

Example Use Cases	Databases (SQL, PostgreSQL), JVM-based apps		Web services (Nginx, API services),
			AI/ML workloads						message queues, microservices
----------------------------------------------------------------------------------------------------------------------






==========================================
Section 5 130. Practice Test - Install VPA
==========================================


1. Lab Objective:
-----------------
In this lab, you will install and configure the Vertical Pod Autoscaler (VPA) in your Kubernetes cluster. The lab will walk you through the installation of VPA using predefined manifests, cloning the VPA repository for advanced control, and deploying a sample application to see how VPA interacts with it. Additionally, you'll learn how to troubleshoot issues using VPA logs.

By the end of this lab, you should be able to:

Install VPA and its components (Recommender, Updater, Admission Controller) in a Kubernetes cluster
Understand the role of each VPA component and how they contribute to efficient resource management
Deploy a sample application to see how VPA recommends and adjusts resources for it
Troubleshoot resource-related issues in your application using logs generated by the VPA components, particularly the VPA Updater
This hands-on experience will give you the skills to manage pod resources dynamically in a production-grade Kubernetes environment, ensuring that applications run efficiently with the appropriate resource requests.


- click 'Nex' button



2. We have prepared the required YAML files for you to deploy the Vertical Pod Autoscaler (VPA). Simply follow the steps below to apply the necessary configurations:
---------------------------------------------------------------------------------------------------------------------------------
Step 1: Install VPA Custom Resource Definitions (CRDs)
These CRDs allow Kubernetes to recognize the custom resources that VPA uses to function properly. To install them, run this command:

terminal --> kubectl apply -f /root/vpa-crds.yml

# result:
customresourcedefinition.apiextensions.k8s.io/verticalpodautoscalercheckpoints.autoscaling.k8s.io created
customresourcedefinition.apiextensions.k8s.io/verticalpodautoscalers.autoscaling.k8s.io created

Step 2: Install VPA Role-Based Access Control (RBAC)
RBAC ensures that VPA has the appropriate permissions to operate within your Kubernetes cluster. To install the RBAC settings, run:

terminal --> kubectl apply -f /root/vpa-rbac.yml

# result:
clusterrole.rbac.authorization.k8s.io/system:metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:vpa-actor created
clusterrole.rbac.authorization.k8s.io/system:vpa-status-actor created
clusterrole.rbac.authorization.k8s.io/system:vpa-checkpoint-actor created
clusterrole.rbac.authorization.k8s.io/system:evictioner created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-reader created
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-actor created
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-status-actor created
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-checkpoint-actor created
clusterrole.rbac.authorization.k8s.io/system:vpa-target-reader created
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-target-reader-binding created
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-evictioner-binding created
serviceaccount/vpa-admission-controller created
serviceaccount/vpa-recommender created
serviceaccount/vpa-updater created
clusterrole.rbac.authorization.k8s.io/system:vpa-admission-controller created
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-admission-controller created
clusterrole.rbac.authorization.k8s.io/system:vpa-status-reader created
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-status-reader-binding created

By running these commands, the VPA will be successfully deployed to your cluster, ready to manage and adjust your pod resources dynamically.

- click 'Check' button




3. Clone the VPA Repository and Set Up the Vertical Pod Autoscaler
------------------------------------------------------------------
You are required to clone the Kubernetes Autoscaler repository into the /root directory and set up the Vertical Pod Autoscaler (VPA) by running the provided script.

Steps:
Clone the repository:

First, navigate to the /root directory and clone the repository:
	terminal --> git clone https://github.com/kubernetes/autoscaler.git

Navigate to the Vertical Pod Autoscaler directory:

After cloning, move into the vertical-pod-autoscaler directory:
	terminal --> cd autoscaler/vertical-pod-autoscaler

Run the setup script:

Execute the provided script to deploy the Vertical Pod Autoscaler:
	terminal --> ./hack/vpa-up.sh

# result:
HEAD is now at 200a292f8 Merge pull request #7774 from jm-franc/vpa-finalize-branch
customresourcedefinition.apiextensions.k8s.io/verticalpodautoscalercheckpoints.autoscaling.k8s.io configured
customresourcedefinition.apiextensions.k8s.io/verticalpodautoscalers.autoscaling.k8s.io configured
clusterrole.rbac.authorization.k8s.io/system:metrics-reader unchanged
clusterrole.rbac.authorization.k8s.io/system:vpa-actor unchanged
clusterrole.rbac.authorization.k8s.io/system:vpa-status-actor unchanged
clusterrole.rbac.authorization.k8s.io/system:vpa-checkpoint-actor unchanged
clusterrole.rbac.authorization.k8s.io/system:evictioner unchanged
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-reader unchanged
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-actor unchanged
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-status-actor unchanged
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-checkpoint-actor unchanged
clusterrole.rbac.authorization.k8s.io/system:vpa-target-reader unchanged
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-target-reader-binding unchanged
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-evictioner-binding unchanged
serviceaccount/vpa-admission-controller unchanged
serviceaccount/vpa-recommender unchanged
serviceaccount/vpa-updater unchanged
clusterrole.rbac.authorization.k8s.io/system:vpa-admission-controller unchanged
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-admission-controller unchanged
clusterrole.rbac.authorization.k8s.io/system:vpa-status-reader unchanged
clusterrolebinding.rbac.authorization.k8s.io/system:vpa-status-reader-binding unchanged
role.rbac.authorization.k8s.io/system:leader-locking-vpa-updater created
rolebinding.rbac.authorization.k8s.io/system:leader-locking-vpa-updater created
role.rbac.authorization.k8s.io/system:leader-locking-vpa-recommender created
rolebinding.rbac.authorization.k8s.io/system:leader-locking-vpa-recommender created
deployment.apps/vpa-updater created
deployment.apps/vpa-recommender created
Generating certs for the VPA Admission Controller in /tmp/vpa-certs.
Certificate request self-signature ok
subject=CN = vpa-webhook.kube-system.svc
Uploading certs to the cluster.
error: failed to create secret secrets "vpa-tls-certs" already exists
deployment.apps/vpa-admission-controller created
service/vpa-webhook created

By following these steps, the Vertical Pod Autoscaler will be installed and ready to manage pod resources in your Kubernetes cluster.

- click 'Check' button




4. Which of the following are the VPA CRDs that get installed as part of the Vertical Pod Autoscaler setup?
-----------------------------------------------------------------------------------------------------------

List VPA CRDs
	terminal --> kubectl get crds | grep verticalpodautoscaler

# result:
verticalpodautoscalercheckpoints.autoscaling.k8s.io   2025-03-02T10:56:08Z
verticalpodautoscalers.autoscaling.k8s.io             2025-03-02T10:56:08Z

- choose 'verticalpodautoscalers.autoscaling.k8s.io & verticalpodautoscalercheckpoints.autoscaling.k8s.io' as answer




5. How many VPA deployments typically run in the kube-system namespace after installation?
------------------------------------------------------------------------------------------

List deployments connected with VPA
	terminal --> kubectl get deployments -n kube-system | grep vpa

# result:
vpa-admission-controller   1/1     1            1           3m9s
vpa-recommender            1/1     1            1           3m10s
vpa-updater                1/1     1            1           3m10s

- choose '3' as answer




6. You are given a Kubernetes deployment file named flask-app.yml located in the /root directory. Your task is to:
------------------------------------------------------------------------------------------------------------------
Deploy the flask-app.yml file to the Kubernetes cluster
After deployment, check the logs of the Vertical Pod Autoscaler(VPA) updater to ensure it is functioning correctly

Note: The pod may take some time to reach the running state.

Navigate to /root diretory
	terminal --> cd ../..

Print the deployment definition file
	terminal --> cat flask-app.yml

flask-app.yml
--------------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flask-app
  labels:
    app: flask-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: flask-app
  template:
    metadata:
      labels:
        app: flask-app
    spec:
      containers:
      - name: flask-app
        image:  kodekloud/flask-session-app:1 
        ports:
        - name: http
          containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: flask-app-service
  labels:
    app: flask-app
spec:
  selector:
    app: flask-app
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 8080
---
apiVersion: "autoscaling.k8s.io/v1"
kind: VerticalPodAutoscaler
metadata:
  name: flask-app
spec:
  # recommenders field can be unset when using the default recommender.
  # When using an alternative recommender, the alternative recommender's name
  # can be specified as the following in a list.
  # recommenders: 
  #   - name: 'alternative'
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: flask-app
  updatePolicy:
    updateMode: "Recreate"
    evictionRequirements:
      - resources: ["cpu", "memory"]
        changeRequirement: TargetHigherThanRequests
  resourcePolicy:
    containerPolicies:
      - containerName: '*'
        minAllowed:
          cpu: 100m
          memory: 100Mi
        maxAllowed:
          cpu: 1
          memory: 500Mi
        controlledResources: ["cpu", "memory"]
--------------------------------------------------


Create the deployment
	terminal --> k create -f flask-app.yml

	# result:
	deployment.apps/flask-app created
	service/flask-app-service created
	verticalpodautoscaler.autoscaling.k8s.io/flask-app created

List pods
	terminal --> k get pods

	# result:
	NAME                         READY   STATUS    RESTARTS   AGE
	flask-app-67b666c5fc-tqtml   1/1     Running   0          42s

Check Logs
	terminal --> k logs flask-app-67b666c5fc-tqtml

# result:
 * Serving Flask app 'app'
 * Debug mode: off
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:8080
 * Running on http://172.17.1.5:8080
Press CTRL+C to quit

- click 'Check' button



7. You have recently deployed a Flask application to your Kubernetes cluster. However, the Vertical Pod Autoscaler (VPA) vpa-updater-XXXX pod indicates that there may be an issue with the newly deployed flask-app pods.
-----------------------------------------------------------------------------------------------------------------------------
Perform the following tasks:

Check the logs of the vpa-updater-XXXX pod to identify any potential issues with the flask-app deployment.
When checking the logs, you see the following error message:

pods_eviction_restriction.go:226] **too few replicas** for **ReplicaSet** default/**flask-app-b6c9c4f78**


Here is a command that picks the name of the vpa-updater pod and prints its logs:
	terminal --> kubectl logs $(kubectl get pods -n kube-system --no-headers -o custom-columns=":metadata.name" | grep vpa-updater) -n kube-system





============================================================
Section 5 131. Practice Test - Modifing CPU reousrces in VPA
============================================================

1. In this lab, you will deploy a sample application on Kubernetes, monitor its CPU resource usage, and utilize a Vertical Pod Autoscaler (VPA) to manage and adjust the resource allocation for the pods. The goal is to observe how the VPA automatically recommends and adjusts memory resource limits, particularly when the application experiences increased load.
------------------------------------------------------------------------------------------------------------------------------
Objectives:
Deploy a sample application and verify that the pods are running correctly.
Monitor pod resource usage using the kubectl top command to assess CPU and memory consumption.
Deploy a Vertical Pod Autoscaler (VPA) to observe how it adjusts CPU and memory resource recommendations based on the applicationâ€™s current usage.
Introduce load to the application and monitor how VPA dynamically updates its recommendations in response to increased demand.
Interpret VPA recommendations by understanding key parameters like lowerBound, upperBound, and uncappedTarget for resource management.
By the end of this lab, you will have a clear understanding of how VPA optimizes CPU resource usage in a Kubernetes environment, improving application performance and efficiency under varying workloads.

- click 'Next' button



2. A file named vpa-cpu-testing.yml has been prepared and is located in the /root directory. Proceed to deploy this file.
-------------------------------------------------------------------------------------------------------------------------

Print the file
	terminal --> cat vpa-cpu-testing.yml

vpa-cpu-testing.yml
---------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flask-app-4
  labels:
    app: flask-app-4
spec:
  replicas: 2
  selector:
    matchLabels:
      app: flask-app-4
  template:
    metadata:
      labels:
        app: flask-app-4
    spec:
      containers:
      - name: flask-app-4
        image:  kodekloud/flask-session-app:1 
        ports:
        - name: http
          containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: flask-app-4-service
  labels:
    app: flask-app-4
spec:
  type: NodePort
  selector:
    app: flask-app-4
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 8080
    nodePort: 30080
---------------------------------------------


Create the deployment
	terminal --> k create -f vpa-cpu-testing.yml

	# result:
	deployment.apps/flask-app-4 created
	service/flask-app-4-service created

- click 'Check' button





3. Monitoring Pod Resource Usage
--------------------------------
To check the current resource usage (CPU and memory) of all running pods in your Kubernetes cluster, use the kubectl top pod command. This command retrieves and displays resource metrics directly from the cluster's resource metrics API. It is particularly useful for tracking how efficiently your workloads are consuming cluster resources.

To view the resource consumption of the pods, run the following command:
	terminal --> kubectl top pod
	
The output will display a table with each pod's name, namespace, CPU usage (in millicores), and memory usage (in mebibytes), allowing you to monitor resource usage in real time.

For example, the output may look like this:

NAME                           CPU(cores)   MEMORY(bytes)   
flask-app-4-5cfb5d78c4-p9l2m   1m           19Mi            
flask-app-4-5cfb5d78c4-t229n   1m           19Mi            

This output provides an overview of how much CPU and memory each pod is currently consuming.

Note: The metrics server may take some time to collect metrics from newly deployed pods.


- click 'Next' button






4. A file named vpa-cpu.yml has been created in the /root directory. Proceed to deploy this file.
-------------------------------------------------------------------------------------------------
Once deployed, check the cpu consumption by running the below command:
	terminal --> kubectl get vpa


Print the file
	terminal --> cat vpa-cpu.yml

vpa-cpu.yml
--------------------------------------------------
apiVersion: "autoscaling.k8s.io/v1"
kind: VerticalPodAutoscaler
metadata:
  name: flask-app
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: flask-app-4
  updatePolicy:
    updateMode: "Off"  # You can set this to "Auto" if you want automatic updates
  resourcePolicy:
    containerPolicies:
      - containerName: '*'
        minAllowed:
          cpu: 100m
        maxAllowed:
          cpu: 1000m
        controlledResources: ["cpu"]
--------------------------------------------------


Apply the file
	terminal --> kubectl apply -f /root/vpa-cpu.yml

	# result: verticalpodautoscaler.autoscaling.k8s.io/flask-app created

List VPAs
	terminal --> kubectl get vpa

	# result:
	NAME        MODE   CPU   MEM   PROVIDED   AGE
	flask-app   Off                           20s



5. Initiate the load on the flask-app-4 deployment by executing the script located at /root/load.sh.
----------------------------------------------------------------------------------------------------

Show load
	terminal --> /root/load.sh

- click 'Check' button



6. Capture the recommended target CPU value from the flask-app VPA and store it in /root/target.
------------------------------------------------------------------------------------------------


List VPAs
	terminal --> k get vpa

	# result:
	NAME        MODE   CPU    MEM   PROVIDED   AGE
	flask-app   Off    587m         True       6m31s


Print the VPA
	terminal --> kubectl get vpa flask-app -o yaml

# result:
--------------------------------------------------------
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"autoscaling.k8s.io/v1","kind":"VerticalPodAutoscaler","metadata":{"annotations":{},"name":"flask-app","namespace":"default"},"spec":{"resourcePolicy":{"containerPolicies":[{"containerName":"*","controlledResources":["cpu"],"maxAllowed":{"cpu":"1000m"},"minAllowed":{"cpu":"100m"}}]},"targetRef":{"apiVersion":"apps/v1","kind":"Deployment","name":"flask-app-4"},"updatePolicy":{"updateMode":"Off"}}}
  creationTimestamp: "2025-03-02T11:16:00Z"
  generation: 1
  name: flask-app
  namespace: default
  resourceVersion: "3135"
  uid: 32733d47-7e91-4706-9dd0-7ea3ba218384
spec:
  resourcePolicy:
    containerPolicies:
    - containerName: '*'
      controlledResources:
      - cpu
      maxAllowed:
        cpu: 1000m
      minAllowed:
        cpu: 100m
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: flask-app-4
  updatePolicy:
    updateMode: "Off"
status:
  conditions:
  - lastTransitionTime: "2025-03-02T11:16:57Z"
    status: "True"
    type: RecommendationProvided
  recommendation:
    containerRecommendations:
    - containerName: flask-app-4
      lowerBound:
        cpu: 100m
      target:
        cpu: 476m			# this is the target cpu value
      uncappedTarget:
        cpu: 476m
      upperBound:
        cpu: "1"
--------------------------------------------------------

Save the target cpu in the required file
	terminal --> echo "476m" > /root/target

- click 'Check' button








